{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced House Price Prediction: Kaggle competition\n",
    "\n",
    "# Link for the dataset is: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "# Data Dictionary can be found in the same link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by: Shankar Pendse (R00195877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import normalize, PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(datafile):\n",
    "    return pd.read_csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categorical_features(train_df, test_df):\n",
    "    # Street: Type of road access to property either gravel or paved. Let's encode Grvl as 0 and Pave as 1,\n",
    "    # since we have only two types associated with it(discrete values), lets give 0 value to Grvl and 1 value to Pave, so that pave\n",
    "    # gets more weight\n",
    "\n",
    "    train_df['Street'] = train_df['Street'].replace({'Grvl':0, 'Pave': 1})\n",
    "    test_df['Street'] = test_df['Street'].replace({'Grvl':0, 'Pave': 1})\n",
    "    \n",
    "    # Alley: Type of alley access to property. \n",
    "    # It is given that, If the value for this attribute is nan, it means that the property has no Alley access\n",
    "    # Let's use 0 for no access and 1 if it has alley access irrespective whether it is Grvl or Paved\n",
    "\n",
    "    train_df['Alley'] = train_df['Alley'].replace({np.nan:0,'Grvl':1, 'Pave':1 })\n",
    "    test_df['Alley'] = test_df['Alley'].replace({np.nan:0,'Grvl':1, 'Pave':1 })\n",
    "    \n",
    "    # LotShape: General shape of the property. \n",
    "    # There are 4 types to this, regular, and 3 types of irregular,\n",
    "    # Let's categorize this into two types, regular(Reg) and irregular(Irr).\n",
    "    # Use 0 for irregular and 1 for regular\n",
    "\n",
    "    train_df['LotShape'] = train_df['LotShape'].replace( {'Reg':1, 'IR1':0, 'IR2':0, 'IR3':0} )\n",
    "    test_df['LotShape'] = test_df['LotShape'].replace( {'Reg':1, 'IR1':0, 'IR2':0, 'IR3':0} )\n",
    "    \n",
    "    # LandContour: Flatness of the property, as we have seen from the unique values for this attribute, it takes 4 different values, \n",
    "    # but let's change the name to IsLevel and represent with 1 if level and 0 if not\n",
    "\n",
    "    train_df = train_df.rename(columns={'LandContour':'IsLevel'})\n",
    "    train_df['IsLevel'] = train_df['IsLevel'].replace( {'Lvl':1, 'Bnk':0, 'Low':0, 'HLS':0} )\n",
    "\n",
    "    test_df = test_df.rename(columns={'LandContour':'IsLevel'})\n",
    "    test_df['IsLevel'] = test_df['IsLevel'].replace({'Lvl':1, 'Bnk':0, 'Low':0, 'HLS':0})\n",
    "    \n",
    "    # Utilities: It has only 2 unique values, AllPub & NoSeWa. \n",
    "    # Let's rename the column to AllUtilities and represent with 1 for all and 0 for partial\n",
    "\n",
    "    train_df = train_df.rename(columns={'Utilities': 'AllUtilities'})\n",
    "    train_df['AllUtilities'] = train_df['AllUtilities'].replace( {'AllPub':1, 'NoSeWa':0} )\n",
    "\n",
    "    test_df = test_df.rename(columns={'Utilities': 'AllUtilities'})\n",
    "    test_df['AllUtilities'] = test_df['AllUtilities'].replace( {'AllPub':1, 'NoSeWa':0} )\n",
    "    \n",
    "    # LotConfig: It can take 5 Unique values,\n",
    "    # let's use get_Dummies to represent this attribute in one-hot encoding format\n",
    "    train_df = pd.get_dummies(data = train_df, columns= ['LotConfig'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns= ['LotConfig'])\n",
    "    \n",
    "    #Neighborhood : Let's use one hot encoding for this attribute \n",
    "    train_df = pd.get_dummies(data = train_df, columns= ['Neighborhood'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Neighborhood'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns= ['Condition1'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Condition2'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Condition1'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Condition2'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['BldgType'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['HouseStyle'])\n",
    "\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['BldgType'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['HouseStyle'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['RoofStyle'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['RoofMatl'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Exterior1st'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Exterior2nd'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['MasVnrType'])\n",
    "\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['RoofStyle'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['RoofMatl'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Exterior1st'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Exterior2nd'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['MasVnrType'])\n",
    "    \n",
    "    # Let's use ordinal encoding for ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC,\n",
    "    # KitchenQual, Functional, FireplaceQu, GarageQual, GarageCond, PoolQC\n",
    "\n",
    "    ExterQual_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "    ExterCond_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "    BsmtQual_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "    BsmtCond_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "    BsmtExposure_dict = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n",
    "    BsmtFinType1_dict = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    BsmtFinType2_dict = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    HeatingQC_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "    KitchenQual_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "    Functional_dict = {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1}\n",
    "    FireplaceQu_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "    GarageQual_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "    GarageCond_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "    PoolQC_dict = {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'NA': 0}\n",
    "\n",
    "    train_df['ExterQual'] = train_df['ExterQual'].map(ExterQual_dict)\n",
    "    train_df['ExterCond'] = train_df['ExterCond'].map(ExterCond_dict)\n",
    "    train_df['BsmtQual'] = train_df['BsmtQual'].map(BsmtQual_dict)\n",
    "    train_df['BsmtCond'] = train_df['BsmtCond'].map(BsmtCond_dict)\n",
    "    train_df['BsmtExposure'] = train_df['BsmtExposure'].map(BsmtExposure_dict)\n",
    "    train_df['BsmtFinType1'] = train_df['BsmtFinType1'].map(BsmtFinType1_dict)\n",
    "    train_df['BsmtFinType2'] = train_df['BsmtFinType2'].map(BsmtFinType2_dict)\n",
    "    train_df['HeatingQC'] = train_df['HeatingQC'].map(HeatingQC_dict)\n",
    "    train_df['KitchenQual'] = train_df['KitchenQual'].map(KitchenQual_dict)\n",
    "    train_df['Functional'] = train_df['Functional'].map(Functional_dict)\n",
    "    train_df['FireplaceQu'] = train_df['FireplaceQu'].map(FireplaceQu_dict)\n",
    "    train_df['GarageQual'] = train_df['GarageQual'].map(GarageQual_dict)\n",
    "    train_df['GarageCond'] = train_df['GarageCond'].map(GarageCond_dict)\n",
    "    train_df['PoolQC'] = train_df['PoolQC'].map(PoolQC_dict)\n",
    "\n",
    "    test_df['ExterQual'] = test_df['ExterQual'].map(ExterQual_dict)\n",
    "    test_df['ExterCond'] = test_df['ExterCond'].map(ExterCond_dict)\n",
    "    test_df['BsmtQual'] = test_df['BsmtQual'].map(BsmtQual_dict)\n",
    "    test_df['BsmtCond'] = test_df['BsmtCond'].map(BsmtCond_dict)\n",
    "    test_df['BsmtExposure'] = test_df['BsmtExposure'].map(BsmtExposure_dict)\n",
    "    test_df['BsmtFinType1'] = test_df['BsmtFinType1'].map(BsmtFinType1_dict)\n",
    "    test_df['BsmtFinType2'] = test_df['BsmtFinType2'].map(BsmtFinType2_dict)\n",
    "    test_df['HeatingQC'] = test_df['HeatingQC'].map(HeatingQC_dict)\n",
    "    test_df['KitchenQual'] = test_df['KitchenQual'].map(KitchenQual_dict)\n",
    "    test_df['Functional'] = test_df['Functional'].map(Functional_dict)\n",
    "    test_df['FireplaceQu'] = test_df['FireplaceQu'].map(FireplaceQu_dict)\n",
    "    test_df['GarageQual'] = test_df['GarageQual'].map(GarageQual_dict)\n",
    "    test_df['GarageCond'] = test_df['GarageCond'].map(GarageCond_dict)\n",
    "    test_df['PoolQC'] = test_df['PoolQC'].map(PoolQC_dict)\n",
    "\n",
    "\n",
    "    # It is given that for above features, nan means that particular feature is absent, so we will just replace nan with 0\n",
    "\n",
    "    train_df['BsmtQual'] = train_df['BsmtQual'].fillna(0)\n",
    "    train_df['BsmtCond'] = train_df['BsmtCond'].fillna(0)\n",
    "    train_df['BsmtExposure'] = train_df['BsmtExposure'].fillna(0)\n",
    "    train_df['BsmtFinType1'] = train_df['BsmtFinType1'].fillna(0)\n",
    "    train_df['BsmtFinType2'] = train_df['BsmtFinType2'].fillna(0)\n",
    "    train_df['FireplaceQu'] = train_df['FireplaceQu'].fillna(0)\n",
    "    train_df['GarageQual'] = train_df['GarageQual'].fillna(0)\n",
    "    train_df['GarageCond'] = train_df['GarageCond'].fillna(0)\n",
    "    train_df['PoolQC'] = train_df['PoolQC'].fillna(0)\n",
    "\n",
    "\n",
    "    test_df['BsmtQual'] = test_df['BsmtQual'].fillna(0)\n",
    "    test_df['BsmtCond'] = test_df['BsmtCond'].fillna(0)\n",
    "    test_df['BsmtExposure'] = test_df['BsmtExposure'].fillna(0)\n",
    "    test_df['BsmtFinType1'] = test_df['BsmtFinType1'].fillna(0)\n",
    "    test_df['BsmtFinType2'] = test_df['BsmtFinType2'].fillna(0)\n",
    "    test_df['FireplaceQu'] = test_df['FireplaceQu'].fillna(0)\n",
    "    test_df['GarageQual'] = test_df['GarageQual'].fillna(0)\n",
    "    test_df['GarageCond'] = test_df['GarageCond'].fillna(0)\n",
    "    test_df['PoolQC'] = test_df['PoolQC'].fillna(0)\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Foundation'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Foundation'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Heating'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Heating'])\n",
    "    \n",
    "    train_df['CentralAir'] = train_df['CentralAir'].replace({'N':0, 'Y': 1})\n",
    "    test_df['CentralAir'] = test_df['CentralAir'].replace({'N':0, 'Y': 1})\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['Electrical'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['Electrical'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['GarageType'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['GarageType'])\n",
    "    \n",
    "    GarageFinish_dict = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    train_df['GarageFinish'] = train_df['GarageFinish'].map(GarageFinish_dict)\n",
    "    test_df['GarageFinish'] = test_df['GarageFinish'].map(GarageFinish_dict)\n",
    "\n",
    "    train_df['GarageFinish'] = train_df['GarageFinish'].fillna(0)\n",
    "    test_df['GarageFinish'] = test_df['GarageFinish'].fillna(0)\n",
    "    \n",
    "    PavedDrive_dict = {'Y': 2, 'P': 1, 'N': 0}\n",
    "    train_df['PavedDrive_ordinal'] = train_df['PavedDrive'].map(PavedDrive_dict)\n",
    "    test_df['PavedDrive_ordinal'] = test_df['PavedDrive'].map(PavedDrive_dict)\n",
    "\n",
    "    train_df.drop('PavedDrive', axis = 1, inplace = True)\n",
    "    test_df.drop('PavedDrive', axis = 1, inplace = True)\n",
    "\n",
    "    # Fence is actually having mix of features : privacy and amount of wood/wire, nan indicates no fence\n",
    "    Fence_dict = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1}\n",
    "    train_df['Fence'] = train_df['Fence'].map(Fence_dict)\n",
    "    test_df['Fence'] = test_df['Fence'].map(Fence_dict)\n",
    "\n",
    "    train_df['Fence'] = train_df['Fence'].fillna(0)\n",
    "    test_df['Fence'] = test_df['Fence'].fillna(0)\n",
    "    \n",
    "    train_df['MiscFeature'] = train_df['MiscFeature'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "    test_df['MiscFeature'] = test_df['MiscFeature'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['SaleType'])\n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['SaleCondition'])\n",
    "\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['SaleType'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['SaleCondition'])\n",
    "    \n",
    "    train_df = pd.get_dummies(data = train_df, columns = ['MSZoning'])\n",
    "    test_df = pd.get_dummies(data = test_df, columns = ['MSZoning'])\n",
    "    \n",
    "    LandSlope_dict = {'Gtl': 2, 'Mod': 1, 'Sev': 0}\n",
    "    train_df['LandSlope'] = train_df['LandSlope'].map(LandSlope_dict)\n",
    "    test_df['LandSlope'] = test_df['LandSlope'].map(LandSlope_dict)\n",
    "    \n",
    "    # After get_dummies, there could be few missing columns in test_df, so we will have to drop them\n",
    "    cols_to_drop = train_df.columns.difference(test_df.columns).tolist()\n",
    "    cols_to_drop.remove('SalePrice')\n",
    "    train_df.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(train_df,test_df):\n",
    "    imputer = SimpleImputer(missing_values = np.NaN , strategy='mean')\n",
    "    \n",
    "    imputer.fit(train_df[['LotFrontage']])\n",
    "    train_df['LotFrontage'] = imputer.transform(train_df[['LotFrontage']])\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values = np.NaN , strategy='mean')\n",
    "    imputer.fit(train_df[['MasVnrArea']])\n",
    "    train_df['MasVnrArea'] = imputer.transform(train_df[['MasVnrArea']])\n",
    "    \n",
    "    # for the records where GarageYrBlt is NaN, there is no Garage and we can drop the column GarageYrBlt from our dataset\n",
    "    train_df.drop('GarageYrBlt',axis = 1, inplace = True)\n",
    "    test_df.drop('GarageYrBlt',axis = 1, inplace = True)\n",
    "    \n",
    "    test_df['LotFrontage'] = imputer.transform(test_df[['LotFrontage']])\n",
    "    test_df['AllUtilities'].fillna(1, inplace = True)\n",
    "    test_df['MasVnrArea'] = imputer.transform(test_df[['MasVnrArea']])\n",
    "    \n",
    "    imputer.fit(test_df[['BsmtFinSF1']])\n",
    "    test_df['BsmtFinSF1'] = imputer.transform(test_df[['BsmtFinSF1']])\n",
    "    \n",
    "    imputer.fit(test_df[['BsmtFinSF2']])\n",
    "    test_df['BsmtFinSF2'] = imputer.transform(test_df[['BsmtFinSF2']])\n",
    "    \n",
    "    imputer.fit(test_df[['BsmtUnfSF']])\n",
    "    test_df['BsmtUnfSF'] = imputer.transform(test_df[['BsmtUnfSF']])\n",
    "    \n",
    "    imputer.fit(test_df[['TotalBsmtSF']])\n",
    "    test_df['TotalBsmtSF'] = imputer.transform(test_df[['TotalBsmtSF']])\n",
    "    \n",
    "    test_df['BsmtFullBath'].fillna(0, inplace = True)\n",
    "    test_df['BsmtHalfBath'].fillna(0, inplace = True)\n",
    "    test_df['KitchenQual'].fillna(test_df['KitchenQual'].mode()[0], inplace = True)\n",
    "    test_df['Functional'].fillna(test_df['Functional'].mode()[0], inplace = True)\n",
    "    test_df['GarageCars'].fillna(0, inplace = True)\n",
    "    test_df['GarageArea'].fillna(0, inplace = True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(train_df,test_df,scale_type):\n",
    "    to_be_scaled = ['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "                    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n",
    "                    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n",
    "                    'ScreenPorch', 'PoolArea', 'MiscVal']\n",
    "    if scale_type == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    train_df[to_be_scaled] = scaler.fit_transform(train_df[to_be_scaled])\n",
    "    \n",
    "    # We also need to apply the same scaler strategy on test dataset, instead of fit_transform, we just have to use transform\n",
    "    test_df[to_be_scaled] = scaler.transform(test_df[to_be_scaled])\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection_and_removal(train_df, test_df, strategy, scale_type):\n",
    "    if strategy == \"isoforest\":\n",
    "        train_df_scaled, test_df_scaled = scale_features(train_df, test_df, scale_type)\n",
    "        isoClf = IsolationForest (contamination = 0.01, random_state = 195877)\n",
    "        isoClf.fit(train_df_scaled)\n",
    "        \n",
    "        results = isoClf.predict(train_df_scaled)\n",
    "        train_df_no_outliers = train_df_scaled[results == 1]\n",
    "\n",
    "    return train_df_no_outliers, test_df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split_data(train_df_no_outliers):\n",
    "    # let's randomly shuffle the train_df_no_outliers\n",
    "    train_df_no_outliers = train_df_no_outliers.sample(frac=1, random_state=195877).reset_index(drop=True)\n",
    "\n",
    "    # Let's keep aside a part of total data as test data for checking the model performance after kfold validation models\n",
    "    train_data, test_data = train_test_split(train_df_no_outliers, test_size = 0.2, random_state = 195877)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_validations(train_data):\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits = 10, shuffle=True, random_state = 195877)\n",
    "    \n",
    "    # KFold for Linear Regression\n",
    "    print(\"\\nRunning kfold validations using Simple Linear Regression model:\\n\")\n",
    "    LRResults = []\n",
    "    LRModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        LRM = LinearRegression(n_jobs = -1)\n",
    "        LRM.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        LRResults.append(LRM.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        LRModels.append(LRM)\n",
    "        print(\"    \",LRResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for Linear Regression model is: \", np.mean(LRResults))\n",
    "    \n",
    "    \n",
    "    # Kfold for KNN Regression\n",
    "    print(\"\\nRunning kfold validations using KNN Regression model:\\n\")\n",
    "    KNNResults = []\n",
    "    KNNModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        KNNRM = KNeighborsRegressor(n_jobs = -1)\n",
    "        KNNRM.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        KNNResults.append(KNNRM.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        KNNModels.append(KNNRM)\n",
    "        print(\"    \",KNNResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for KNN Regression model is: \", np.mean(KNNResults))\n",
    "    \n",
    "    \n",
    "    # Kfold for Support Vector Regression\n",
    "    print(\"\\nRunning kfold validations using support vector regression model:\\n\")\n",
    "    SVResults = []\n",
    "    SVRModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        SVRM = SVR(kernel = 'linear')\n",
    "        SVRM.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        SVResults.append(SVRM.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        SVRModels.append(SVRM)\n",
    "        print(\"    \",SVResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for BayesianRidge Regression model is: \", np.mean(SVResults))\n",
    "    \n",
    "    \n",
    "    # Kfold for BayesianRidge Regression\n",
    "    print(\"\\nRunning kfold valiations using Bayesian Ridge Regression:\\n\")\n",
    "    BRResults = []\n",
    "    BRModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        BRM = BayesianRidge()\n",
    "        BRM.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        BRResults.append(BRM.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        BRModels.append(BRM)\n",
    "        print(\"    \",BRResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for BayesianRidge Regression model is: \", np.mean(BRResults))\n",
    "    \n",
    "    \n",
    "    # Kfold for Random Forest Regression\n",
    "    print(\"\\nRunning kfold valiations using Random Forest Regression:\\n\")\n",
    "    RFResults = []\n",
    "    RFModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        RFM = RandomForestRegressor(random_state = 195877, n_jobs = -1)\n",
    "        RFM.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        RFResults.append(RFM.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        RFModels.append(RFM)\n",
    "        print(\"    \",RFResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for Random Forest Regression model is: \", np.mean(RFResults))\n",
    "    \n",
    "    \n",
    "    # Kfold for Gradient Boost Regression\n",
    "    print(\"\\nRunning kfold validations using Gradient Boost Regression:\\n\")\n",
    "    GBResults = []\n",
    "    GBModels = []\n",
    "    for train_indices, val_indices in kf.split(train_data.drop('SalePrice', axis = 1) , train_data['SalePrice']):\n",
    "        GBR = GradientBoostingRegressor(random_state = 195877)\n",
    "        GBR.fit(train_data.drop('SalePrice', axis = 1).iloc[train_indices], train_data['SalePrice'].iloc[train_indices])\n",
    "        GBResults.append(GBR.score(train_data.drop('SalePrice', axis = 1).iloc[val_indices], train_data['SalePrice'].iloc[val_indices]))\n",
    "        GBModels.append(GBR)\n",
    "\n",
    "        print(\"    \",GBResults[-1])\n",
    "\n",
    "    print(\"\\nMean R2 Score for Gradient Boost Regression model is: \", np.mean(GBResults))\n",
    "    \n",
    "    BestLRM = LRModels[np.argmax(LRResults)]\n",
    "    BestRFM = RFModels[np.argmax(RFResults)]\n",
    "    BestGBM = GBModels[np.argmax(GBResults)]\n",
    "    BestBRM = BRModels[np.argmax(BRResults)]\n",
    "    \n",
    "    return BestLRM, BestRFM, BestGBM, BestBRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning_RFM(BestRFM, train_data):\n",
    "    \n",
    "    RF_parameters = {\"n_estimators\"      : [1000,2000,3000],\n",
    "                     \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n",
    "                     \"max_depth\"         : [4,6,8,None],\n",
    "                     \"min_samples_split\" : [2,3,4],\n",
    "                     \"warm_start\"        : [True, False],\n",
    "                     \"random_state\"      : [195877],\n",
    "                     }\n",
    "    RF_grid = GridSearchCV(BestRFM, RF_parameters, n_jobs=-1, cv=5, verbose = 10, return_train_score=True)\n",
    "    RF_grid.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])\n",
    "    return RF_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning_GBM(BestGBM, train_data):\n",
    "    GB_parameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n",
    "                      'subsample'    : [0.9, 0.5, 0.2, 0.1],\n",
    "                      'n_estimators' : [1000,2000,3000],\n",
    "                      'max_depth'    : [4,6,8,10,None],\n",
    "                      'random_state' : [195877]\n",
    "                     }\n",
    "    GB_grid = GridSearchCV(BestGBM, GB_parameters, n_jobs=-1, cv=5, verbose = 10, return_train_score=True)\n",
    "    GB_grid.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])\n",
    "    return GB_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe shape:  (1460, 81)\n",
      "test dataframe shape:  (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "# Read data (train and test dataframes)\n",
    "train_df = read_data(\"./house-prices-advanced-regression-techniques/train.csv\")\n",
    "test_df = read_data(\"./house-prices-advanced-regression-techniques/test.csv\")\n",
    "print(\"train dataframe shape: \", train_df.shape)\n",
    "print(\"test dataframe shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop('Id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = preprocess_categorical_features(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 199)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = handle_missing_values(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 199)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 198)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_no_outliers, test_df = outlier_detection_and_removal(train_df, test_df, \"isoforest\",\"minmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1445, 199)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 198)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = shuffle_split_data(train_df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 199)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 199)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running kfold validations using Simple Linear Regression model:\n",
      "\n",
      "     0.8705652282207911\n",
      "     0.8622832842362065\n",
      "     0.8455069460395264\n",
      "     0.8370294368794542\n",
      "     0.8783512994306039\n",
      "     0.9052462249472807\n",
      "     0.8992719048023904\n",
      "     0.8268710164259795\n",
      "     0.9110290866654857\n",
      "     0.909829055286262\n",
      "\n",
      "Mean R2 Score for Linear Regression model is:  0.874598348293398\n",
      "\n",
      "Running kfold validations using KNN Regression model:\n",
      "\n",
      "     0.6306515859066968\n",
      "     0.6670466109113717\n",
      "     0.6741380929719193\n",
      "     0.6914920973648763\n",
      "     0.5744575575416151\n",
      "     0.6992936310467013\n",
      "     0.7527515647161587\n",
      "     0.6681147372370724\n",
      "     0.6547720157356081\n",
      "     0.7555268039122046\n",
      "\n",
      "Mean R2 Score for KNN Regression model is:  0.6768244697344225\n",
      "\n",
      "Running kfold validations using support vector regression model:\n",
      "\n",
      "     0.3007292658703544\n",
      "     0.21468321180639027\n",
      "     0.39643568909417815\n",
      "     0.4472420115098026\n",
      "     0.2487132057777922\n",
      "     0.34883929908515954\n",
      "     0.38697822534601933\n",
      "     0.401489065366079\n",
      "     0.3032287113454043\n",
      "     0.396841425812439\n",
      "\n",
      "Mean R2 Score for BayesianRidge Regression model is:  0.3445180111013618\n",
      "\n",
      "Running kfold valiations using Bayesian Ridge Regression:\n",
      "\n",
      "     0.8618244396040383\n",
      "     0.8937653259441449\n",
      "     0.8540440461876815\n",
      "     0.8627593452364226\n",
      "     0.8481823161156624\n",
      "     0.9052104663207335\n",
      "     0.9043936159658295\n",
      "     0.8355701389769017\n",
      "     0.890663747998973\n",
      "     0.9266055183619599\n",
      "\n",
      "Mean R2 Score for BayesianRidge Regression model is:  0.8783018960712348\n",
      "\n",
      "Running kfold valiations using Random Forest Regression:\n",
      "\n",
      "     0.8177097427462068\n",
      "     0.8567236635144684\n",
      "     0.8814571893517086\n",
      "     0.8749506621081191\n",
      "     0.852056141223601\n",
      "     0.916522813819299\n",
      "     0.8887953129525671\n",
      "     0.77995049326894\n",
      "     0.8710205725406519\n",
      "     0.8942960510676788\n",
      "\n",
      "Mean R2 Score for Random Forest Regression model is:  0.8633482642593242\n",
      "\n",
      "Running kfold validations using Gradient Boost Regression:\n",
      "\n",
      "     0.8283338442785109\n",
      "     0.8593091700096178\n",
      "     0.9016933968110532\n",
      "     0.8239269498719934\n",
      "     0.9137847903549984\n",
      "     0.9390886399264032\n",
      "     0.9167554066113233\n",
      "     0.8636874829295256\n",
      "     0.9255798727694955\n",
      "     0.9321633926121485\n",
      "\n",
      "Mean R2 Score for Gradient Boost Regression model is:  0.890432294617507\n"
     ]
    }
   ],
   "source": [
    "BestLRM, BestRFM, BestGBM, BestBRM = kfold_validations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1034 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 26.3min finished\n"
     ]
    }
   ],
   "source": [
    "RF_grid = param_tuning_RFM(BestRFM, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875790018299529"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 3000,\n",
       " 'random_state': 195877,\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Random Forest, above parameters will be selected for all of the work that follows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838704915285805"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105867735028106"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data.drop('SalePrice', axis = 1), test_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed: 24.1min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed: 38.4min\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1034 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1081 tasks      | elapsed: 43.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 45.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1177 tasks      | elapsed: 49.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed: 52.8min finished\n"
     ]
    }
   ],
   "source": [
    "GB_grid = param_tuning_GBM(BestGBM, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166425947247937"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.02,\n",
       " 'max_depth': 4,\n",
       " 'n_estimators': 3000,\n",
       " 'random_state': 195877,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Gradient Boost, above parameters will be selected for all of the work that follows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995578051899319"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9431867662711404"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data.drop('SalePrice', axis = 1), test_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.930168073268189"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899511265062054"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data.drop('SalePrice', axis = 1), test_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9250511886874816"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9057343497711716"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data.drop('SalePrice', axis = 1), test_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even though Both the models are overfitting the training data, if we compare the R2 score, Gradient Boost Regressor is performing much better than Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******************************** END OF PART 1 *************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2) BASIC EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape:  (1156, 199)\n",
      "test_data shape:  (289, 199)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of our train_data and test_data\n",
    "print(\"train_data shape: \", train_data.shape)\n",
    "print(\"test_data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 199 features, we will see what we can do with them as part of feature selection process\n",
    "\n",
    "### Initially there were 37 numerical and 43 categorical attributes, due to conversion of the categorical attributes and all the preprocessing\n",
    "\n",
    "### steps taken has increased the number of total features to 198 plus 1 target feature\n",
    "\n",
    "### we can check the correlation of the features with the target attribute (SalePrice)\n",
    "\n",
    "### We have 198 features, so it is not possible to check correlation all at once\n",
    "\n",
    "### We will make use of the dataset without outliers: train_df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',200)\n",
    "pd.set_option('display.max_rows',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix = train_df_no_outliers.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 199)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, corr_matrix.shape[0], 20):\n",
    "    print(corr_matrix.iloc[i:i+20]['SalePrice'])\n",
    "    print(\"__________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Part 1 : Let's get rid of attributes which are least correlated to the target attribute 'SalePrice' and check how our tuned model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's filter out the columns which are having positive & negative correlation with SalePrice\n",
    "neg_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] < 0].index.tolist()\n",
    "pos_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] > 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positively correlated attributes:  100\n",
      "Total number of negatively correlated attributes:  99\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of positively correlated attributes: \", len(pos_corr_features))\n",
    "print(\"Total number of negatively correlated attributes: \", len(neg_corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_corr_matrix = corr_matrix.loc[pos_corr_features]['SalePrice']\n",
    "neg_corr_matrix = corr_matrix.loc[neg_corr_features]['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's get rid of attributes which have correlation of less than 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features_to_use = pos_corr_matrix[pos_corr_matrix > 0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_features_to_use = neg_corr_matrix[neg_corr_matrix > -0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_features_to_use = pos_features_to_use + neg_features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 98)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[tot_features_to_use].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have reduced the number of features from 199 to 105, let's check how our models perform based on this set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9837336474478761"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9071690165101107"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994615195789084"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935054022173731"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) We have not used Gridsearch cv for Linear regression model, as it does not have any parameters to tune, lets check its performance on the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9152322018153768"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8989551646625593"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Bayesian Ridge Regression without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135306151569729"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044670946367023"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above scores when compared to the final scores from Part1, there is no much difference even after getting rid of some least correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Part 2 : Let's get rid of attributes which are Highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For regression, generally we should not be having high correlation between the features, it could bring down the model performance.\n",
    "\n",
    "#### Let's find out the features which are highly correlated (we set our threshold of correlation at 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "                  .stack()\n",
    "                  .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Exterior1st_CBlock', 'Exterior2nd_CBlock'] 1.0\n",
      "['SaleType_New', 'SaleCondition_Partial'] 0.9865105303450532\n",
      "['Exterior1st_VinylSd', 'Exterior2nd_VinylSd'] 0.9788636937329255\n",
      "['Exterior1st_CemntBd', 'Exterior2nd_CmentBd'] 0.9733158107377518\n",
      "['Exterior1st_MetalSd', 'Exterior2nd_MetalSd'] 0.9730165311004486\n",
      "['GarageQual', 'GarageCond'] 0.9569048397312127\n",
      "['PoolArea', 'PoolQC'] 0.8920478113756265\n",
      "['GarageCars', 'GarageArea'] 0.885709708781672\n",
      "['Exterior1st_HdBoard', 'Exterior2nd_HdBoard'] 0.8830637970191939\n",
      "['Fireplaces', 'FireplaceQu'] 0.8650844198512795\n",
      "['Neighborhood_Somerst', 'MSZoning_FV'] 0.8627354384774\n",
      "['Exterior1st_Wd Sdng', 'Exterior2nd_Wd Sdng'] 0.8584218635814506\n",
      "['Exterior1st_AsbShng', 'Exterior2nd_AsbShng'] 0.8400014763416322\n",
      "['GrLivArea', 'TotRmsAbvGrd'] 0.82976633433873\n",
      "['2ndFlrSF', 'HouseStyle_2Story'] 0.8128225370646163\n",
      "['TotalBsmtSF', '1stFlrSF'] 0.8021329602320946\n",
      "['OverallQual', 'SalePrice'] 0.7983352031814452\n",
      "['BsmtFinType2', 'BsmtFinSF2'] 0.7888786239898098\n",
      "['RoofStyle_Flat', 'RoofMatl_Tar&Grv'] 0.7813004037263575\n",
      "['Exterior1st_Stucco', 'Exterior2nd_Stucco'] 0.7518826535912014\n",
      "['Exterior1st_Plywood', 'Exterior2nd_Plywood'] 0.7489549002546407\n",
      "['GrLivArea', 'SalePrice'] 0.7416983601323051\n",
      "['OverallQual', 'ExterQual'] 0.7210280513186306\n",
      "['BsmtFinType1', 'BsmtFinSF1'] 0.7209413437041139\n",
      "['KitchenAbvGr', 'BldgType_Duplex'] 0.716043543533291\n",
      "['ExterQual', 'KitchenQual'] 0.7105647316824497\n"
     ]
    }
   ],
   "source": [
    "high_corr_features_list = []\n",
    "for k,v in sol.items():\n",
    "    #if v > 0.7 and 'SalePrice' not in list(k):\n",
    "    if v > 0.7:\n",
    "        print(list(k),v)\n",
    "        high_corr_features_list += list(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features_list = list(set(high_corr_features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_corr_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have the list of highly correlated features, let's check how they are correlated to SalePrice, so that we can decide which ones to drop from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exterior1st_MetalSd     -0.167732\n",
       "Exterior2nd_Wd Sdng     -0.163710\n",
       "Exterior2nd_MetalSd     -0.163023\n",
       "Exterior1st_Wd Sdng     -0.160460\n",
       "KitchenAbvGr            -0.138546\n",
       "BldgType_Duplex         -0.115225\n",
       "Exterior1st_AsbShng     -0.103203\n",
       "Exterior1st_HdBoard     -0.095319\n",
       "Exterior2nd_AsbShng     -0.092983\n",
       "Exterior2nd_HdBoard     -0.067863\n",
       "Exterior2nd_Plywood     -0.053568\n",
       "Exterior2nd_Stucco      -0.053414\n",
       "Exterior1st_Stucco      -0.040205\n",
       "Exterior2nd_CBlock      -0.025146\n",
       "Exterior1st_CBlock      -0.025146\n",
       "Exterior1st_Plywood     -0.019281\n",
       "BsmtFinSF2              -0.014242\n",
       "BsmtFinType2            -0.009364\n",
       "RoofMatl_Tar&Grv         0.002858\n",
       "RoofStyle_Flat           0.016215\n",
       "MSZoning_FV              0.090746\n",
       "PoolArea                 0.100170\n",
       "Exterior2nd_CmentBd      0.121737\n",
       "Exterior1st_CemntBd      0.127208\n",
       "Neighborhood_Somerst     0.141212\n",
       "PoolQC                   0.142680\n",
       "HouseStyle_2Story        0.246540\n",
       "GarageCond               0.257715\n",
       "GarageQual               0.268727\n",
       "BsmtFinType1             0.302737\n",
       "Exterior2nd_VinylSd      0.309657\n",
       "Exterior1st_VinylSd      0.310728\n",
       "2ndFlrSF                 0.323615\n",
       "SaleCondition_Partial    0.352472\n",
       "SaleType_New             0.358049\n",
       "BsmtFinSF1               0.406411\n",
       "Fireplaces               0.469589\n",
       "FireplaceQu              0.518742\n",
       "TotRmsAbvGrd             0.545918\n",
       "GarageArea               0.629915\n",
       "1stFlrSF                 0.630420\n",
       "GarageCars               0.642012\n",
       "TotalBsmtSF              0.649450\n",
       "KitchenQual              0.662734\n",
       "ExterQual                0.686945\n",
       "GrLivArea                0.741698\n",
       "OverallQual              0.798335\n",
       "SalePrice                1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of correlation of highly correlated features with SalePrice in ascending order\n",
    "train_df_no_outliers[high_corr_features_list].corrwith(train_df_no_outliers['SalePrice']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>Exterior2nd_HdBoard</th>\n",
       "      <th>Exterior2nd_VinylSd</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>Exterior1st_MetalSd</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Exterior2nd_Plywood</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Exterior1st_VinylSd</th>\n",
       "      <th>Exterior2nd_CmentBd</th>\n",
       "      <th>Exterior2nd_AsbShng</th>\n",
       "      <th>BldgType_Duplex</th>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.178668</td>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.812823</td>\n",
       "      <td>-0.026804</td>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.129945</td>\n",
       "      <td>-0.025236</td>\n",
       "      <td>0.043080</td>\n",
       "      <td>-0.164398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181763</td>\n",
       "      <td>0.075067</td>\n",
       "      <td>-0.112077</td>\n",
       "      <td>-0.234308</td>\n",
       "      <td>0.609899</td>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.037818</td>\n",
       "      <td>-0.051454</td>\n",
       "      <td>0.067351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExterQual</th>\n",
       "      <td>0.178668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>0.200797</td>\n",
       "      <td>-0.133262</td>\n",
       "      <td>0.423507</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>-0.133905</td>\n",
       "      <td>-0.143039</td>\n",
       "      <td>0.179136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>-0.154853</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.304495</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.155304</td>\n",
       "      <td>-0.080917</td>\n",
       "      <td>-0.139398</td>\n",
       "      <td>0.257266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071497</td>\n",
       "      <td>0.060153</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.552166</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.163793</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566530</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>-0.052273</td>\n",
       "      <td>-0.090686</td>\n",
       "      <td>-0.133518</td>\n",
       "      <td>0.064504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <td>0.812823</td>\n",
       "      <td>0.200797</td>\n",
       "      <td>0.071497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.174322</td>\n",
       "      <td>0.129556</td>\n",
       "      <td>-0.053769</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>-0.129116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>-0.089245</td>\n",
       "      <td>-0.295771</td>\n",
       "      <td>0.435397</td>\n",
       "      <td>0.173418</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>-0.023412</td>\n",
       "      <td>-0.050966</td>\n",
       "      <td>0.145477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_HdBoard</th>\n",
       "      <td>-0.026804</td>\n",
       "      <td>-0.133262</td>\n",
       "      <td>0.060153</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.298802</td>\n",
       "      <td>-0.033821</td>\n",
       "      <td>-0.156792</td>\n",
       "      <td>-0.022521</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048813</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.131803</td>\n",
       "      <td>-0.035825</td>\n",
       "      <td>-0.032464</td>\n",
       "      <td>-0.299244</td>\n",
       "      <td>-0.083618</td>\n",
       "      <td>-0.047200</td>\n",
       "      <td>-0.001758</td>\n",
       "      <td>-0.102864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_VinylSd</th>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.423507</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.174322</td>\n",
       "      <td>-0.298802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289762</td>\n",
       "      <td>-0.309672</td>\n",
       "      <td>-0.094290</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>-0.235536</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.130406</td>\n",
       "      <td>0.978864</td>\n",
       "      <td>-0.149429</td>\n",
       "      <td>-0.084348</td>\n",
       "      <td>-0.082703</td>\n",
       "      <td>0.129340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.129945</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.552166</td>\n",
       "      <td>0.129556</td>\n",
       "      <td>-0.033821</td>\n",
       "      <td>0.289762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122289</td>\n",
       "      <td>-0.062246</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.479365</td>\n",
       "      <td>0.335810</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>-0.083096</td>\n",
       "      <td>-0.037072</td>\n",
       "      <td>0.188097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_MetalSd</th>\n",
       "      <td>-0.025236</td>\n",
       "      <td>-0.133905</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.053769</td>\n",
       "      <td>-0.156792</td>\n",
       "      <td>-0.309672</td>\n",
       "      <td>-0.122289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>-0.049575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159473</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>-0.136598</td>\n",
       "      <td>-0.150834</td>\n",
       "      <td>-0.113004</td>\n",
       "      <td>-0.314408</td>\n",
       "      <td>-0.086660</td>\n",
       "      <td>-0.048917</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.080659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <td>0.043080</td>\n",
       "      <td>-0.143039</td>\n",
       "      <td>-0.163793</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>-0.022521</td>\n",
       "      <td>-0.094290</td>\n",
       "      <td>-0.062246</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.077644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046839</td>\n",
       "      <td>-0.011177</td>\n",
       "      <td>0.074724</td>\n",
       "      <td>0.077420</td>\n",
       "      <td>0.238617</td>\n",
       "      <td>-0.090217</td>\n",
       "      <td>-0.043381</td>\n",
       "      <td>0.116597</td>\n",
       "      <td>0.716044</td>\n",
       "      <td>-0.053365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>-0.164398</td>\n",
       "      <td>0.179136</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>-0.129116</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>-0.049575</td>\n",
       "      <td>-0.077644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230753</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>0.090818</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>-0.015376</td>\n",
       "      <td>0.065919</td>\n",
       "      <td>-0.069257</td>\n",
       "      <td>-0.019420</td>\n",
       "      <td>-0.064579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_MetalSd</th>\n",
       "      <td>-0.030653</td>\n",
       "      <td>-0.127243</td>\n",
       "      <td>-0.025346</td>\n",
       "      <td>-0.059372</td>\n",
       "      <td>-0.170491</td>\n",
       "      <td>-0.304675</td>\n",
       "      <td>-0.117415</td>\n",
       "      <td>0.973017</td>\n",
       "      <td>0.029189</td>\n",
       "      <td>-0.043051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151781</td>\n",
       "      <td>-0.021967</td>\n",
       "      <td>-0.134393</td>\n",
       "      <td>-0.145805</td>\n",
       "      <td>-0.111833</td>\n",
       "      <td>-0.309335</td>\n",
       "      <td>-0.085262</td>\n",
       "      <td>-0.048128</td>\n",
       "      <td>0.027665</td>\n",
       "      <td>0.084523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_AsbShng</th>\n",
       "      <td>0.023339</td>\n",
       "      <td>-0.080917</td>\n",
       "      <td>-0.125053</td>\n",
       "      <td>-0.023412</td>\n",
       "      <td>-0.047200</td>\n",
       "      <td>-0.084348</td>\n",
       "      <td>-0.087879</td>\n",
       "      <td>-0.048917</td>\n",
       "      <td>0.144813</td>\n",
       "      <td>-0.077743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087146</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>-0.063914</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>-0.085638</td>\n",
       "      <td>-0.023604</td>\n",
       "      <td>0.840001</td>\n",
       "      <td>0.044627</td>\n",
       "      <td>-0.029037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoofStyle_Flat</th>\n",
       "      <td>-0.036584</td>\n",
       "      <td>-0.029056</td>\n",
       "      <td>0.033211</td>\n",
       "      <td>-0.036994</td>\n",
       "      <td>-0.034135</td>\n",
       "      <td>-0.061000</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>-0.035377</td>\n",
       "      <td>-0.017709</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.154486</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.068603</td>\n",
       "      <td>-0.031511</td>\n",
       "      <td>-0.061933</td>\n",
       "      <td>-0.017071</td>\n",
       "      <td>-0.009636</td>\n",
       "      <td>-0.015804</td>\n",
       "      <td>-0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_CBlock</th>\n",
       "      <td>0.011555</td>\n",
       "      <td>-0.064888</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>-0.017384</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>-0.019230</td>\n",
       "      <td>-0.016849</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>-0.005583</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027350</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.008288</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-0.005381</td>\n",
       "      <td>-0.003038</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.006620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_Wd Sdng</th>\n",
       "      <td>-0.010557</td>\n",
       "      <td>-0.197525</td>\n",
       "      <td>-0.089284</td>\n",
       "      <td>-0.109026</td>\n",
       "      <td>-0.154938</td>\n",
       "      <td>-0.288788</td>\n",
       "      <td>-0.183059</td>\n",
       "      <td>-0.172310</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>-0.086832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194891</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>-0.076717</td>\n",
       "      <td>-0.054624</td>\n",
       "      <td>-0.022667</td>\n",
       "      <td>-0.301659</td>\n",
       "      <td>-0.083146</td>\n",
       "      <td>-0.029522</td>\n",
       "      <td>-0.044422</td>\n",
       "      <td>-0.102284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>-0.105464</td>\n",
       "      <td>-0.074455</td>\n",
       "      <td>0.049236</td>\n",
       "      <td>-0.094258</td>\n",
       "      <td>0.032474</td>\n",
       "      <td>-0.121836</td>\n",
       "      <td>-0.021451</td>\n",
       "      <td>-0.031355</td>\n",
       "      <td>-0.043642</td>\n",
       "      <td>-0.047736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043377</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>0.115991</td>\n",
       "      <td>0.094573</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>-0.113160</td>\n",
       "      <td>-0.021761</td>\n",
       "      <td>-0.027944</td>\n",
       "      <td>-0.021487</td>\n",
       "      <td>-0.071866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SaleType_New</th>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>-0.122497</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>0.289589</td>\n",
       "      <td>-0.084911</td>\n",
       "      <td>-0.040166</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287024</td>\n",
       "      <td>-0.015783</td>\n",
       "      <td>-0.096561</td>\n",
       "      <td>0.197897</td>\n",
       "      <td>0.141716</td>\n",
       "      <td>0.288047</td>\n",
       "      <td>0.118295</td>\n",
       "      <td>-0.034580</td>\n",
       "      <td>-0.056715</td>\n",
       "      <td>0.265137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_Stucco</th>\n",
       "      <td>0.046022</td>\n",
       "      <td>-0.079395</td>\n",
       "      <td>-0.029948</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>-0.052004</td>\n",
       "      <td>-0.092934</td>\n",
       "      <td>-0.042111</td>\n",
       "      <td>-0.038505</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>-0.046361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042563</td>\n",
       "      <td>-0.006701</td>\n",
       "      <td>-0.040993</td>\n",
       "      <td>-0.039656</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>-0.082800</td>\n",
       "      <td>-0.026007</td>\n",
       "      <td>-0.014680</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>-0.031993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.375732</td>\n",
       "      <td>0.060266</td>\n",
       "      <td>0.037537</td>\n",
       "      <td>-0.124172</td>\n",
       "      <td>0.290164</td>\n",
       "      <td>0.287666</td>\n",
       "      <td>-0.087120</td>\n",
       "      <td>-0.041298</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>-0.097881</td>\n",
       "      <td>0.198149</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>0.289669</td>\n",
       "      <td>0.115437</td>\n",
       "      <td>-0.035052</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>0.270796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_Wd Sdng</th>\n",
       "      <td>-0.025419</td>\n",
       "      <td>-0.199229</td>\n",
       "      <td>-0.069920</td>\n",
       "      <td>-0.125451</td>\n",
       "      <td>-0.161984</td>\n",
       "      <td>-0.289471</td>\n",
       "      <td>-0.180165</td>\n",
       "      <td>-0.156625</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>-0.095809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193339</td>\n",
       "      <td>-0.020871</td>\n",
       "      <td>-0.127687</td>\n",
       "      <td>-0.060662</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>-0.289675</td>\n",
       "      <td>-0.081007</td>\n",
       "      <td>-0.045726</td>\n",
       "      <td>-0.041822</td>\n",
       "      <td>-0.099652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoofMatl_Tar&amp;Grv</th>\n",
       "      <td>-0.026060</td>\n",
       "      <td>-0.019386</td>\n",
       "      <td>0.032322</td>\n",
       "      <td>-0.029009</td>\n",
       "      <td>-0.030510</td>\n",
       "      <td>-0.054522</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>-0.031620</td>\n",
       "      <td>-0.015828</td>\n",
       "      <td>-0.001473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001970</td>\n",
       "      <td>0.173583</td>\n",
       "      <td>0.103716</td>\n",
       "      <td>0.080244</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.055356</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>-0.008613</td>\n",
       "      <td>-0.014126</td>\n",
       "      <td>-0.018770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>-0.211421</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>0.174370</td>\n",
       "      <td>-0.206101</td>\n",
       "      <td>-0.036251</td>\n",
       "      <td>0.190750</td>\n",
       "      <td>0.475503</td>\n",
       "      <td>-0.129118</td>\n",
       "      <td>-0.070498</td>\n",
       "      <td>0.467596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452154</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.802133</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>0.197124</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>-0.074672</td>\n",
       "      <td>-0.046195</td>\n",
       "      <td>0.049440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OverallQual</th>\n",
       "      <td>0.291839</td>\n",
       "      <td>0.721028</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.276341</td>\n",
       "      <td>-0.069074</td>\n",
       "      <td>0.357469</td>\n",
       "      <td>0.555957</td>\n",
       "      <td>-0.176040</td>\n",
       "      <td>-0.184530</td>\n",
       "      <td>0.220542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601333</td>\n",
       "      <td>0.050585</td>\n",
       "      <td>-0.099870</td>\n",
       "      <td>0.472316</td>\n",
       "      <td>0.428587</td>\n",
       "      <td>0.356906</td>\n",
       "      <td>0.108425</td>\n",
       "      <td>-0.114917</td>\n",
       "      <td>-0.160411</td>\n",
       "      <td>0.226938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSZoning_FV</th>\n",
       "      <td>0.117932</td>\n",
       "      <td>0.213183</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.197856</td>\n",
       "      <td>-0.088745</td>\n",
       "      <td>0.079719</td>\n",
       "      <td>0.117066</td>\n",
       "      <td>0.121784</td>\n",
       "      <td>-0.046040</td>\n",
       "      <td>-0.055365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103619</td>\n",
       "      <td>-0.011434</td>\n",
       "      <td>-0.069955</td>\n",
       "      <td>-0.097043</td>\n",
       "      <td>-0.043308</td>\n",
       "      <td>0.076222</td>\n",
       "      <td>0.091704</td>\n",
       "      <td>-0.025052</td>\n",
       "      <td>-0.041088</td>\n",
       "      <td>0.862735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KitchenQual</th>\n",
       "      <td>0.169642</td>\n",
       "      <td>0.710565</td>\n",
       "      <td>0.219205</td>\n",
       "      <td>0.179499</td>\n",
       "      <td>-0.102842</td>\n",
       "      <td>0.363263</td>\n",
       "      <td>0.478443</td>\n",
       "      <td>-0.151974</td>\n",
       "      <td>-0.165371</td>\n",
       "      <td>0.214288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502264</td>\n",
       "      <td>0.045578</td>\n",
       "      <td>-0.139731</td>\n",
       "      <td>0.380196</td>\n",
       "      <td>0.293048</td>\n",
       "      <td>0.367920</td>\n",
       "      <td>0.151032</td>\n",
       "      <td>-0.080760</td>\n",
       "      <td>-0.153332</td>\n",
       "      <td>0.212438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_Plywood</th>\n",
       "      <td>-0.079335</td>\n",
       "      <td>-0.123008</td>\n",
       "      <td>0.044411</td>\n",
       "      <td>-0.048496</td>\n",
       "      <td>-0.097928</td>\n",
       "      <td>-0.202442</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>-0.117405</td>\n",
       "      <td>0.078646</td>\n",
       "      <td>0.114373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.104854</td>\n",
       "      <td>0.748955</td>\n",
       "      <td>0.122043</td>\n",
       "      <td>-0.003181</td>\n",
       "      <td>-0.205538</td>\n",
       "      <td>-0.056652</td>\n",
       "      <td>-0.031979</td>\n",
       "      <td>0.138864</td>\n",
       "      <td>-0.069692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>0.049823</td>\n",
       "      <td>0.194120</td>\n",
       "      <td>0.956905</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>0.539914</td>\n",
       "      <td>-0.037787</td>\n",
       "      <td>-0.182411</td>\n",
       "      <td>0.140393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557951</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.171431</td>\n",
       "      <td>0.086678</td>\n",
       "      <td>0.085738</td>\n",
       "      <td>-0.052116</td>\n",
       "      <td>-0.108148</td>\n",
       "      <td>-0.133772</td>\n",
       "      <td>0.065302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>-0.116931</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.140547</td>\n",
       "      <td>-0.036087</td>\n",
       "      <td>0.098097</td>\n",
       "      <td>0.084699</td>\n",
       "      <td>0.208212</td>\n",
       "      <td>-0.064520</td>\n",
       "      <td>-0.161613</td>\n",
       "      <td>0.720941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191738</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.050792</td>\n",
       "      <td>0.205111</td>\n",
       "      <td>-0.066478</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.069486</td>\n",
       "      <td>-0.084614</td>\n",
       "      <td>-0.095615</td>\n",
       "      <td>0.010115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>0.206390</td>\n",
       "      <td>0.356743</td>\n",
       "      <td>0.215193</td>\n",
       "      <td>0.124520</td>\n",
       "      <td>-0.048590</td>\n",
       "      <td>0.094986</td>\n",
       "      <td>0.320215</td>\n",
       "      <td>-0.130114</td>\n",
       "      <td>-0.158298</td>\n",
       "      <td>0.172252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364798</td>\n",
       "      <td>0.024713</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.359216</td>\n",
       "      <td>0.091210</td>\n",
       "      <td>0.055615</td>\n",
       "      <td>-0.069076</td>\n",
       "      <td>-0.171610</td>\n",
       "      <td>-0.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GrLivArea</th>\n",
       "      <td>0.692361</td>\n",
       "      <td>0.438275</td>\n",
       "      <td>0.176173</td>\n",
       "      <td>0.476979</td>\n",
       "      <td>-0.052047</td>\n",
       "      <td>0.152119</td>\n",
       "      <td>0.461256</td>\n",
       "      <td>-0.126943</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.143806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482801</td>\n",
       "      <td>0.121214</td>\n",
       "      <td>-0.011279</td>\n",
       "      <td>0.534098</td>\n",
       "      <td>0.829766</td>\n",
       "      <td>0.153567</td>\n",
       "      <td>0.055804</td>\n",
       "      <td>-0.013194</td>\n",
       "      <td>0.019470</td>\n",
       "      <td>0.045212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>-0.068773</td>\n",
       "      <td>-0.062156</td>\n",
       "      <td>0.048073</td>\n",
       "      <td>-0.060090</td>\n",
       "      <td>0.048809</td>\n",
       "      <td>-0.099884</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>-0.018042</td>\n",
       "      <td>-0.094401</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029939</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>0.064446</td>\n",
       "      <td>0.037791</td>\n",
       "      <td>-0.047104</td>\n",
       "      <td>-0.093587</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>-0.031599</td>\n",
       "      <td>-0.077504</td>\n",
       "      <td>-0.068865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalePrice</th>\n",
       "      <td>0.323615</td>\n",
       "      <td>0.686945</td>\n",
       "      <td>0.268727</td>\n",
       "      <td>0.246540</td>\n",
       "      <td>-0.067863</td>\n",
       "      <td>0.309657</td>\n",
       "      <td>0.629915</td>\n",
       "      <td>-0.167732</td>\n",
       "      <td>-0.138546</td>\n",
       "      <td>0.406411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642012</td>\n",
       "      <td>0.142680</td>\n",
       "      <td>-0.053568</td>\n",
       "      <td>0.630420</td>\n",
       "      <td>0.545918</td>\n",
       "      <td>0.310728</td>\n",
       "      <td>0.121737</td>\n",
       "      <td>-0.092983</td>\n",
       "      <td>-0.115225</td>\n",
       "      <td>0.141212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolArea</th>\n",
       "      <td>0.057196</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>-0.042709</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>-0.024769</td>\n",
       "      <td>-0.012399</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>0.892048</td>\n",
       "      <td>0.102011</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>0.055752</td>\n",
       "      <td>-0.043362</td>\n",
       "      <td>-0.011952</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>-0.011065</td>\n",
       "      <td>-0.014703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_Stucco</th>\n",
       "      <td>0.027434</td>\n",
       "      <td>-0.057241</td>\n",
       "      <td>-0.032060</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>-0.050843</td>\n",
       "      <td>-0.090859</td>\n",
       "      <td>-0.058162</td>\n",
       "      <td>-0.052693</td>\n",
       "      <td>0.026123</td>\n",
       "      <td>-0.064064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052896</td>\n",
       "      <td>-0.006551</td>\n",
       "      <td>-0.040078</td>\n",
       "      <td>-0.026566</td>\n",
       "      <td>0.031504</td>\n",
       "      <td>-0.092249</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>-0.014352</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>-0.031279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_CBlock</th>\n",
       "      <td>0.011555</td>\n",
       "      <td>-0.064888</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>-0.017384</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>-0.019230</td>\n",
       "      <td>-0.016849</td>\n",
       "      <td>-0.011152</td>\n",
       "      <td>-0.005583</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027350</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.008288</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-0.005381</td>\n",
       "      <td>-0.003038</td>\n",
       "      <td>-0.004982</td>\n",
       "      <td>-0.006620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_HdBoard</th>\n",
       "      <td>-0.038078</td>\n",
       "      <td>-0.149648</td>\n",
       "      <td>0.057680</td>\n",
       "      <td>0.019008</td>\n",
       "      <td>0.883064</td>\n",
       "      <td>-0.311331</td>\n",
       "      <td>-0.052964</td>\n",
       "      <td>-0.180554</td>\n",
       "      <td>-0.019073</td>\n",
       "      <td>0.072987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064382</td>\n",
       "      <td>-0.010271</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.073060</td>\n",
       "      <td>-0.062203</td>\n",
       "      <td>-0.316092</td>\n",
       "      <td>-0.087124</td>\n",
       "      <td>-0.049179</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>-0.107177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fireplaces</th>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.245043</td>\n",
       "      <td>0.205730</td>\n",
       "      <td>0.119068</td>\n",
       "      <td>-0.006364</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>0.258036</td>\n",
       "      <td>-0.117300</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>0.240740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299017</td>\n",
       "      <td>0.066477</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>0.397684</td>\n",
       "      <td>0.327566</td>\n",
       "      <td>-0.000938</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>-0.062364</td>\n",
       "      <td>-0.144626</td>\n",
       "      <td>-0.056544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_CemntBd</th>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.151586</td>\n",
       "      <td>-0.050950</td>\n",
       "      <td>0.038592</td>\n",
       "      <td>-0.084366</td>\n",
       "      <td>-0.150766</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>-0.087435</td>\n",
       "      <td>-0.043769</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>-0.066503</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.048698</td>\n",
       "      <td>-0.153072</td>\n",
       "      <td>0.973316</td>\n",
       "      <td>-0.023816</td>\n",
       "      <td>-0.039061</td>\n",
       "      <td>0.110688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.181763</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.566530</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>-0.048813</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>-0.159473</td>\n",
       "      <td>-0.046839</td>\n",
       "      <td>0.230753</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>0.370888</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>0.030312</td>\n",
       "      <td>-0.087146</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>0.169151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>0.075067</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>-0.011177</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088273</td>\n",
       "      <td>0.077349</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>-0.039088</td>\n",
       "      <td>-0.010774</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>-0.009975</td>\n",
       "      <td>-0.013254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_Plywood</th>\n",
       "      <td>-0.112077</td>\n",
       "      <td>-0.154853</td>\n",
       "      <td>0.008912</td>\n",
       "      <td>-0.089245</td>\n",
       "      <td>-0.131803</td>\n",
       "      <td>-0.235536</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>-0.136598</td>\n",
       "      <td>0.074724</td>\n",
       "      <td>0.090818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>0.088273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116972</td>\n",
       "      <td>-0.031882</td>\n",
       "      <td>-0.229233</td>\n",
       "      <td>-0.065914</td>\n",
       "      <td>-0.037206</td>\n",
       "      <td>0.120522</td>\n",
       "      <td>-0.081085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1stFlrSF</th>\n",
       "      <td>-0.234308</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>-0.295771</td>\n",
       "      <td>-0.035825</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.479365</td>\n",
       "      <td>-0.150834</td>\n",
       "      <td>0.077420</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>0.077349</td>\n",
       "      <td>0.116972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>0.086330</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>-0.060506</td>\n",
       "      <td>0.088978</td>\n",
       "      <td>-0.015909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <td>0.609899</td>\n",
       "      <td>0.304495</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.435397</td>\n",
       "      <td>-0.032464</td>\n",
       "      <td>0.130406</td>\n",
       "      <td>0.335810</td>\n",
       "      <td>-0.113004</td>\n",
       "      <td>0.238617</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370888</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>-0.031882</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128881</td>\n",
       "      <td>0.045801</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.139211</td>\n",
       "      <td>-0.013399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_VinylSd</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>0.173418</td>\n",
       "      <td>-0.299244</td>\n",
       "      <td>0.978864</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>-0.314408</td>\n",
       "      <td>-0.090217</td>\n",
       "      <td>-0.015376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>-0.039088</td>\n",
       "      <td>-0.229233</td>\n",
       "      <td>0.086330</td>\n",
       "      <td>0.128881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151714</td>\n",
       "      <td>-0.072942</td>\n",
       "      <td>-0.085069</td>\n",
       "      <td>0.131233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_CmentBd</th>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.155304</td>\n",
       "      <td>-0.052273</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>-0.083618</td>\n",
       "      <td>-0.149429</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>-0.086660</td>\n",
       "      <td>-0.043381</td>\n",
       "      <td>0.065919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030312</td>\n",
       "      <td>-0.010774</td>\n",
       "      <td>-0.065914</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.045801</td>\n",
       "      <td>-0.151714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023604</td>\n",
       "      <td>-0.038715</td>\n",
       "      <td>0.112485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_AsbShng</th>\n",
       "      <td>0.037818</td>\n",
       "      <td>-0.080917</td>\n",
       "      <td>-0.090686</td>\n",
       "      <td>-0.023412</td>\n",
       "      <td>-0.047200</td>\n",
       "      <td>-0.084348</td>\n",
       "      <td>-0.083096</td>\n",
       "      <td>-0.048917</td>\n",
       "      <td>0.116597</td>\n",
       "      <td>-0.069257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087146</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>-0.037206</td>\n",
       "      <td>-0.060506</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>-0.072942</td>\n",
       "      <td>-0.023604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>-0.029037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BldgType_Duplex</th>\n",
       "      <td>-0.051454</td>\n",
       "      <td>-0.139398</td>\n",
       "      <td>-0.133518</td>\n",
       "      <td>-0.050966</td>\n",
       "      <td>-0.001758</td>\n",
       "      <td>-0.082703</td>\n",
       "      <td>-0.037072</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.716044</td>\n",
       "      <td>-0.019420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.009975</td>\n",
       "      <td>0.120522</td>\n",
       "      <td>0.088978</td>\n",
       "      <td>0.139211</td>\n",
       "      <td>-0.085069</td>\n",
       "      <td>-0.038715</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "      <td>0.067351</td>\n",
       "      <td>0.257266</td>\n",
       "      <td>0.064504</td>\n",
       "      <td>0.145477</td>\n",
       "      <td>-0.102864</td>\n",
       "      <td>0.129340</td>\n",
       "      <td>0.188097</td>\n",
       "      <td>0.080659</td>\n",
       "      <td>-0.053365</td>\n",
       "      <td>-0.064579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169151</td>\n",
       "      <td>-0.013254</td>\n",
       "      <td>-0.081085</td>\n",
       "      <td>-0.015909</td>\n",
       "      <td>-0.013399</td>\n",
       "      <td>0.131233</td>\n",
       "      <td>0.112485</td>\n",
       "      <td>-0.029037</td>\n",
       "      <td>-0.047625</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       2ndFlrSF  ExterQual  GarageQual  HouseStyle_2Story  \\\n",
       "2ndFlrSF               1.000000   0.178668    0.060015           0.812823   \n",
       "ExterQual              0.178668   1.000000    0.198751           0.200797   \n",
       "GarageQual             0.060015   0.198751    1.000000           0.071497   \n",
       "HouseStyle_2Story      0.812823   0.200797    0.071497           1.000000   \n",
       "Exterior2nd_HdBoard   -0.026804  -0.133262    0.060153           0.021958   \n",
       "Exterior2nd_VinylSd    0.111925   0.423507    0.084648           0.174322   \n",
       "GarageArea             0.129945   0.483553    0.552166           0.129556   \n",
       "Exterior1st_MetalSd   -0.025236  -0.133905   -0.041164          -0.053769   \n",
       "KitchenAbvGr           0.043080  -0.143039   -0.163793           0.006631   \n",
       "BsmtFinSF1            -0.164398   0.179136    0.139302          -0.129116   \n",
       "Exterior2nd_MetalSd   -0.030653  -0.127243   -0.025346          -0.059372   \n",
       "Exterior1st_AsbShng    0.023339  -0.080917   -0.125053          -0.023412   \n",
       "RoofStyle_Flat        -0.036584  -0.029056    0.033211          -0.036994   \n",
       "Exterior1st_CBlock     0.011555  -0.064888    0.006748          -0.017384   \n",
       "Exterior1st_Wd Sdng   -0.010557  -0.197525   -0.089284          -0.109026   \n",
       "BsmtFinSF2            -0.105464  -0.074455    0.049236          -0.094258   \n",
       "SaleType_New           0.004830   0.380859    0.059015           0.042953   \n",
       "Exterior2nd_Stucco     0.046022  -0.079395   -0.029948           0.012173   \n",
       "SaleCondition_Partial -0.001155   0.375732    0.060266           0.037537   \n",
       "Exterior2nd_Wd Sdng   -0.025419  -0.199229   -0.069920          -0.125451   \n",
       "RoofMatl_Tar&Grv      -0.026060  -0.019386    0.032322          -0.029009   \n",
       "TotalBsmtSF           -0.211421   0.468850    0.174370          -0.206101   \n",
       "OverallQual            0.291839   0.721028    0.284600           0.276341   \n",
       "MSZoning_FV            0.117932   0.213183    0.055650           0.197856   \n",
       "KitchenQual            0.169642   0.710565    0.219205           0.179499   \n",
       "Exterior1st_Plywood   -0.079335  -0.123008    0.044411          -0.048496   \n",
       "GarageCond             0.049823   0.194120    0.956905           0.060303   \n",
       "BsmtFinType1          -0.116931   0.218218    0.140547          -0.036087   \n",
       "FireplaceQu            0.206390   0.356743    0.215193           0.124520   \n",
       "GrLivArea              0.692361   0.438275    0.176173           0.476979   \n",
       "BsmtFinType2          -0.068773  -0.062156    0.048073          -0.060090   \n",
       "SalePrice              0.323615   0.686945    0.268727           0.246540   \n",
       "PoolArea               0.057196   0.002956    0.035079           0.006348   \n",
       "Exterior1st_Stucco     0.027434  -0.057241   -0.032060           0.003887   \n",
       "Exterior2nd_CBlock     0.011555  -0.064888    0.006748          -0.017384   \n",
       "Exterior1st_HdBoard   -0.038078  -0.149648    0.057680           0.019008   \n",
       "Fireplaces             0.197681   0.245043    0.205730           0.119068   \n",
       "Exterior1st_CemntBd    0.021326   0.151586   -0.050950           0.038592   \n",
       "GarageCars             0.181763   0.520003    0.566530           0.199497   \n",
       "PoolQC                 0.075067   0.017303    0.032137           0.012929   \n",
       "Exterior2nd_Plywood   -0.112077  -0.154853    0.008912          -0.089245   \n",
       "1stFlrSF              -0.234308   0.389984    0.176948          -0.295771   \n",
       "TotRmsAbvGrd           0.609899   0.304495    0.097894           0.435397   \n",
       "Exterior1st_VinylSd    0.111846   0.429330    0.094112           0.173418   \n",
       "Exterior2nd_CmentBd    0.011364   0.155304   -0.052273           0.033572   \n",
       "Exterior2nd_AsbShng    0.037818  -0.080917   -0.090686          -0.023412   \n",
       "BldgType_Duplex       -0.051454  -0.139398   -0.133518          -0.050966   \n",
       "Neighborhood_Somerst   0.067351   0.257266    0.064504           0.145477   \n",
       "\n",
       "                       Exterior2nd_HdBoard  Exterior2nd_VinylSd  GarageArea  \\\n",
       "2ndFlrSF                         -0.026804             0.111925    0.129945   \n",
       "ExterQual                        -0.133262             0.423507    0.483553   \n",
       "GarageQual                        0.060153             0.084648    0.552166   \n",
       "HouseStyle_2Story                 0.021958             0.174322    0.129556   \n",
       "Exterior2nd_HdBoard               1.000000            -0.298802   -0.033821   \n",
       "Exterior2nd_VinylSd              -0.298802             1.000000    0.289762   \n",
       "GarageArea                       -0.033821             0.289762    1.000000   \n",
       "Exterior1st_MetalSd              -0.156792            -0.309672   -0.122289   \n",
       "KitchenAbvGr                     -0.022521            -0.094290   -0.062246   \n",
       "BsmtFinSF1                        0.085222            -0.016989    0.273692   \n",
       "Exterior2nd_MetalSd              -0.170491            -0.304675   -0.117415   \n",
       "Exterior1st_AsbShng              -0.047200            -0.084348   -0.087879   \n",
       "RoofStyle_Flat                   -0.034135            -0.061000    0.018735   \n",
       "Exterior1st_CBlock               -0.010761            -0.019230   -0.016849   \n",
       "Exterior1st_Wd Sdng              -0.154938            -0.288788   -0.183059   \n",
       "BsmtFinSF2                        0.032474            -0.121836   -0.021451   \n",
       "SaleType_New                     -0.122497             0.288416    0.289589   \n",
       "Exterior2nd_Stucco               -0.052004            -0.092934   -0.042111   \n",
       "SaleCondition_Partial            -0.124172             0.290164    0.287666   \n",
       "Exterior2nd_Wd Sdng              -0.161984            -0.289471   -0.180165   \n",
       "RoofMatl_Tar&Grv                 -0.030510            -0.054522    0.007614   \n",
       "TotalBsmtSF                      -0.036251             0.190750    0.475503   \n",
       "OverallQual                      -0.069074             0.357469    0.555957   \n",
       "MSZoning_FV                      -0.088745             0.079719    0.117066   \n",
       "KitchenQual                      -0.102842             0.363263    0.478443   \n",
       "Exterior1st_Plywood              -0.097928            -0.202442    0.038000   \n",
       "GarageCond                        0.061239             0.076162    0.539914   \n",
       "BsmtFinType1                      0.098097             0.084699    0.208212   \n",
       "FireplaceQu                      -0.048590             0.094986    0.320215   \n",
       "GrLivArea                        -0.052047             0.152119    0.461256   \n",
       "BsmtFinType2                      0.048809            -0.099884   -0.001615   \n",
       "SalePrice                        -0.067863             0.309657    0.629915   \n",
       "PoolArea                          0.011709            -0.042709    0.013387   \n",
       "Exterior1st_Stucco               -0.050843            -0.090859   -0.058162   \n",
       "Exterior2nd_CBlock               -0.010761            -0.019230   -0.016849   \n",
       "Exterior1st_HdBoard               0.883064            -0.311331   -0.052964   \n",
       "Fireplaces                       -0.006364             0.003801    0.258036   \n",
       "Exterior1st_CemntBd              -0.084366            -0.150766    0.043771   \n",
       "GarageCars                       -0.048813             0.329445    0.885710   \n",
       "PoolQC                           -0.009011            -0.038500    0.020704   \n",
       "Exterior2nd_Plywood              -0.131803            -0.235536    0.004364   \n",
       "1stFlrSF                         -0.035825             0.084123    0.479365   \n",
       "TotRmsAbvGrd                     -0.032464             0.130406    0.335810   \n",
       "Exterior1st_VinylSd              -0.299244             0.978864    0.291506   \n",
       "Exterior2nd_CmentBd              -0.083618            -0.149429    0.040819   \n",
       "Exterior2nd_AsbShng              -0.047200            -0.084348   -0.083096   \n",
       "BldgType_Duplex                  -0.001758            -0.082703   -0.037072   \n",
       "Neighborhood_Somerst             -0.102864             0.129340    0.188097   \n",
       "\n",
       "                       Exterior1st_MetalSd  KitchenAbvGr  BsmtFinSF1  ...  \\\n",
       "2ndFlrSF                         -0.025236      0.043080   -0.164398  ...   \n",
       "ExterQual                        -0.133905     -0.143039    0.179136  ...   \n",
       "GarageQual                       -0.041164     -0.163793    0.139302  ...   \n",
       "HouseStyle_2Story                -0.053769      0.006631   -0.129116  ...   \n",
       "Exterior2nd_HdBoard              -0.156792     -0.022521    0.085222  ...   \n",
       "Exterior2nd_VinylSd              -0.309672     -0.094290   -0.016989  ...   \n",
       "GarageArea                       -0.122289     -0.062246    0.273692  ...   \n",
       "Exterior1st_MetalSd               1.000000      0.035354   -0.049575  ...   \n",
       "KitchenAbvGr                      0.035354      1.000000   -0.077644  ...   \n",
       "BsmtFinSF1                       -0.049575     -0.077644    1.000000  ...   \n",
       "Exterior2nd_MetalSd               0.973017      0.029189   -0.043051  ...   \n",
       "Exterior1st_AsbShng              -0.048917      0.144813   -0.077743  ...   \n",
       "RoofStyle_Flat                   -0.035377     -0.017709    0.005371  ...   \n",
       "Exterior1st_CBlock               -0.011152     -0.005583   -0.007499  ...   \n",
       "Exterior1st_Wd Sdng              -0.172310      0.005865   -0.086832  ...   \n",
       "BsmtFinSF2                       -0.031355     -0.043642   -0.047736  ...   \n",
       "SaleType_New                     -0.084911     -0.040166    0.005149  ...   \n",
       "Exterior2nd_Stucco               -0.038505      0.024385   -0.046361  ...   \n",
       "SaleCondition_Partial            -0.087120     -0.041298    0.005748  ...   \n",
       "Exterior2nd_Wd Sdng              -0.156625      0.000448   -0.095809  ...   \n",
       "RoofMatl_Tar&Grv                 -0.031620     -0.015828   -0.001473  ...   \n",
       "TotalBsmtSF                      -0.129118     -0.070498    0.467596  ...   \n",
       "OverallQual                      -0.176040     -0.184530    0.220542  ...   \n",
       "MSZoning_FV                       0.121784     -0.046040   -0.055365  ...   \n",
       "KitchenQual                      -0.151974     -0.165371    0.214288  ...   \n",
       "Exterior1st_Plywood              -0.117405      0.078646    0.114373  ...   \n",
       "GarageCond                       -0.037787     -0.182411    0.140393  ...   \n",
       "BsmtFinType1                     -0.064520     -0.161613    0.720941  ...   \n",
       "FireplaceQu                      -0.130114     -0.158298    0.172252  ...   \n",
       "GrLivArea                        -0.126943      0.092150    0.143806  ...   \n",
       "BsmtFinType2                     -0.018042     -0.094401   -0.012698  ...   \n",
       "SalePrice                        -0.167732     -0.138546    0.406411  ...   \n",
       "PoolArea                         -0.024769     -0.012399    0.043421  ...   \n",
       "Exterior1st_Stucco               -0.052693      0.026123   -0.064064  ...   \n",
       "Exterior2nd_CBlock               -0.011152     -0.005583   -0.007499  ...   \n",
       "Exterior1st_HdBoard              -0.180554     -0.019073    0.072987  ...   \n",
       "Fireplaces                       -0.117300     -0.126506    0.240740  ...   \n",
       "Exterior1st_CemntBd              -0.087435     -0.043769    0.081306  ...   \n",
       "GarageCars                       -0.159473     -0.046839    0.230753  ...   \n",
       "PoolQC                           -0.022328     -0.011177    0.062512  ...   \n",
       "Exterior2nd_Plywood              -0.136598      0.074724    0.090818  ...   \n",
       "1stFlrSF                         -0.150834      0.077420    0.395313  ...   \n",
       "TotRmsAbvGrd                     -0.113004      0.238617    0.014037  ...   \n",
       "Exterior1st_VinylSd              -0.314408     -0.090217   -0.015376  ...   \n",
       "Exterior2nd_CmentBd              -0.086660     -0.043381    0.065919  ...   \n",
       "Exterior2nd_AsbShng              -0.048917      0.116597   -0.069257  ...   \n",
       "BldgType_Duplex                   0.035703      0.716044   -0.019420  ...   \n",
       "Neighborhood_Somerst              0.080659     -0.053365   -0.064579  ...   \n",
       "\n",
       "                       GarageCars    PoolQC  Exterior2nd_Plywood  1stFlrSF  \\\n",
       "2ndFlrSF                 0.181763  0.075067            -0.112077 -0.234308   \n",
       "ExterQual                0.520003  0.017303            -0.154853  0.389984   \n",
       "GarageQual               0.566530  0.032137             0.008912  0.176948   \n",
       "HouseStyle_2Story        0.199497  0.012929            -0.089245 -0.295771   \n",
       "Exterior2nd_HdBoard     -0.048813 -0.009011            -0.131803 -0.035825   \n",
       "Exterior2nd_VinylSd      0.329445 -0.038500            -0.235536  0.084123   \n",
       "GarageArea               0.885710  0.020704             0.004364  0.479365   \n",
       "Exterior1st_MetalSd     -0.159473 -0.022328            -0.136598 -0.150834   \n",
       "KitchenAbvGr            -0.046839 -0.011177             0.074724  0.077420   \n",
       "BsmtFinSF1               0.230753  0.062512             0.090818  0.395313   \n",
       "Exterior2nd_MetalSd     -0.151781 -0.021967            -0.134393 -0.145805   \n",
       "Exterior1st_AsbShng     -0.087146 -0.006082             0.004406 -0.063914   \n",
       "RoofStyle_Flat           0.014707  0.154486             0.144628  0.068603   \n",
       "Exterior1st_CBlock      -0.027350 -0.001386            -0.008482 -0.022529   \n",
       "Exterior1st_Wd Sdng     -0.194891  0.028911            -0.076717 -0.054624   \n",
       "BsmtFinSF2              -0.043377  0.008256             0.115991  0.094573   \n",
       "SaleType_New             0.287024 -0.015783            -0.096561  0.197897   \n",
       "Exterior2nd_Stucco      -0.042563 -0.006701            -0.040993 -0.039656   \n",
       "SaleCondition_Partial    0.282757 -0.015999            -0.097881  0.198149   \n",
       "Exterior2nd_Wd Sdng     -0.193339 -0.020871            -0.127687 -0.060662   \n",
       "RoofMatl_Tar&Grv        -0.001970  0.173583             0.103716  0.080244   \n",
       "TotalBsmtSF              0.452154  0.050641             0.034548  0.802133   \n",
       "OverallQual              0.601333  0.050585            -0.099870  0.472316   \n",
       "MSZoning_FV              0.103619 -0.011434            -0.069955 -0.097043   \n",
       "KitchenQual              0.502264  0.045578            -0.139731  0.380196   \n",
       "Exterior1st_Plywood      0.042632  0.104854             0.748955  0.122043   \n",
       "GarageCond               0.557951  0.032392             0.012955  0.171431   \n",
       "BsmtFinType1             0.191738  0.017687             0.050792  0.205111   \n",
       "FireplaceQu              0.364798  0.024713             0.029706  0.408754   \n",
       "GrLivArea                0.482801  0.121214            -0.011279  0.534098   \n",
       "BsmtFinType2            -0.029939 -0.004499             0.064446  0.037791   \n",
       "SalePrice                0.642012  0.142680            -0.053568  0.630420   \n",
       "PoolArea                 0.017644  0.892048             0.102011  0.062335   \n",
       "Exterior1st_Stucco      -0.052896 -0.006551            -0.040078 -0.026566   \n",
       "Exterior2nd_CBlock      -0.027350 -0.001386            -0.008482 -0.022529   \n",
       "Exterior1st_HdBoard     -0.064382 -0.010271             0.013841 -0.073060   \n",
       "Fireplaces               0.299017  0.066477             0.100623  0.397684   \n",
       "Exterior1st_CemntBd      0.031154 -0.010870            -0.066503  0.064096   \n",
       "GarageCars               1.000000  0.022327            -0.008510  0.450495   \n",
       "PoolQC                   0.022327  1.000000             0.088273  0.077349   \n",
       "Exterior2nd_Plywood     -0.008510  0.088273             1.000000  0.116972   \n",
       "1stFlrSF                 0.450495  0.077349             0.116972  1.000000   \n",
       "TotRmsAbvGrd             0.370888  0.057516            -0.031882  0.399683   \n",
       "Exterior1st_VinylSd      0.334418 -0.039088            -0.229233  0.086330   \n",
       "Exterior2nd_CmentBd      0.030312 -0.010774            -0.065914  0.065084   \n",
       "Exterior2nd_AsbShng     -0.087146 -0.006082            -0.037206 -0.060506   \n",
       "BldgType_Duplex         -0.017783 -0.009975             0.120522  0.088978   \n",
       "Neighborhood_Somerst     0.169151 -0.013254            -0.081085 -0.015909   \n",
       "\n",
       "                       TotRmsAbvGrd  Exterior1st_VinylSd  Exterior2nd_CmentBd  \\\n",
       "2ndFlrSF                   0.609899             0.111846             0.011364   \n",
       "ExterQual                  0.304495             0.429330             0.155304   \n",
       "GarageQual                 0.097894             0.094112            -0.052273   \n",
       "HouseStyle_2Story          0.435397             0.173418             0.033572   \n",
       "Exterior2nd_HdBoard       -0.032464            -0.299244            -0.083618   \n",
       "Exterior2nd_VinylSd        0.130406             0.978864            -0.149429   \n",
       "GarageArea                 0.335810             0.291506             0.040819   \n",
       "Exterior1st_MetalSd       -0.113004            -0.314408            -0.086660   \n",
       "KitchenAbvGr               0.238617            -0.090217            -0.043381   \n",
       "BsmtFinSF1                 0.014037            -0.015376             0.065919   \n",
       "Exterior2nd_MetalSd       -0.111833            -0.309335            -0.085262   \n",
       "Exterior1st_AsbShng        0.001624            -0.085638            -0.023604   \n",
       "RoofStyle_Flat            -0.031511            -0.061933            -0.017071   \n",
       "Exterior1st_CBlock        -0.008288            -0.019524            -0.005381   \n",
       "Exterior1st_Wd Sdng       -0.022667            -0.301659            -0.083146   \n",
       "BsmtFinSF2                -0.040295            -0.113160            -0.021761   \n",
       "SaleType_New               0.141716             0.288047             0.118295   \n",
       "Exterior2nd_Stucco         0.025624            -0.082800            -0.026007   \n",
       "SaleCondition_Partial      0.136214             0.289669             0.115437   \n",
       "Exterior2nd_Wd Sdng       -0.045166            -0.289675            -0.081007   \n",
       "RoofMatl_Tar&Grv          -0.000178            -0.055356            -0.015258   \n",
       "TotalBsmtSF                0.264780             0.197124             0.084864   \n",
       "OverallQual                0.428587             0.356906             0.108425   \n",
       "MSZoning_FV               -0.043308             0.076222             0.091704   \n",
       "KitchenQual                0.293048             0.367920             0.151032   \n",
       "Exterior1st_Plywood       -0.003181            -0.205538            -0.056652   \n",
       "GarageCond                 0.086678             0.085738            -0.052116   \n",
       "BsmtFinType1              -0.066478             0.086769             0.069486   \n",
       "FireplaceQu                0.359216             0.091210             0.055615   \n",
       "GrLivArea                  0.829766             0.153567             0.055804   \n",
       "BsmtFinType2              -0.047104            -0.093587             0.007777   \n",
       "SalePrice                  0.545918             0.310728             0.121737   \n",
       "PoolArea                   0.055752            -0.043362            -0.011952   \n",
       "Exterior1st_Stucco         0.031504            -0.092249             0.003368   \n",
       "Exterior2nd_CBlock        -0.008288            -0.019524            -0.005381   \n",
       "Exterior1st_HdBoard       -0.062203            -0.316092            -0.087124   \n",
       "Fireplaces                 0.327566            -0.000938             0.025767   \n",
       "Exterior1st_CemntBd        0.048698            -0.153072             0.973316   \n",
       "GarageCars                 0.370888             0.334418             0.030312   \n",
       "PoolQC                     0.057516            -0.039088            -0.010774   \n",
       "Exterior2nd_Plywood       -0.031882            -0.229233            -0.065914   \n",
       "1stFlrSF                   0.399683             0.086330             0.065084   \n",
       "TotRmsAbvGrd               1.000000             0.128881             0.045801   \n",
       "Exterior1st_VinylSd        0.128881             1.000000            -0.151714   \n",
       "Exterior2nd_CmentBd        0.045801            -0.151714             1.000000   \n",
       "Exterior2nd_AsbShng        0.013018            -0.072942            -0.023604   \n",
       "BldgType_Duplex            0.139211            -0.085069            -0.038715   \n",
       "Neighborhood_Somerst      -0.013399             0.131233             0.112485   \n",
       "\n",
       "                       Exterior2nd_AsbShng  BldgType_Duplex  \\\n",
       "2ndFlrSF                          0.037818        -0.051454   \n",
       "ExterQual                        -0.080917        -0.139398   \n",
       "GarageQual                       -0.090686        -0.133518   \n",
       "HouseStyle_2Story                -0.023412        -0.050966   \n",
       "Exterior2nd_HdBoard              -0.047200        -0.001758   \n",
       "Exterior2nd_VinylSd              -0.084348        -0.082703   \n",
       "GarageArea                       -0.083096        -0.037072   \n",
       "Exterior1st_MetalSd              -0.048917         0.035703   \n",
       "KitchenAbvGr                      0.116597         0.716044   \n",
       "BsmtFinSF1                       -0.069257        -0.019420   \n",
       "Exterior2nd_MetalSd              -0.048128         0.027665   \n",
       "Exterior1st_AsbShng               0.840001         0.044627   \n",
       "RoofStyle_Flat                   -0.009636        -0.015804   \n",
       "Exterior1st_CBlock               -0.003038        -0.004982   \n",
       "Exterior1st_Wd Sdng              -0.029522        -0.044422   \n",
       "BsmtFinSF2                       -0.027944        -0.021487   \n",
       "SaleType_New                     -0.034580        -0.056715   \n",
       "Exterior2nd_Stucco               -0.014680         0.006176   \n",
       "SaleCondition_Partial            -0.035052        -0.057491   \n",
       "Exterior2nd_Wd Sdng              -0.045726        -0.041822   \n",
       "RoofMatl_Tar&Grv                 -0.008613        -0.014126   \n",
       "TotalBsmtSF                      -0.074672        -0.046195   \n",
       "OverallQual                      -0.114917        -0.160411   \n",
       "MSZoning_FV                      -0.025052        -0.041088   \n",
       "KitchenQual                      -0.080760        -0.153332   \n",
       "Exterior1st_Plywood              -0.031979         0.138864   \n",
       "GarageCond                       -0.108148        -0.133772   \n",
       "BsmtFinType1                     -0.084614        -0.095615   \n",
       "FireplaceQu                      -0.069076        -0.171610   \n",
       "GrLivArea                        -0.013194         0.019470   \n",
       "BsmtFinType2                     -0.031599        -0.077504   \n",
       "SalePrice                        -0.092983        -0.115225   \n",
       "PoolArea                         -0.006746        -0.011065   \n",
       "Exterior1st_Stucco               -0.014352         0.038306   \n",
       "Exterior2nd_CBlock               -0.003038        -0.004982   \n",
       "Exterior1st_HdBoard              -0.049179         0.003343   \n",
       "Fireplaces                       -0.062364        -0.144626   \n",
       "Exterior1st_CemntBd              -0.023816        -0.039061   \n",
       "GarageCars                       -0.087146        -0.017783   \n",
       "PoolQC                           -0.006082        -0.009975   \n",
       "Exterior2nd_Plywood              -0.037206         0.120522   \n",
       "1stFlrSF                         -0.060506         0.088978   \n",
       "TotRmsAbvGrd                      0.013018         0.139211   \n",
       "Exterior1st_VinylSd              -0.072942        -0.085069   \n",
       "Exterior2nd_CmentBd              -0.023604        -0.038715   \n",
       "Exterior2nd_AsbShng               1.000000         0.011387   \n",
       "BldgType_Duplex                   0.011387         1.000000   \n",
       "Neighborhood_Somerst             -0.029037        -0.047625   \n",
       "\n",
       "                       Neighborhood_Somerst  \n",
       "2ndFlrSF                           0.067351  \n",
       "ExterQual                          0.257266  \n",
       "GarageQual                         0.064504  \n",
       "HouseStyle_2Story                  0.145477  \n",
       "Exterior2nd_HdBoard               -0.102864  \n",
       "Exterior2nd_VinylSd                0.129340  \n",
       "GarageArea                         0.188097  \n",
       "Exterior1st_MetalSd                0.080659  \n",
       "KitchenAbvGr                      -0.053365  \n",
       "BsmtFinSF1                        -0.064579  \n",
       "Exterior2nd_MetalSd                0.084523  \n",
       "Exterior1st_AsbShng               -0.029037  \n",
       "RoofStyle_Flat                    -0.021000  \n",
       "Exterior1st_CBlock                -0.006620  \n",
       "Exterior1st_Wd Sdng               -0.102284  \n",
       "BsmtFinSF2                        -0.071866  \n",
       "SaleType_New                       0.265137  \n",
       "Exterior2nd_Stucco                -0.031993  \n",
       "SaleCondition_Partial              0.270796  \n",
       "Exterior2nd_Wd Sdng               -0.099652  \n",
       "RoofMatl_Tar&Grv                  -0.018770  \n",
       "TotalBsmtSF                        0.049440  \n",
       "OverallQual                        0.226938  \n",
       "MSZoning_FV                        0.862735  \n",
       "KitchenQual                        0.212438  \n",
       "Exterior1st_Plywood               -0.069692  \n",
       "GarageCond                         0.065302  \n",
       "BsmtFinType1                       0.010115  \n",
       "FireplaceQu                       -0.005513  \n",
       "GrLivArea                          0.045212  \n",
       "BsmtFinType2                      -0.068865  \n",
       "SalePrice                          0.141212  \n",
       "PoolArea                          -0.014703  \n",
       "Exterior1st_Stucco                -0.031279  \n",
       "Exterior2nd_CBlock                -0.006620  \n",
       "Exterior1st_HdBoard               -0.107177  \n",
       "Fireplaces                        -0.056544  \n",
       "Exterior1st_CemntBd                0.110688  \n",
       "GarageCars                         0.169151  \n",
       "PoolQC                            -0.013254  \n",
       "Exterior2nd_Plywood               -0.081085  \n",
       "1stFlrSF                          -0.015909  \n",
       "TotRmsAbvGrd                      -0.013399  \n",
       "Exterior1st_VinylSd                0.131233  \n",
       "Exterior2nd_CmentBd                0.112485  \n",
       "Exterior2nd_AsbShng               -0.029037  \n",
       "BldgType_Duplex                   -0.047625  \n",
       "Neighborhood_Somerst               1.000000  \n",
       "\n",
       "[48 rows x 48 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_no_outliers[high_corr_features_list].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('Exterior1st_MetalSd', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_Wd Sdng', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_MetalSd', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_Wd Sdng', axis = 1, inplace = True)\n",
    "train_data.drop('KitchenAbvGr', axis = 1, inplace = True)\n",
    "train_data.drop('BldgType_Duplex', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_AsbShng', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_HdBoard', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_AsbShng', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_HdBoard', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_Plywood', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_Stucco', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_Stucco', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_CBlock', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_CBlock', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior1st_Plywood', axis = 1, inplace = True)\n",
    "train_data.drop('BsmtFinSF2', axis = 1, inplace = True)\n",
    "train_data.drop('BsmtFinType2', axis = 1, inplace = True)\n",
    "train_data.drop('RoofMatl_Tar&Grv', axis = 1, inplace = True)\n",
    "train_data.drop('RoofStyle_Flat', axis = 1, inplace = True)\n",
    "train_data.drop('MSZoning_FV', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "test_data.drop('Exterior1st_MetalSd', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_Wd Sdng', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_MetalSd', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_Wd Sdng', axis = 1, inplace = True)\n",
    "test_data.drop('KitchenAbvGr', axis = 1, inplace = True)\n",
    "test_data.drop('BldgType_Duplex', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_AsbShng', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_HdBoard', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_AsbShng', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_HdBoard', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_Plywood', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_Stucco', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_Stucco', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_CBlock', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_CBlock', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior1st_Plywood', axis = 1, inplace = True)\n",
    "test_data.drop('BsmtFinSF2', axis = 1, inplace = True)\n",
    "test_data.drop('BsmtFinType2', axis = 1, inplace = True)\n",
    "test_data.drop('RoofMatl_Tar&Grv', axis = 1, inplace = True)\n",
    "test_data.drop('RoofStyle_Flat', axis = 1, inplace = True)\n",
    "test_data.drop('MSZoning_FV', axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 178)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 178)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features_list.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df_no_outliers.drop(['Exterior1st_MetalSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_MetalSd', \n",
    "                                        'Exterior1st_Wd Sdng', 'KitchenAbvGr', 'BldgType_Duplex', 'Exterior1st_AsbShng',\n",
    "                                        'Exterior1st_HdBoard', 'Exterior2nd_AsbShng', 'Exterior2nd_HdBoard',\n",
    "                                        'Exterior2nd_Plywood', 'Exterior2nd_Stucco', 'Exterior1st_Stucco',\n",
    "                                        'Exterior1st_CBlock', 'Exterior2nd_CBlock', 'Exterior1st_Plywood',\n",
    "                                        'BsmtFinSF2', 'BsmtFinType2', 'RoofMatl_Tar&Grv',\n",
    "                                        'RoofStyle_Flat', 'MSZoning_FV'], axis = 1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out the columns which are having positive & negative correlation with SalePrice\n",
    "neg_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] < 0].index.tolist()\n",
    "pos_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] > 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positively correlated attributes:  97\n",
      "Total number of negatively correlated attributes:  81\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of positively correlated attributes: \", len(pos_corr_features))\n",
    "print(\"Total number of negatively correlated attributes: \", len(neg_corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_corr_matrix = corr_matrix.loc[pos_corr_features]['SalePrice']\n",
    "neg_corr_matrix = corr_matrix.loc[neg_corr_features]['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features_to_use = pos_corr_matrix[pos_corr_matrix > 0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_features_to_use = neg_corr_matrix[neg_corr_matrix > -0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_features_to_use = pos_features_to_use + neg_features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_features_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9837243629715577"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9067632856438212"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boost Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994701065165377"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340003889536813"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9147307988028279"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8987971939915026"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130075331478078"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042365876129241"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we still have highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "                  .stack()\n",
    "                  .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SaleType_New', 'SaleCondition_Partial'] 0.9865105303450532\n",
      "['Exterior1st_VinylSd', 'Exterior2nd_VinylSd'] 0.9788636937329255\n",
      "['Exterior1st_CemntBd', 'Exterior2nd_CmentBd'] 0.9733158107377518\n",
      "['GarageQual', 'GarageCond'] 0.9569048397312127\n",
      "['PoolArea', 'PoolQC'] 0.8920478113756265\n",
      "['GarageCars', 'GarageArea'] 0.885709708781672\n",
      "['Fireplaces', 'FireplaceQu'] 0.8650844198512795\n",
      "['GrLivArea', 'TotRmsAbvGrd'] 0.82976633433873\n",
      "['2ndFlrSF', 'HouseStyle_2Story'] 0.8128225370646163\n",
      "['TotalBsmtSF', '1stFlrSF'] 0.8021329602320946\n",
      "['OverallQual', 'SalePrice'] 0.7983352031814452\n",
      "['GrLivArea', 'SalePrice'] 0.7416983601323051\n",
      "['OverallQual', 'ExterQual'] 0.7210280513186306\n",
      "['BsmtFinType1', 'BsmtFinSF1'] 0.7209413437041139\n",
      "['ExterQual', 'KitchenQual'] 0.7105647316824497\n"
     ]
    }
   ],
   "source": [
    "high_corr_features_list = []\n",
    "for k,v in sol.items():\n",
    "    #if v > 0.7 and 'SalePrice' not in list(k):\n",
    "    if v > 0.7:\n",
    "        print(list(k),v)\n",
    "        high_corr_features_list += list(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features_list = list(set(high_corr_features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>Exterior2nd_VinylSd</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>Exterior1st_CemntBd</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Exterior1st_VinylSd</th>\n",
       "      <th>Exterior2nd_CmentBd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.178668</td>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.812823</td>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.129945</td>\n",
       "      <td>-0.164398</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>-0.211421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323615</td>\n",
       "      <td>0.057196</td>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.181763</td>\n",
       "      <td>0.075067</td>\n",
       "      <td>-0.234308</td>\n",
       "      <td>0.609899</td>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExterQual</th>\n",
       "      <td>0.178668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>0.200797</td>\n",
       "      <td>0.423507</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.179136</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.375732</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686945</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.245043</td>\n",
       "      <td>0.151586</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.304495</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.155304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>0.060015</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071497</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.552166</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.060266</td>\n",
       "      <td>0.174370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268727</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.205730</td>\n",
       "      <td>-0.050950</td>\n",
       "      <td>0.566530</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>-0.052273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <td>0.812823</td>\n",
       "      <td>0.200797</td>\n",
       "      <td>0.071497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174322</td>\n",
       "      <td>0.129556</td>\n",
       "      <td>-0.129116</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.037537</td>\n",
       "      <td>-0.206101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246540</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.119068</td>\n",
       "      <td>0.038592</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>-0.295771</td>\n",
       "      <td>0.435397</td>\n",
       "      <td>0.173418</td>\n",
       "      <td>0.033572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_VinylSd</th>\n",
       "      <td>0.111925</td>\n",
       "      <td>0.423507</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.174322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289762</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>0.290164</td>\n",
       "      <td>0.190750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309657</td>\n",
       "      <td>-0.042709</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>-0.150766</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.130406</td>\n",
       "      <td>0.978864</td>\n",
       "      <td>-0.149429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.129945</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.552166</td>\n",
       "      <td>0.129556</td>\n",
       "      <td>0.289762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>0.289589</td>\n",
       "      <td>0.287666</td>\n",
       "      <td>0.475503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629915</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.258036</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.479365</td>\n",
       "      <td>0.335810</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>0.040819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>-0.164398</td>\n",
       "      <td>0.179136</td>\n",
       "      <td>0.139302</td>\n",
       "      <td>-0.129116</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>0.273692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.467596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406411</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>0.230753</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>-0.015376</td>\n",
       "      <td>0.065919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SaleType_New</th>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>0.289589</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986511</td>\n",
       "      <td>0.243713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358049</td>\n",
       "      <td>-0.017509</td>\n",
       "      <td>0.057141</td>\n",
       "      <td>0.116283</td>\n",
       "      <td>0.287024</td>\n",
       "      <td>-0.015783</td>\n",
       "      <td>0.197897</td>\n",
       "      <td>0.141716</td>\n",
       "      <td>0.288047</td>\n",
       "      <td>0.118295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.375732</td>\n",
       "      <td>0.060266</td>\n",
       "      <td>0.037537</td>\n",
       "      <td>0.290164</td>\n",
       "      <td>0.287666</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.986511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.244884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352472</td>\n",
       "      <td>-0.017748</td>\n",
       "      <td>0.057170</td>\n",
       "      <td>0.113434</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>0.198149</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>0.289669</td>\n",
       "      <td>0.115437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>-0.211421</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>0.174370</td>\n",
       "      <td>-0.206101</td>\n",
       "      <td>0.190750</td>\n",
       "      <td>0.475503</td>\n",
       "      <td>0.467596</td>\n",
       "      <td>0.243713</td>\n",
       "      <td>0.244884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649450</td>\n",
       "      <td>0.031572</td>\n",
       "      <td>0.324817</td>\n",
       "      <td>0.082350</td>\n",
       "      <td>0.452154</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>0.802133</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>0.197124</td>\n",
       "      <td>0.084864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OverallQual</th>\n",
       "      <td>0.291839</td>\n",
       "      <td>0.721028</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.276341</td>\n",
       "      <td>0.357469</td>\n",
       "      <td>0.555957</td>\n",
       "      <td>0.220542</td>\n",
       "      <td>0.317783</td>\n",
       "      <td>0.313657</td>\n",
       "      <td>0.539731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798335</td>\n",
       "      <td>0.034171</td>\n",
       "      <td>0.399290</td>\n",
       "      <td>0.102179</td>\n",
       "      <td>0.601333</td>\n",
       "      <td>0.050585</td>\n",
       "      <td>0.472316</td>\n",
       "      <td>0.428587</td>\n",
       "      <td>0.356906</td>\n",
       "      <td>0.108425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KitchenQual</th>\n",
       "      <td>0.169642</td>\n",
       "      <td>0.710565</td>\n",
       "      <td>0.219205</td>\n",
       "      <td>0.179499</td>\n",
       "      <td>0.363263</td>\n",
       "      <td>0.478443</td>\n",
       "      <td>0.214288</td>\n",
       "      <td>0.332085</td>\n",
       "      <td>0.333864</td>\n",
       "      <td>0.431949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662734</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.249677</td>\n",
       "      <td>0.147074</td>\n",
       "      <td>0.502264</td>\n",
       "      <td>0.045578</td>\n",
       "      <td>0.380196</td>\n",
       "      <td>0.293048</td>\n",
       "      <td>0.367920</td>\n",
       "      <td>0.151032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>0.049823</td>\n",
       "      <td>0.194120</td>\n",
       "      <td>0.956905</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>0.076162</td>\n",
       "      <td>0.539914</td>\n",
       "      <td>0.140393</td>\n",
       "      <td>0.056304</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>0.178029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257715</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.194741</td>\n",
       "      <td>-0.050784</td>\n",
       "      <td>0.557951</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.171431</td>\n",
       "      <td>0.086678</td>\n",
       "      <td>0.085738</td>\n",
       "      <td>-0.052116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>-0.116931</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.140547</td>\n",
       "      <td>-0.036087</td>\n",
       "      <td>0.084699</td>\n",
       "      <td>0.208212</td>\n",
       "      <td>0.720941</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.012150</td>\n",
       "      <td>0.334811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302737</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.135026</td>\n",
       "      <td>0.074661</td>\n",
       "      <td>0.191738</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.205111</td>\n",
       "      <td>-0.066478</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.069486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>0.206390</td>\n",
       "      <td>0.356743</td>\n",
       "      <td>0.215193</td>\n",
       "      <td>0.124520</td>\n",
       "      <td>0.094986</td>\n",
       "      <td>0.320215</td>\n",
       "      <td>0.172252</td>\n",
       "      <td>0.158158</td>\n",
       "      <td>0.154374</td>\n",
       "      <td>0.350422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518742</td>\n",
       "      <td>0.026883</td>\n",
       "      <td>0.865084</td>\n",
       "      <td>0.057446</td>\n",
       "      <td>0.364798</td>\n",
       "      <td>0.024713</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.359216</td>\n",
       "      <td>0.091210</td>\n",
       "      <td>0.055615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GrLivArea</th>\n",
       "      <td>0.692361</td>\n",
       "      <td>0.438275</td>\n",
       "      <td>0.176173</td>\n",
       "      <td>0.476979</td>\n",
       "      <td>0.152119</td>\n",
       "      <td>0.461256</td>\n",
       "      <td>0.143806</td>\n",
       "      <td>0.147231</td>\n",
       "      <td>0.142222</td>\n",
       "      <td>0.405399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741698</td>\n",
       "      <td>0.094686</td>\n",
       "      <td>0.462686</td>\n",
       "      <td>0.063639</td>\n",
       "      <td>0.482801</td>\n",
       "      <td>0.121214</td>\n",
       "      <td>0.534098</td>\n",
       "      <td>0.829766</td>\n",
       "      <td>0.153567</td>\n",
       "      <td>0.055804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SalePrice</th>\n",
       "      <td>0.323615</td>\n",
       "      <td>0.686945</td>\n",
       "      <td>0.268727</td>\n",
       "      <td>0.246540</td>\n",
       "      <td>0.309657</td>\n",
       "      <td>0.629915</td>\n",
       "      <td>0.406411</td>\n",
       "      <td>0.358049</td>\n",
       "      <td>0.352472</td>\n",
       "      <td>0.649450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100170</td>\n",
       "      <td>0.469589</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>0.642012</td>\n",
       "      <td>0.142680</td>\n",
       "      <td>0.630420</td>\n",
       "      <td>0.545918</td>\n",
       "      <td>0.310728</td>\n",
       "      <td>0.121737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolArea</th>\n",
       "      <td>0.057196</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.035079</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.042709</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>-0.017509</td>\n",
       "      <td>-0.017748</td>\n",
       "      <td>0.031572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068001</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>0.892048</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>0.055752</td>\n",
       "      <td>-0.043362</td>\n",
       "      <td>-0.011952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fireplaces</th>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.245043</td>\n",
       "      <td>0.205730</td>\n",
       "      <td>0.119068</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>0.258036</td>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.057141</td>\n",
       "      <td>0.057170</td>\n",
       "      <td>0.324817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469589</td>\n",
       "      <td>0.068001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>0.299017</td>\n",
       "      <td>0.066477</td>\n",
       "      <td>0.397684</td>\n",
       "      <td>0.327566</td>\n",
       "      <td>-0.000938</td>\n",
       "      <td>0.025767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_CemntBd</th>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.151586</td>\n",
       "      <td>-0.050950</td>\n",
       "      <td>0.038592</td>\n",
       "      <td>-0.150766</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.081306</td>\n",
       "      <td>0.116283</td>\n",
       "      <td>0.113434</td>\n",
       "      <td>0.082350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127208</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.048698</td>\n",
       "      <td>-0.153072</td>\n",
       "      <td>0.973316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.181763</td>\n",
       "      <td>0.520003</td>\n",
       "      <td>0.566530</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>0.230753</td>\n",
       "      <td>0.287024</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>0.452154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642012</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>0.299017</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>0.370888</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>0.030312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>0.075067</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>0.012929</td>\n",
       "      <td>-0.038500</td>\n",
       "      <td>0.020704</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>-0.015783</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142680</td>\n",
       "      <td>0.892048</td>\n",
       "      <td>0.066477</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077349</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>-0.039088</td>\n",
       "      <td>-0.010774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1stFlrSF</th>\n",
       "      <td>-0.234308</td>\n",
       "      <td>0.389984</td>\n",
       "      <td>0.176948</td>\n",
       "      <td>-0.295771</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.479365</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>0.197897</td>\n",
       "      <td>0.198149</td>\n",
       "      <td>0.802133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630420</td>\n",
       "      <td>0.062335</td>\n",
       "      <td>0.397684</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>0.077349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>0.086330</td>\n",
       "      <td>0.065084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <td>0.609899</td>\n",
       "      <td>0.304495</td>\n",
       "      <td>0.097894</td>\n",
       "      <td>0.435397</td>\n",
       "      <td>0.130406</td>\n",
       "      <td>0.335810</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.141716</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545918</td>\n",
       "      <td>0.055752</td>\n",
       "      <td>0.327566</td>\n",
       "      <td>0.048698</td>\n",
       "      <td>0.370888</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>0.399683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128881</td>\n",
       "      <td>0.045801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior1st_VinylSd</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>0.173418</td>\n",
       "      <td>0.978864</td>\n",
       "      <td>0.291506</td>\n",
       "      <td>-0.015376</td>\n",
       "      <td>0.288047</td>\n",
       "      <td>0.289669</td>\n",
       "      <td>0.197124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310728</td>\n",
       "      <td>-0.043362</td>\n",
       "      <td>-0.000938</td>\n",
       "      <td>-0.153072</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>-0.039088</td>\n",
       "      <td>0.086330</td>\n",
       "      <td>0.128881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exterior2nd_CmentBd</th>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.155304</td>\n",
       "      <td>-0.052273</td>\n",
       "      <td>0.033572</td>\n",
       "      <td>-0.149429</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.065919</td>\n",
       "      <td>0.118295</td>\n",
       "      <td>0.115437</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121737</td>\n",
       "      <td>-0.011952</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.973316</td>\n",
       "      <td>0.030312</td>\n",
       "      <td>-0.010774</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>0.045801</td>\n",
       "      <td>-0.151714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       2ndFlrSF  ExterQual  GarageQual  HouseStyle_2Story  \\\n",
       "2ndFlrSF               1.000000   0.178668    0.060015           0.812823   \n",
       "ExterQual              0.178668   1.000000    0.198751           0.200797   \n",
       "GarageQual             0.060015   0.198751    1.000000           0.071497   \n",
       "HouseStyle_2Story      0.812823   0.200797    0.071497           1.000000   \n",
       "Exterior2nd_VinylSd    0.111925   0.423507    0.084648           0.174322   \n",
       "GarageArea             0.129945   0.483553    0.552166           0.129556   \n",
       "BsmtFinSF1            -0.164398   0.179136    0.139302          -0.129116   \n",
       "SaleType_New           0.004830   0.380859    0.059015           0.042953   \n",
       "SaleCondition_Partial -0.001155   0.375732    0.060266           0.037537   \n",
       "TotalBsmtSF           -0.211421   0.468850    0.174370          -0.206101   \n",
       "OverallQual            0.291839   0.721028    0.284600           0.276341   \n",
       "KitchenQual            0.169642   0.710565    0.219205           0.179499   \n",
       "GarageCond             0.049823   0.194120    0.956905           0.060303   \n",
       "BsmtFinType1          -0.116931   0.218218    0.140547          -0.036087   \n",
       "FireplaceQu            0.206390   0.356743    0.215193           0.124520   \n",
       "GrLivArea              0.692361   0.438275    0.176173           0.476979   \n",
       "SalePrice              0.323615   0.686945    0.268727           0.246540   \n",
       "PoolArea               0.057196   0.002956    0.035079           0.006348   \n",
       "Fireplaces             0.197681   0.245043    0.205730           0.119068   \n",
       "Exterior1st_CemntBd    0.021326   0.151586   -0.050950           0.038592   \n",
       "GarageCars             0.181763   0.520003    0.566530           0.199497   \n",
       "PoolQC                 0.075067   0.017303    0.032137           0.012929   \n",
       "1stFlrSF              -0.234308   0.389984    0.176948          -0.295771   \n",
       "TotRmsAbvGrd           0.609899   0.304495    0.097894           0.435397   \n",
       "Exterior1st_VinylSd    0.111846   0.429330    0.094112           0.173418   \n",
       "Exterior2nd_CmentBd    0.011364   0.155304   -0.052273           0.033572   \n",
       "\n",
       "                       Exterior2nd_VinylSd  GarageArea  BsmtFinSF1  \\\n",
       "2ndFlrSF                          0.111925    0.129945   -0.164398   \n",
       "ExterQual                         0.423507    0.483553    0.179136   \n",
       "GarageQual                        0.084648    0.552166    0.139302   \n",
       "HouseStyle_2Story                 0.174322    0.129556   -0.129116   \n",
       "Exterior2nd_VinylSd               1.000000    0.289762   -0.016989   \n",
       "GarageArea                        0.289762    1.000000    0.273692   \n",
       "BsmtFinSF1                       -0.016989    0.273692    1.000000   \n",
       "SaleType_New                      0.288416    0.289589    0.005149   \n",
       "SaleCondition_Partial             0.290164    0.287666    0.005748   \n",
       "TotalBsmtSF                       0.190750    0.475503    0.467596   \n",
       "OverallQual                       0.357469    0.555957    0.220542   \n",
       "KitchenQual                       0.363263    0.478443    0.214288   \n",
       "GarageCond                        0.076162    0.539914    0.140393   \n",
       "BsmtFinType1                      0.084699    0.208212    0.720941   \n",
       "FireplaceQu                       0.094986    0.320215    0.172252   \n",
       "GrLivArea                         0.152119    0.461256    0.143806   \n",
       "SalePrice                         0.309657    0.629915    0.406411   \n",
       "PoolArea                         -0.042709    0.013387    0.043421   \n",
       "Fireplaces                        0.003801    0.258036    0.240740   \n",
       "Exterior1st_CemntBd              -0.150766    0.043771    0.081306   \n",
       "GarageCars                        0.329445    0.885710    0.230753   \n",
       "PoolQC                           -0.038500    0.020704    0.062512   \n",
       "1stFlrSF                          0.084123    0.479365    0.395313   \n",
       "TotRmsAbvGrd                      0.130406    0.335810    0.014037   \n",
       "Exterior1st_VinylSd               0.978864    0.291506   -0.015376   \n",
       "Exterior2nd_CmentBd              -0.149429    0.040819    0.065919   \n",
       "\n",
       "                       SaleType_New  SaleCondition_Partial  TotalBsmtSF  ...  \\\n",
       "2ndFlrSF                   0.004830              -0.001155    -0.211421  ...   \n",
       "ExterQual                  0.380859               0.375732     0.468850  ...   \n",
       "GarageQual                 0.059015               0.060266     0.174370  ...   \n",
       "HouseStyle_2Story          0.042953               0.037537    -0.206101  ...   \n",
       "Exterior2nd_VinylSd        0.288416               0.290164     0.190750  ...   \n",
       "GarageArea                 0.289589               0.287666     0.475503  ...   \n",
       "BsmtFinSF1                 0.005149               0.005748     0.467596  ...   \n",
       "SaleType_New               1.000000               0.986511     0.243713  ...   \n",
       "SaleCondition_Partial      0.986511               1.000000     0.244884  ...   \n",
       "TotalBsmtSF                0.243713               0.244884     1.000000  ...   \n",
       "OverallQual                0.317783               0.313657     0.539731  ...   \n",
       "KitchenQual                0.332085               0.333864     0.431949  ...   \n",
       "GarageCond                 0.056304               0.057609     0.178029  ...   \n",
       "BsmtFinType1              -0.013927              -0.012150     0.334811  ...   \n",
       "FireplaceQu                0.158158               0.154374     0.350422  ...   \n",
       "GrLivArea                  0.147231               0.142222     0.405399  ...   \n",
       "SalePrice                  0.358049               0.352472     0.649450  ...   \n",
       "PoolArea                  -0.017509              -0.017748     0.031572  ...   \n",
       "Fireplaces                 0.057141               0.057170     0.324817  ...   \n",
       "Exterior1st_CemntBd        0.116283               0.113434     0.082350  ...   \n",
       "GarageCars                 0.287024               0.282757     0.452154  ...   \n",
       "PoolQC                    -0.015783              -0.015999     0.050641  ...   \n",
       "1stFlrSF                   0.197897               0.198149     0.802133  ...   \n",
       "TotRmsAbvGrd               0.141716               0.136214     0.264780  ...   \n",
       "Exterior1st_VinylSd        0.288047               0.289669     0.197124  ...   \n",
       "Exterior2nd_CmentBd        0.118295               0.115437     0.084864  ...   \n",
       "\n",
       "                       SalePrice  PoolArea  Fireplaces  Exterior1st_CemntBd  \\\n",
       "2ndFlrSF                0.323615  0.057196    0.197681             0.021326   \n",
       "ExterQual               0.686945  0.002956    0.245043             0.151586   \n",
       "GarageQual              0.268727  0.035079    0.205730            -0.050950   \n",
       "HouseStyle_2Story       0.246540  0.006348    0.119068             0.038592   \n",
       "Exterior2nd_VinylSd     0.309657 -0.042709    0.003801            -0.150766   \n",
       "GarageArea              0.629915  0.013387    0.258036             0.043771   \n",
       "BsmtFinSF1              0.406411  0.043421    0.240740             0.081306   \n",
       "SaleType_New            0.358049 -0.017509    0.057141             0.116283   \n",
       "SaleCondition_Partial   0.352472 -0.017748    0.057170             0.113434   \n",
       "TotalBsmtSF             0.649450  0.031572    0.324817             0.082350   \n",
       "OverallQual             0.798335  0.034171    0.399290             0.102179   \n",
       "KitchenQual             0.662734  0.042553    0.249677             0.147074   \n",
       "GarageCond              0.257715  0.035358    0.194741            -0.050784   \n",
       "BsmtFinType1            0.302737  0.013777    0.135026             0.074661   \n",
       "FireplaceQu             0.518742  0.026883    0.865084             0.057446   \n",
       "GrLivArea               0.741698  0.094686    0.462686             0.063639   \n",
       "SalePrice               1.000000  0.100170    0.469589             0.127208   \n",
       "PoolArea                0.100170  1.000000    0.068001            -0.012059   \n",
       "Fireplaces              0.469589  0.068001    1.000000             0.027693   \n",
       "Exterior1st_CemntBd     0.127208 -0.012059    0.027693             1.000000   \n",
       "GarageCars              0.642012  0.017644    0.299017             0.031154   \n",
       "PoolQC                  0.142680  0.892048    0.066477            -0.010870   \n",
       "1stFlrSF                0.630420  0.062335    0.397684             0.064096   \n",
       "TotRmsAbvGrd            0.545918  0.055752    0.327566             0.048698   \n",
       "Exterior1st_VinylSd     0.310728 -0.043362   -0.000938            -0.153072   \n",
       "Exterior2nd_CmentBd     0.121737 -0.011952    0.025767             0.973316   \n",
       "\n",
       "                       GarageCars    PoolQC  1stFlrSF  TotRmsAbvGrd  \\\n",
       "2ndFlrSF                 0.181763  0.075067 -0.234308      0.609899   \n",
       "ExterQual                0.520003  0.017303  0.389984      0.304495   \n",
       "GarageQual               0.566530  0.032137  0.176948      0.097894   \n",
       "HouseStyle_2Story        0.199497  0.012929 -0.295771      0.435397   \n",
       "Exterior2nd_VinylSd      0.329445 -0.038500  0.084123      0.130406   \n",
       "GarageArea               0.885710  0.020704  0.479365      0.335810   \n",
       "BsmtFinSF1               0.230753  0.062512  0.395313      0.014037   \n",
       "SaleType_New             0.287024 -0.015783  0.197897      0.141716   \n",
       "SaleCondition_Partial    0.282757 -0.015999  0.198149      0.136214   \n",
       "TotalBsmtSF              0.452154  0.050641  0.802133      0.264780   \n",
       "OverallQual              0.601333  0.050585  0.472316      0.428587   \n",
       "KitchenQual              0.502264  0.045578  0.380196      0.293048   \n",
       "GarageCond               0.557951  0.032392  0.171431      0.086678   \n",
       "BsmtFinType1             0.191738  0.017687  0.205111     -0.066478   \n",
       "FireplaceQu              0.364798  0.024713  0.408754      0.359216   \n",
       "GrLivArea                0.482801  0.121214  0.534098      0.829766   \n",
       "SalePrice                0.642012  0.142680  0.630420      0.545918   \n",
       "PoolArea                 0.017644  0.892048  0.062335      0.055752   \n",
       "Fireplaces               0.299017  0.066477  0.397684      0.327566   \n",
       "Exterior1st_CemntBd      0.031154 -0.010870  0.064096      0.048698   \n",
       "GarageCars               1.000000  0.022327  0.450495      0.370888   \n",
       "PoolQC                   0.022327  1.000000  0.077349      0.057516   \n",
       "1stFlrSF                 0.450495  0.077349  1.000000      0.399683   \n",
       "TotRmsAbvGrd             0.370888  0.057516  0.399683      1.000000   \n",
       "Exterior1st_VinylSd      0.334418 -0.039088  0.086330      0.128881   \n",
       "Exterior2nd_CmentBd      0.030312 -0.010774  0.065084      0.045801   \n",
       "\n",
       "                       Exterior1st_VinylSd  Exterior2nd_CmentBd  \n",
       "2ndFlrSF                          0.111846             0.011364  \n",
       "ExterQual                         0.429330             0.155304  \n",
       "GarageQual                        0.094112            -0.052273  \n",
       "HouseStyle_2Story                 0.173418             0.033572  \n",
       "Exterior2nd_VinylSd               0.978864            -0.149429  \n",
       "GarageArea                        0.291506             0.040819  \n",
       "BsmtFinSF1                       -0.015376             0.065919  \n",
       "SaleType_New                      0.288047             0.118295  \n",
       "SaleCondition_Partial             0.289669             0.115437  \n",
       "TotalBsmtSF                       0.197124             0.084864  \n",
       "OverallQual                       0.356906             0.108425  \n",
       "KitchenQual                       0.367920             0.151032  \n",
       "GarageCond                        0.085738            -0.052116  \n",
       "BsmtFinType1                      0.086769             0.069486  \n",
       "FireplaceQu                       0.091210             0.055615  \n",
       "GrLivArea                         0.153567             0.055804  \n",
       "SalePrice                         0.310728             0.121737  \n",
       "PoolArea                         -0.043362            -0.011952  \n",
       "Fireplaces                       -0.000938             0.025767  \n",
       "Exterior1st_CemntBd              -0.153072             0.973316  \n",
       "GarageCars                        0.334418             0.030312  \n",
       "PoolQC                           -0.039088            -0.010774  \n",
       "1stFlrSF                          0.086330             0.065084  \n",
       "TotRmsAbvGrd                      0.128881             0.045801  \n",
       "Exterior1st_VinylSd               1.000000            -0.151714  \n",
       "Exterior2nd_CmentBd              -0.151714             1.000000  \n",
       "\n",
       "[26 rows x 26 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_no_outliers[high_corr_features_list].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After looking at the above correlation matrix, we will remove 'SaleCondition_Partial', 'Exterior2nd_VinylSd',\n",
    "# 'Exterior2nd_CmentBd', 'GarageCond', 'PoolArea', 'GarageArea', 'Fireplaces', 'TotRmsAbvGrd', 'HouseStyle_2Story',\n",
    "# '1stFlrSF', 'ExterQual', 'BsmtFinType1'\n",
    "\n",
    "train_data.drop('SaleCondition_Partial', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_VinylSd', axis = 1, inplace = True)\n",
    "train_data.drop('Exterior2nd_CmentBd', axis = 1, inplace = True)\n",
    "train_data.drop('GarageCond', axis = 1, inplace = True)\n",
    "train_data.drop('PoolArea', axis = 1, inplace = True)\n",
    "train_data.drop('GarageArea', axis = 1, inplace = True)\n",
    "train_data.drop('Fireplaces', axis = 1, inplace = True)\n",
    "train_data.drop('TotRmsAbvGrd', axis = 1, inplace = True)\n",
    "train_data.drop('HouseStyle_2Story', axis = 1, inplace = True)\n",
    "train_data.drop('1stFlrSF', axis = 1, inplace = True)\n",
    "train_data.drop('ExterQual', axis = 1, inplace = True)\n",
    "train_data.drop('BsmtFinType1', axis = 1, inplace = True)\n",
    "\n",
    "test_data.drop('SaleCondition_Partial', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_VinylSd', axis = 1, inplace = True)\n",
    "test_data.drop('Exterior2nd_CmentBd', axis = 1, inplace = True)\n",
    "test_data.drop('GarageCond', axis = 1, inplace = True)\n",
    "test_data.drop('PoolArea', axis = 1, inplace = True)\n",
    "test_data.drop('GarageArea', axis = 1, inplace = True)\n",
    "test_data.drop('Fireplaces', axis = 1, inplace = True)\n",
    "test_data.drop('TotRmsAbvGrd', axis = 1, inplace = True)\n",
    "test_data.drop('HouseStyle_2Story', axis = 1, inplace = True)\n",
    "test_data.drop('1stFlrSF', axis = 1, inplace = True)\n",
    "test_data.drop('ExterQual', axis = 1, inplace = True)\n",
    "test_data.drop('BsmtFinType1', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df_no_outliers.drop(['Exterior1st_MetalSd', 'Exterior2nd_Wd Sdng', 'Exterior2nd_MetalSd', \n",
    "                                        'Exterior1st_Wd Sdng', 'KitchenAbvGr', 'BldgType_Duplex', 'Exterior1st_AsbShng',\n",
    "                                        'Exterior1st_HdBoard', 'Exterior2nd_AsbShng', 'Exterior2nd_HdBoard',\n",
    "                                        'Exterior2nd_Plywood', 'Exterior2nd_Stucco', 'Exterior1st_Stucco',\n",
    "                                        'Exterior1st_CBlock', 'Exterior2nd_CBlock', 'Exterior1st_Plywood',\n",
    "                                        'BsmtFinSF2', 'BsmtFinType2', 'RoofMatl_Tar&Grv',\n",
    "                                        'RoofStyle_Flat', 'MSZoning_FV', 'SaleCondition_Partial', 'Exterior2nd_VinylSd',\n",
    "                                        'Exterior2nd_CmentBd', 'GarageCond', 'PoolArea', 'GarageArea', 'Fireplaces', \n",
    "                                        'TotRmsAbvGrd', 'HouseStyle_2Story','1stFlrSF', 'ExterQual', 'BsmtFinType1'], \n",
    "                                        axis = 1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out the columns which are having positive & negative correlation with SalePrice\n",
    "neg_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] < 0].index.tolist()\n",
    "pos_corr_features = corr_matrix[corr_matrix.iloc[:]['SalePrice'] > 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positively correlated attributes:  85\n",
      "Total number of negatively correlated attributes:  81\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of positively correlated attributes: \", len(pos_corr_features))\n",
    "print(\"Total number of negatively correlated attributes: \", len(neg_corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_corr_matrix = corr_matrix.loc[pos_corr_features]['SalePrice']\n",
    "neg_corr_matrix = corr_matrix.loc[neg_corr_features]['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_features_to_use = pos_corr_matrix[pos_corr_matrix > 0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_features_to_use = neg_corr_matrix[neg_corr_matrix > -0.02].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_features_to_use = pos_features_to_use + neg_features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_features_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983861938452126"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9067764684125953"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993328872447595"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9323288740158635"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9109760631382335"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041703810162245"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 82)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[tot_features_to_use].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.909885233653074"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9052241187828818"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change in Preprocessing pipeline\n",
    "## 1) Standard Scaler of features instead of Min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 199)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 198)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_no_outliers, test_df = outlier_detection_and_removal(train_df, test_df, \"isoforest\",\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = shuffle_split_data(train_df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_features_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838422334497096"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9059798050932434"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993687092084735"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337633437244202"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9109760631382335"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041703810162239"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9085718525667557"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9039622695501153"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Let's check the distribution of target variable SalePrice and transform it to have normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of SalePrice')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_df_no_outliers['SalePrice']).set_title(\"Distribution of SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "res = stats.probplot(train_df_no_outliers['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.7.8\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Using the log1p function applies log(1+x) to all elements of the column\n",
    "train_df_no_outliers[\"SalePrice\"] = np.log1p(train_df_no_outliers[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of SalePrice')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_df_no_outliers['SalePrice']).set_title(\"Distribution of SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyyUlEQVR4nO3deZyW8/7H8denUVqPqHBKi6OFOmQZWU+WknSI7JSdKB3bD0UKEZHDsSdHkobsx3pUsmQLlfa0nJSSoywdKrTM5/fHdd11m+5tprm3mffz8ZjH3Nd1f6/r+szI/Znvbu6OiIhISVWyHYCIiOQmJQgREYlJCUJERGJSghARkZiUIEREJCYlCBERiUkJQio9M3Mza17GaxebWcc47/3FzObFKmtm15vZP8sWcaniO9zMlqX7OVIxKUFIXgo/bH8xs9Vm9q2ZPW5mtbMdVzR3f9/dW8V57zZ3vxDAzJqFSWqbsjzHzM41s43h7+InM5tmZseW4T4jzezWssQgFZMShOSz49y9NrAvsD9wQ8kCZf3QzUMfh7+LusBjwLNmtkN2Q5J8pwQhec/dvwb+DfwZNjUZXWpmC4AF4bmLzGyhmf1gZq+YWcMSt+liZovM7DszG2pmVcLrdjOzt83s+/C9IjOrW+La/c1sjpn9GNZkqofXxm3eMbObzGx0eDgx/L4qrAUcFsa5Z1T5HcMaU4Mkv4tiYARQA/hTjOfuYWbvmtkqM5ttZl3D8z2B7sC1YQyvJnqOVA5KEJL3zKwx0AX4POr0CcABQGszOxK4HTgV+COwBBhT4jbdgEKC2sjxwPmR24fXNgT2ABoDN5W4tjtwNLAb0JIYNZkk2off67p7bXd/L4yvR1SZM4C33H1lohuFNaYLgdWEyTHqvarAq8A4YEfgb0CRmbVy9+FAEXBnGMNxpfwZpAJSgpB89i8zWwV8ALwH3Bb13u3u/oO7/0LwAT7C3ae6+2/AdcBBZtYsqvwdYfmvgH8QfCDj7gvdfby7/xZ+ON8NHFYijgfcfam7/wAMjly7lZ4AzozUZICzgCcTlD8w/F38N3x+N3f/X8kyQG1giLuvc/e3gdfKKV6pgCpL+6xUTCe4+1tx3lsa9bohMDVy4O6rzex7oBGwOEb5JeE1mNmOwH3AX4A6BH9U/ZjgWZuu3Rru/omZrQEOM7NvgObAKwkumeTuhya5bUNgadgMFbGE4PcgsgXVIKSiil6meDnQNHJgZrWAesDXUWUaR71uEl4DQfOSA3u5+x8Imn2sxLPiXVuWWKM9ET7vLOB5d/+1lPctaTnQOKpWAkG8kd+DlnaW31GCkMrgKeA8M9vbzLYlaIr6xN0XR5W5xsy2D/szLgeeCc/XIWjPX2VmjYBrYtz/UjPbJRw1dH3UtalaCRSzZafykwR9Iz2AUaW8ZyyfAGsIOqKrmtnhwHFs7o/5NkYMUokpQUiF5+4TgAHAC8A3BJ3Jp5co9jIwBZgGvE4wVBTgZoKO6/+F51+M8YinCDp+F4VfpZpL4O5rCfouPgxHFx0Ynl9G0DTmwPuluWec56wDugLHAN8BDwFnu/sXYZHHCDr1V5nZv7b2eZL/TBsGieQuMxsBLHf30o6MEtlq6qQWyVHhKKsTgX2yHIpUUmpiEslBZnYLMAsY6u5fZjseqZzUxCQiIjGpBiEiIjFVqD6I+vXre7NmzbIdhohI3pgyZcp37h5zja8KlSCaNWvG5MmTsx2GiEjeMLMl8d5TE5OIiMSkBCEiIjEpQYiISExKECIiEpMShIiIxKQEISKSp4qKoFkzqFIl+F5UVL73V4IQEckRpfnALyqCnj1hyRJwD7737Fm+SSJtCcLMRpjZCjObFXXuFjObYWbTzGxcjI3jI+UWm9nMsJwmNohIhVfaD/z+/WHt2t+fW7s2OF9e0rYWk5m1J9hoZZS7/zk89wd3/yl8fRnQ2t0viXHtYqDQ3b8rzTMLCwtdE+VEJB81axYkhZKaNoXFi7c8X6VKkEhKMoPi4i3Px2NmU9y9MNZ7aatBuPtE4IcS536KOqyFtjgUEQHgq69Kd75Jk9KdL4uM90GY2WAzWwp0BwbGKebAODObYmY9k9yvp5lNNrPJK1euLO9wRUQyorQf+IMHQ82avz9Xs2ZwvrxkPEG4e393bwwUAX3iFDvE3fcl2Brx0rC5Kt79hrt7obsXNmgQc70pEZGcV9oP/O7dYfjwoAnKLPg+fHhwvrxkcxTTU8BJsd5w9+Xh9xXAS0C7DMYlIpJxZfnA79496J8oLg6+l2dygAyv5mpmLdx9QXjYFfgiRplaQBV3/zl83QkYlMEwRUSyonv38v+Q3xppSxBm9jRwOFDfzJYBNwJdzKwVUAwsAS4JyzYE/unuXYCdgJfMLBLfU+7+ZrriFBGR2NKWINz9jBinH4tTdjnQJXy9CGibrrhERCQ1mkktIiIxKUGIiGRYutdQKi8VastREZFcF1lSI7JMRmRJDcitDmpQDUJEJK1K1hYuvzz9ayiVFyUIEZE0ibUA3/ffxy4bb0mNhNzhhRfg+uu3Ks54lCBERNIk1oqr8ZR6DaVJk+DQQ+Hkk+HVV1N/UCkoQYiIlKPoJqVYq7PGUqo1lP7zHzj1VDjoIFi0KJhu/fnnW67TUQ6UIEREyknJJqV46tUrwxpKP/wAV10Fe+wBr78ON94ICxbARRfBNukZb6RRTCIi5SSVJqWaNeHee0sxYum33+CBB+DWW+Gnn+D88+Hmm6FhzP3WypVqECIiZRRpTjIL/ohP1KRU6hVX3WHMGNh9d7j66qBJafp0ePTRjCQHUA1CRKRMSs5n2Lgxftl4u8LF9f77QVL49FNo2xbGjYOjjtqacMtENQgRkVIqKoJzzklt4FCpOqDnz4du3aB9e/j6axg5EqZMyUpyACUIEZFSidQcEtUYIlJuUlq5Evr0gTZt4K23gv6G+fODLFRQUC5xl4WamERESiHVuQ0pNSv98gv84x9w++3BTXv2DEYn7bRTOUS69ZQgRERKIZUZz0mblYqLg6pI//6wdCl07Qp33BF0SOcQNTGJiJRCshnPSZuV3n4bCgvh7LODmsK778LLL+dccgAlCBGRlBUVwerVW56vWRNGjw5GpsbdG3rOHDj2WOjQIViQqagIPvkEDjss3WGXmRKEiEgKIp3TJRfbq1cvSY3hv/+Fiy+GPfeEDz6AO++EefPgzDOD9ThymPogRESSiAxrjTVyqXbtOMlhzRr4+9+DhPDbb8EopQEDoH79tMdbXpQgRETiKCoK9m+It0Q3xOi03rgxmL8wYAB88w2cdFIwSqlFi3SGmha5Xb8REcmw6OUzevRInBygRKf12LGwzz5w4YVBb/WHH8Lzz+dlcgAlCBGRTaJXY03FpuGs06dDp07QuXPQtPTss/DRR3DwwWmNN92UIEREQrG2A42noACeHPI13SecH9QaJk+Ge+4JRiudckpQBclzaUsQZjbCzFaY2ayoc7eY2Qwzm2Zm48ws5pKEZtbZzOaZ2UIz65euGEVEInr3Tt6cFLFjjZ/5/NgBnNi3RVDt+L//CzbyueIK2HbbtMaZSemsQYwEOpc4N9Td93L3vYHXgIElLzKzAuBB4BigNXCGmbVOY5wiUkkVFQWDiszg4YeTly9gA/9XaxiLqzZnz5dvhRNOgC++gKFDYfvt0x5vpqUtQbj7ROCHEud+ijqsBcTac6kdsNDdF7n7OmAMcHy64hSRyql379Q6oQPOsbzGsrp7cteaXtRo2yqY5PbUU7DrrukONWsyPszVzAYDZwP/A46IUaQRsDTqeBlwQAZCE5EKLpVhqyXtyxQe2PZqDvrtXdixJYz8V7B2UgXoY0gm453U7t7f3RsDRUCfGEVi/dbj7u5qZj3NbLKZTV65cmV5hSkiFUxREZx3XurJoQlLeJIeTKGQg+rMggcfhFmz4PjjK0VygOyOYnoKOCnG+WVA46jjXYDl8W7i7sPdvdDdCxs0aFDOIYpIRVBUFKyNt3598rLbsYoh9GUerTi14AW47jpYuDBok6paNf3B5pCMJggzi54t0hX4Ikaxz4AWZrarmVUDTgdeyUR8IlLxRJbJKC5OXK4q6+jD/SykOdcwlGktT6Pal/Phtttgu+0yE2yOSecw16eBj4FWZrbMzC4AhpjZLDObAXQCLg/LNjSzNwDcfQNB09NYYC7wrLvPTlecIlIxRUYo9eiRbPc3pxsvMps23M9lzNmmLW/eOoUD5z0BjRsnurDCM/e4zft5p7Cw0CdPnpztMEQkS4qKgoVT16xJrfwBTOIuruZQPmT59q1pOHooHHNMpeljADCzKe5eGOs9zaQWkQqhqAjOOiu15LArixjDaUziIFrYQiZdMJyGK6ZDly6VKjkko9VcRSSv9e4Nw4YFm/Uksz0/cAO30ocHWE9VZnQbyF6jrmGn2rXTH2geUoIQkbxT2qakavxGHx7gBm7lD/zE45zHf84exO1PxFztR0JqYhKRvNKxY9DxnFpycE5jDHPZg79zNZM4kL2ZxtRe/1RySIEShIjkjY4dYcKE1MoeyvtM4kDGcAY/U4ejGMeptf9Nv9F78tBD6Y2zolCCEJG8kGpyaMF8XuBE3qc9jfiac3mco3aYyrmjj+LnnxPsHS1bUB+EiOS83r2TJ4f6rGQgg7iEYfxKdfpzK9MOv5LX36mZmSArINUgRCSnFRUlXoq7Or/QlyEspDm9eJhHuYiWtpAfe/VXcthKqkGISM5K1KxkFNOdIgbTnyYs5RWOY9rpdzDw6T3ondkwKyzVIEQkp/TuHcxVM4ufHI7gbT5jf57kbFawI/d0fYeu/goDn94js8FWcEoQIpIzevdO3Jy0B3N4lWN5mw7U5zvOpIh5oz7lypcPz1SIlYoShIhkXceOibf93In/MoyLmcmeHMoHXMsdtGIedXudSfez9DGWLvrNikjWRJqT4jUl1WQNN3ALC2nO+YzgAfrQnIUM5VoO7VBd8xnSTJ3UIpJRqcxnqMJGzuEJbmEAjVjO85zEddzOQoItZVq3hrfeykCwlZxqECKSEUVFiWsLEZ0Yy+fswwgu4CuacAgfcArPb0oOHTrAbO0QkxFKECKSdr17B+snJbInM3iToxlLZ2qxhlN4loP5iI84BIDq1WH0aNUcMklNTCKSVsmalBryNbcwgHMZySrqciV38xC9Wce2QNCcpBpDdihBiEjaJFoiozY/cy138n/8nQI2cjdXMZj+rGL7TWU6dFCNIZvUxCQiaRNr2GoBG7iYYSykOQO4lZc5nt35gmu4i1VsT0FB0JTkruSQbUoQIlLuIvMafs/5K68xg70YRi/m0Yp2fMKZPM1idgWgVy/YsEErruYKJQgRKTfxRirtyxTe5khe4zgK2MgJvMRhvMdka7eptuCO5jXkGPVBiEi5aNQIli///bnGfMVg+nMWo1lJfS7lAYbTkw1UpUYNWLs2O7FKakpVgzCzKmb2h3QFIyL5JXphvejk8Af+x+30Yz4tOZnnuY3raM5CHuJSNlAVUHLIB0lrEGb2FHAJsBGYAmxnZne7+9B0ByciuataNVi//vfntmE9lzCMG7mZ+nzPKM7iBm5lKU1+V2706AwGKmWWSg2itbv/BJwAvAE0Ac5KdpGZjTCzFWY2K+rcUDP7wsxmmNlLZlY3zrWLzWymmU0zs8kp/SQikhGRfobfJwfnBF5iNm24n8uYTlv2ZQrnMGqL5NCrlzqh80UqCaKqmVUlSBAvu/t6wFO4biTQucS58cCf3X0vYD5wXYLrj3D3vd29MIVniUgGNGq05YzodnzC+/yFlziR9VSlC6/Tkbf4nH1/V84sqDmoIzp/pJIgHgEWA7WAiWbWFPgp2UXuPhH4ocS5ce6+ITycBOxSqmhFJCsifQ3R/Qy7sogxnMYnHEhzFtKTR2jLdP5NF+D3Y1x79YLiYtUc8k3SPgh3vw+4L+rUEjM7ohyefT7wTLzHAuPMzIFH3H14vJuYWU+gJ0CTJk3iFRORMqpZE375ZfPx9vxAfwbzN+5nPVW5mYHcxdWsps4W19atCz/+mLlYpXwlrUGY2U5m9piZ/Ts8bg2cszUPNbP+wAagKE6RQ9x9X+AY4FIzax/vXu4+3N0L3b2wQYMGWxOWiERp1CioNUSSQzV+40ruZiHNuZJ7GMXZtGABN3HzFskh0pyk5JDfUmliGgmMBRqGx/OBK8r6QDM7BzgW6O7uMfsy3H15+H0F8BLQrqzPE5HSq1YtujnJOZVnmMse3M3/8QkH0JbpXMQ/+WbTx0KgdetgwpuakyqGVBJEfXd/FigGCPsQNpblYWbWGegLdHX3mKOgzayWmdWJvAY6AbNilRWR8ldQsHmE0iF8wMccxDOczs/UoRNj6cK/mcWeW1znrlVXK5pUEsQaM6tHOHLJzA4E/pfsIjN7GvgYaGVmy8zsAuABoA4wPhzCOiws29DM3ggv3Qn4wMymA58Cr7v7m6X9wUQkddET3oqLoQXzeYET+YC/0JilnMvj7MtUxtNpi2urVg2Sg1Q8qSy1cRXwCrCbmX0INABOTnaRu58R4/RjccouB7qErxcBbVOIS0TKQfSievVZyUAGcQnD+JXq3MAt3M1V/ELNmNf26qVhqxVZKqOYpprZYUArgrFr88K5ECKSx3r33rwcd3V+4XLu5TpupxZreJSLuImbWMFOMa/VOkqVQypLbZxd4tS+Zoa7j0pTTCKSRkVFmye7GcV0p4jB9KcJS3mF4+jLHXzBHnGvV62h8kiliWn/qNfVgQ7AVEAJQiSPRCcGgMN5h7u4mv2YymT242xG8R6Hx71etYbKJ5Umpr9FH5vZdsCTaYtIRMpd9L7QezCHO+jLcbzGEprQndE8zRl4gjErmvBWOZVlw6C1QIvyDkRE0qNatSA57MR/eZhLmMmetGci13IHrZjHU3RPmBx69VJyqKxS6YN4lc2L81UBWgPPpjMoEdl6kSUyarKGvtxNX+5gW37jQS5lEAP5nvpxr1VzkkBqfRB3Rb3eACxx92VpikdEyoEZVGEj5/EEtzCARiznBU6kH0NYmKQBYPRozYKWQCp9EO9lIhAR2XqRbT+PYhx3cTV7MZNJHMCpPMtHHJLwWtUapKS4CcLMfib2vg8GuLtr61GRHGIGezKDEVzD0YxjEbtyCs/yPCdTcvntaFWrwrp1mYtT8kfcBOHuW67dKyI5qZF9zT8ZyHk8zirqciV38xC9Wce2Ca/TEhmSSCp9EACY2Y4E8yAAcPev0hKRiKRsxxo/0+fXoSzgLgrYyN1cxWD6s4rtk16r5CDJpDKKqSvwd4LlvlcATYG5QJv0hiYi8XQ6cgO7vvMYM7iRnfmWpzmd67mNxeya9NrWrbXqqqQmlRrELcCBwFvuvk+4m1yshfhEJN3cObbK6/yDa2nNXN7nUI7nZT7lgKSXarKblFYqE+XWu/v3QBUzq+Lu7wB7pzcsESlpX5vK21U68BrHsQ0bOIGXaM/ElJJDhw5KDlJ6qdQgVplZbWAiUGRmKwjmQ4hIBjSxrxhMf6YympXUpw/38wgXs4GqSa+tUgU2lml7L5EENQgzO9nMqgPHEyyvcSXwJvAf4LjMhCdSee1c438MsX7MpyUn8zy304/mLORB+qSUHDp0UHKQrZOoBtEdeIggKTwNjHP3JzISlUglVVQE5/ZYzyUMYyaDaMB3jOIsbuBWltIk5ftohJKUh7g1CHfvBjQHJgCXAUvN7GEza5+p4EQqEzPn+R4vMZs23M9lzGRP9mUK5zAq5eTQoYOSg5SfhH0Q7v4T8ATwRLgv9cnA/Wa2g7s3zkSAIhVZQUGwB3Q7PmEiV/MXPmA2rfkrr/EGXUg0A7okJQYpbykt921m2wMnAqcBOwAvpDMokYrOLPhqWryIpzmdTziQFiygJ4/Qlum8wV9JNTn06qXkIOmRaC2mOsAJBHMe9gVeAW4F3nHXP0eRsogswb09P9CfwfyN+9nANgxiAEO5htWkvsKN/i+UdEvUxPQlMBZ4GHjT3ddnJiSRiskMqvEbV/IgN3ArdVnF45zHQAaxnEYp30fLcUumJEoQTdxdi/+KbCUzAOcUnmMI/fgTX/ImR3MtdzKTvVK+T8OG8PXXaQtTZAuJRjFtVXIwsxFmtsLMZkWdG2pmX5jZDDN7yczqxrm2s5nNM7OFZtZva+IQyZY2bYLkcAgf8DEH8Syn8TN16MRYjuHNlJODe/Cl5CCZVpY9qVM1Euhc4tx44M/uvhcwH7iu5EVmVgA8CBxDsL3pGWbWOo1xipSrSGJYN2cBz3MSH/AXGrOU8xjBvkxlPJ1Suk8kMYhkS9oShLtPBH4ocW6cu0eW6ZgE7BLj0nbAQndf5O7rgDEEs7lFcp4ZfDvnO+7lMubQmk6M4wZuoSXzGcl5FFOQ8PrWrZUYJHckGsX0KrF3lAPA3btu5bPPB56Jcb4RsDTqeBnEX43MzHoCPQGaNEl9pqlIeSkqgh49oDq/cC33cT23UYs1PMpF3MRNrGCnpPfo1QseeigDwYqUQqJO6rvC7ycCOwOjw+MzgMVb81Az60+w4F9RrLdjnEuUqIYDwwEKCwv1d5dkVKNG8M3yYrrzFLdxPU1YyqscS1/uYC7JW0a1BLfkskSd1O+5+3vAPu5+mru/Gn6dCRxa1gea2TnAsUD3OPMplgHRs7R3AZaX9Xki6dCxY9Cc1HL5O3zG/ozmLFbSgCN4m668mlJyGD1ayUFyWyrLfTcwsz+5+yIAM9sVaFCWh5lZZ6AvcFiCUVKfAS3C53wNnA6cWZbniZS3Nm1gzhzYnbm8wrUcx2ssoQk9eJKnOBNPoVtP/QuSL1LppL4SeNfM3jWzd4F3gCuSXWRmTwMfA63MbJmZXQA8ANQBxpvZNDMbFpZtaGZvAISd2H0IJunNBZ51d22QKFkVqTF8N+dbHuYSZrIn7ZlIX4awO19QRI+kyaFKFSUHyS+WyqoZZrYtsHt4+IW7/5bWqMqosLDQJ0+enO0wpIKIdD4D1GQNV3E313In1fmVh+nFIAbyPfVTupcSg+QqM5vi7oWx3ktagzCzmsA1QB93nw40MbNjyzlGkZzSpk2QHKqwkXN5nPm05BYGMo5OtGE2l3NfSslBC+lJPkulD+JxYApwUHi8DHgOeC1dQYlkg5UYP3cU4xjKNbRlBpM4gNN4hg9THJ+hpCAVQSp9ELu5+53AegB3/4XSLFIvksMis56jk8OezODfdGYcR1OHnzmVZziIj1NKDlWrKjlIxZFKglhnZjUI5yKY2W5ATvZBiKSiUaPNSWHOnM3nG/I1/+QCprE37fiUK7mbPZjLc5xKsr+JIh3Q69alN3aRTEqlielGgn2pG5tZEXAIcG46gxJJh0aNYHmMGTW1+ZlrGMrV3EUBG7mHKxlMf35kh6T3rFpVSUEqroQJwsyqAJHd5A4k+DPqcnf/LgOxiZSLkn0LEQVs4HxGMIiB7My3jOE0ruc2vuRPSe9ZpQps3FjOgYrkmGR7UhebWR93fxZ4PUMxiWy1yM5tsTldeIOhXENr5vI+h3I8L/Np/CW/Nl+p/gWpRFLpgxhvZlebWWMz2yHylfbIRMqgZs2gxhAvOezDVCbQgdc5lm3YQDdepD0TkyYHTXKTyiiVBHE+cCkwkWC46xRAs9EkZ2y//eZO53iJoTFf8QRnM5X92IsZ9OF+2jCbf9GNeB3Qo0dvXnpbzUlSGSXtpHb3XTMRiEhpRc90jucP/I9+DOFK7gHgdvoxhH78xHYxy9eoAWu10a4IkOJMajO7wcyGh8ctNJNasi1ZctiG9VzKAyykOdcxhGc5lZbM53pu3yI51K27uaag5CCyWWlmUh8cHmsmtWRFvGGqv+ccz8vcybW0ZAFvcwTXMJSp7LdFSe3FIJKYZlJLzouspJosObTjEybSnn/RjQ1sw195jQ5M2CI5RGoLSg4iiWkmteSsyDIYEyYkLteML3ma0/mEA2nBAi5mGHsxgzf4K5G/ZTp00F7PIqWlmdSSk+JNbou2PT/Qn8H04QE2UsAgBjCUa1hNnU1ltNezSNmlMoppvJlNRTOpJQM6dkxeY6jGb/TmIQZwC3VZxeOcx0AGsZxGQFBbeOutDAQrUsHFTRBmtm+JU9+E35uYWRN3n5q+sKQy2X57WLUqlZLOKTzH7VzHbiziTY7mWu5kJnttLqEmJJFyk6gG8ffwe3WgEJhOUIPYC/gEUlwYXySGVOYwRDuYD7mLqzmISUxnLzoxlvF02vS+Fs0TKX9xO6nd/Qh3PwJYAuzr7oXuvh+wD7AwUwFKxRIZkZRqcmjOAp7nJD7kUJrwFecxgn2Zyng6/a7jWclBpPylMoppd3efGTlw91nA3mmLSCqk3r1TG5EUUY/vuJfLmENrjmYsAxhES+YzkvPYvXUB7upnEEm3VEYxfWFm/wRGEwx17QHMTWtUUqGk0vEcsS2/chn30Z/B1GY1j3IRN3ET37IzrVvD7NnpjVVENkulBnEuMBu4HLgCmAOcl76QpCLp3Tu15GAUcyZFzKMVd9KXibRnT2bSi2F8y86MHq3kIJJpyTYMKgBec/eOEK52JpKC3r3h4YdTK3s473AXV7MfU5nKPpzH47zDkQCqNYhkUcIahLtvBNaaWeylLxMwsxFmtsLMZkWdO8XMZptZsZkVJrh2sZnNNLNpZqalxfNMx46pJYfdmcvLdOUdjqQBK+nBk3TcbjJv+5GbOp+VHESyJ5U+iF+BmWY2HlgTOenulyW5biTwADAq6twsgu1LH0nhuUdoQl7+adMG5sxJXGZHvuUmbuIiHmUNtejLEMbvfhlT59bITJAikpJUEsTrlGG7UXefaGbNSpybC2CprKMgeSdZcqjBWq7ibvpyB9X5lYfozbJzB3Dn4w24I3NhikiKUkkQzwDNCUYw/cfdf01vSBA+a5yZOfCIuw+PV9DMegI9AZo0aZKB0CSWjh3jJ4cqbORsRnErN9CI5bxIN27adggzfm2Z2SBFpFQSLbWxDXAbwZajSwj6K3Yxs8eB/u6+Po1xHeLuy81sR4I9sb9w94mxCobJYzhAYWGhFlrIgkRLZXRkPHdxNW2ZwSe04zSeYWqNQ7Uxj0geSNRJPRTYAdjV3fdz932A3YC6wF3pDMrdl4ffVwAvAe3S+Twpm8hy3LGSw5+Zyb/pzHg6UYefOY0xHMgkfmyt5CCSLxIliGOBi9z958gJd/8J6AV0SVdAZlbLzOpEXgOdCDq3JUdElsuI1aT0R5bzTy5gGntzAJ9wFX9nD+Yyru5puJtGJYnkkUQJwt23XBszHPqatCnHzJ4GPgZamdkyM7vAzLqZ2TLgIOB1Mxsblm1oZm+El+4EfGBm04FPgdfd/c3S/ViSDomWy6jFam7iRhbQgrN4kn9wBbvxH+7hKuo33Fa7t4nkoUSd1HPM7Gx3jx6mipn1AL5IdmN3PyPOWy/FKLucsFbi7ouAtsnuL5lTVARnnRV7Ke0CNnA+IxjEQHbmW8ZwGtdzG1/yJ0AT3UTyWaIEcSnwopmdD0whqDXsD9QAumUgNskB8YeuOl14gzu5ljbM4X0O5Xhe5lMO2FSiYUMlB5F8lmi576/d/QBgELAY+AoY5O7t3P3rDMUnWRDpY4jXz7APU3mLjrzOsVRjHd14kfZM/F1y6NULvta/EpG8lsqWo28Db2cgFsmyRE1JALuwlMH052ye5Dvq8Tfu4xEuZj3VNpXRdp8iFUcqE+Wkgku2HPcf+B/9GMIV/APDGUJfbuc6fmLzEl3qaxCpeFJZ7lsqoKIiqF8/8SY+27Ce3jzIQppzHUN4npNpxTyuY8im5NC6tRbVE6molCAqgcjw1OivHj3g++/jXeEcz7+YxZ95kD7M4s/sx2TO5km+oikABQVojwaRCk4JooJLdentiP35lIm05190o5gqHMurHMnbTGW/TWV69YING6B79zQELCI5QwmiAkt1NzeAZnzJU5zBpxxAS+ZzMcPYk5m8zrFAsPpur15Bc9JDD6UvZhHJHeqkrqBS3Qe6Lj/Sn8H8jfvZSAGDGMBQrmE1dQCNShKpzFSDqIBSSQ7V+I0ruIf/sBtXcTdFdKcFC7iRQaymDrVrB30MSg4ilZcSRAWTvFnJOZnnmENr7uEqPmN/9uFzLmAE31ijTc1IP/+sPgaRyk4JogLp3Ttxh/TBfMhHHMxznMoaanE0b9KZsSyq3ZbRo6G4WP0LIrKZ+iAqiKIiGDYs9nvNWcAQ+nESL7Kq5h/h/sfY65xzGFtQkNkgRSSvqAZRQfTvv+USGfX4jnu5jDm05mjG8vifBlF3xQI4//xgIoOISAKqQVQQX321+fW2/Mpl3Ed/BlOb1TzKRUw49Caee3/n7AUoInlHNYgKoKgonCFNMWdSxDxacSd9mUh79mQmM3oNU3IQkVJTgshT0Wsp9egBfyl+l09pRxE9+J56HMkEuvIqh/dqrY5nESkTJYg8VFQE550XrKW0O3N5ma68yxHsyArOYhSFTOYdjqRePY1KEpGyU4LIQ5dfDtuv/5aH6MVM9uQw3qMft9OKeYzmLDz8z/rDD1kOVETymjqp88yYEWu5+Pt76McQqvMrD9OLQQzkOxpsUbZJkywEKCIVhhJEvti4EZ58ksN63sDpfM2LdKMfQ1hAy5jFq1WDwYMzHKOIVChKEPlg/Hi4+mqYMYOvaMepPM0H/CVu8dq1g0lzWipDRLaG+iBy2cyZcMwx0KkT/PQTjx01hgOZFDc51KsXLLCndZREpDykLUGY2QgzW2Fms6LOnWJms82s2MwKE1zb2czmmdlCM+uXrhhz1vLlcOGFsPfe/DZxEgNq/p1tF3/BheNPI7I3QzSzIDF8950Sg4iUn3TWIEYCnUucmwWcCEyMd5GZFQAPAscArYEzzKx1mmLMLatXw403QosWMGoUc4++gibr/sOta69iHdvGvcxdiUFEyl/a+iDcfaKZNStxbi6A2ZZ/BUdpByx090Vh2THA8cCc9ESaAzZsgMcfhwED4Ntv4dRTefmA2zjp2t3YuDH55U2bpj9EEal8crEPohGwNOp4WXguJjPraWaTzWzyypUr0x5cuXKHN96Atm2hZ09o3hw+/piirs9w5oDUkoOZRiuJSHrkYoKIVb3wGOeCN9yHu3uhuxc2aLDlXICc9fnnwdZvf/0rrFsHL7wA778PBx5I//6wdm3yW5jBJZeoeUlE0iMXE8QyoHHU8S7A8izFUv6WLoWzz4b99oPp0+G++2D2bDjxRDCjqAiWLEl+m3r14MkntZSGiKRPLiaIz4AWZrarmVUDTgdeyXJMW++nn+D666FlS3j2Wbj2Wli4EP72N6hWbdPiez16JL5NZCirRiyJSLqlc5jr08DHQCszW2ZmF5hZNzNbBhwEvG5mY8OyDc3sDQB33wD0AcYCc4Fn3X12uuJMu/Xr4cEHg/6F22+Hk0+GefNgyBCoWxcIFt/r2TNYfC+emjWVGEQks8xLbkOWxwoLC33y5MnZDiPgDi+/DH37wvz5cPjhcNddQdNSCc2aJW9WGj1aiUFEyp+ZTXH3mPPScrGJKf99+ikcdhh06wZVqsArr8Dbb29KDkVFQVKoUiVoVkqWHJo2VXIQkcxTgihPX34JZ5wBBxwQNCM9/HCwXMZxxwVDjtjcnLRkSVDJSNSsBEHTkoaxikg2aLG+8vDjj8Gn+P33Q0EB3HBD0Aldp84WRVMdwgpBh/S996r2ICLZoQSxNdatC8aZDhoEq1bBuefCLbdAo7jz+vjqq9RurT4HEck2NTGVhTs89xzssQdceSUUFgYT30aMSJgcILVNfNTnICK5QAmitD76CA45BE49NeggePNNGDcuWC4jBYMHB5fFoz4HEckVShCpWrgwmMNwyCGweDE89hhMmwZHH12q23TvDsOHB7UEs6CfoV694HXTpsF7qj2ISC5QH0Qy330X9Cs89BBsu23Q33DVVVCrVplv2b27koCI5D4liHh+/TUYlTR4cLBF24UXws03w847ZzsyEZGMUIIoqbgYxowJ1k1asiRYbfWOO6BNm2xHJiKSUeqDiPbuu9CuXdD+s8MOMGECvPaakoOIVEpKEABz50LXrnDEEcGObqNGweTJcOSR2Y5MRCRr1MS0alUwj6GgIFht9fLLoUaNbEclIpJ1ShB16wYLJB1yCOTTjnQiImmmBAFwwgnZjkBEJOeoD0JERGJSghARkZiUIDIoeqOgZs2CYxGRXKUEkUYld447//zNGwUtWRJsHKQkISK5SgkiTWLtHLdu3e/LrF0bbCAkIpKLlCDSJNWd41LdQEhEJNOUIMpJyf6FJUtSuy6VDYRERLJB8yDKQaQ5KVJjWLIk2N/BPfF12hxIRHJZ2moQZjbCzFaY2ayoczuY2XgzWxB+3z7OtYvNbKaZTTOzyemKsbzEak5yD5JEtKpVtTmQiOSPdDYxjQQ6lzjXD5jg7i2ACeFxPEe4+97uXpim+MpNvH4E9807xzVtCo8/Huw/VFwcbEqn5CAiuSxtCcLdJwI/lDh9PPBE+PoJ4IR0PT9dYs1liNeP0LRpkAiUEEQkH2W6k3ond/8GIPy+Y5xyDowzsylm1jPRDc2sp5lNNrPJK1euLHVApZm8VnLoamQuQ5cuQX9CNPUviEi+y9VRTIe4+77AMcClZtY+XkF3H+7uhe5e2KCUq7HG+8CPlyRi9TWsXQtvvBH0J0Q3J6l/QUTynXmyoTZbc3OzZsBr7v7n8HgecLi7f2NmfwTedfdWSe5xE7Da3e9K9rzCwkKfPDn1Pu14w1EjTUMlVakSe2SSWdCMJCKSb8xsSry+3kzXIF4BzglfnwO8XLKAmdUyszqR10AnYFbJcuUhXudyvPPx+ho0l0FEKqJ0DnN9GvgYaGVmy8zsAmAIcJSZLQCOCo8xs4Zm9kZ46U7AB2Y2HfgUeN3d30xHjKX9wB88WH0NIlJ5pG2inLufEeetDjHKLge6hK8XAW3TFVe0wYN/P8ENEn/gR/oU+vcPahlNmgRl1dcgIhVRpZ5JXZYP/O7dlRBEpHKo1AkC9IEvIhJPrg5zFRGRLFOCEBGRmJQgREQkJiUIERGJSQlCRERiSutSG5lmZiuBFPdyKzf1ge8y/MzyoLgzS3FnluJOXVN3j7mQXYVKENlgZpPzYc+KkhR3ZinuzFLc5UNNTCIiEpMShIiIxKQEsfWGZzuAMlLcmaW4M0txlwP1QYiISEyqQYiISExKECIiEpMSxFYys1vMbIaZTTOzcWbWMNsxpcrMhprZF2H8L5lZ3WzHlAozO8XMZptZsZnlzJDAWMyss5nNM7OFZtYv2/GkysxGmNkKM0vLbo7pYmaNzewdM5sb/hu5PNsxpcLMqpvZp2Y2PYz75mzHBOqD2Gpm9gd3/yl8fRnQ2t0vyXJYKTGzTsDb7r7BzO4AcPe+WQ4rKTPbAygGHgGudvfUNyLPIDMrAOYT7J64DPgMOMPd52Q1sBSYWXtgNTAqsqd8Pgj3uv+ju08Nty6eApyQ679zMzOglruvNrOqwAfA5e4+KZtxqQaxlSLJIVQLyJuM6+7j3H1DeDgJ2CWb8aTK3ee6+7xsx5GCdsBCd1/k7uuAMcDxWY4pJe4+Efgh23GUlrt/4+5Tw9c/A3OBRtmNKjkPrA4Pq4ZfWf8sUYIoB2Y22MyWAt2BgdmOp4zOB/6d7SAqmEbA0qjjZeTBh1VFYWbNgH2AT7IcSkrMrMDMpgErgPHunvW4lSBSYGZvmdmsGF/HA7h7f3dvDBQBfbIb7e8liz0s0x/YQBB/Tkgl7jxgMc5l/a/CysDMagMvAFeUqOXnLHff6O57E9Tk25lZ1pv2Kv2Wo6lw944pFn0KeB24MY3hlEqy2M3sHOBYoIPnUIdUKX7nuWwZ0DjqeBdgeZZiqTTCNvwXgCJ3fzHb8ZSWu68ys3eBzkBWBwmoBrGVzKxF1GFX4ItsxVJaZtYZ6At0dfe12Y6nAvoMaGFmu5pZNeB04JUsx1ShhZ29jwFz3f3ubMeTKjNrEBlFaGY1gI7kwGeJRjFtJTN7AWhFMKpmCXCJu3+d3ahSY2YLgW2B78NTk/JhBJaZdQPuBxoAq4Bp7n50VoOKw8y6AP8ACoAR7j44uxGlxsyeBg4nWH76W+BGd38sq0GlwMwOBd4HZhL8Pwlwvbu/kb2okjOzvYAnCP6dVAGedfdB2Y1KCUJEROJQE5OIiMSkBCEiIjEpQYiISExKECIiEpMShIiIxKQEITnHzOqFq+NOM7P/mtnX4etVZpbRRdfM7AQzax11PMjMSj2Jz8yaZXNlVDO7vsTxR+H3rMYluU0JQnKOu3/v7nuHyw4MA+4JX+/N5rHt5cbMEq0ocAKwKUG4+0B3f6u8Y8iA3yUIdz84W4FI/lCCkHxTYGaPhmvmjwtnnWJmu5nZm2Y2xczeN7Pdw/NNzWxCuOfFBDNrEp4faWZ3m9k7wB2xrjezgwlmxw8NazC7hdedHN5jfzP7KFzD/1MzqxP+Rf6+mU0NvxJ+EFvgATObY2avm9kbUfdfbGb1w9eF4fILmFm78Lmfh99bhefPNbMXw59jgZndGZ4fAtQIf4ai8NzqGLEUWLBHyGfh7+vi8PwfzWxieP0sM/vLVv43lHzh7vrSV85+ATcR7PkA0IxgUcG9w+NngR7h6wlAi/D1AQT7XAC8CpwTvj4f+Ff4eiTwGlCQ5PqRwMlR8YwETgaqAYuA/cPzfyBY26wmUD081wKYHBX7rBg/34nAeIIZtA0JZoafHL63GKgfvi4E3o1+Vvi6I/BC+PrcMKbtgOoEM/sbh++tLvHc1SXjAnoCN4SvtwUmA7sC/wf0D88XAHWy/e9CX5n50mJ9km++dPdp4espQLNw5c6DgeeCpXiA4AMO4CCCD2GAJ4E7o+71nLtvTHJ9PK2Ab9z9M9i8L4iZ1QIeMLO9gY1AyyT3aQ887e4bgeVm9naS8hAkgCfCdcCcYO+AiAnu/r8wljlAU36/5HginYC9IjWY8DktCNaUGhEugvevqN+/VHBKEJJvfot6vRGoQdBUusqDfopkoteWWRN+L831EUbspbuvJFi7qG14319LGVO0DWxuBq4edf4W4B1372bBngfvRr1X8vdTmv/HDfibu4/d4o1gh7m/Ak+a2VB3H1WK+0qeUh+E5L3wr/cvzewU2NSu3zZ8+yOCVVQh2NDpg1Je/zNQJ8ZjvwAamtn+4TV1ws7u7QhqFsXAWQRNMolMBE4P2///CBwR9d5iYL/w9UlR57cDIgtCnpvk/hHrwxpAImOBXpFyZtbSzGqZWVNghbs/SrBS6r4pPlPynBKEVBTdgQvMbDowm81be14GnGdmMwg+sONtYh/v+jHANWGH8G6Rwh5sIXoacH94zXiCv/IfAs4xs0kEzUtrSOwlYAHB6qMPA+9FvXczcK+ZvU9QG4i4E7jdzD4keQKKGA7MiHRSx/FPYA4wNRz6+ghBDeRwYJqZfU6QqO5N8ZmS57Saq0gOMbORwGvu/ny2YxFRDUJERGJSDUJERGJSDUJERGJSghARkZiUIEREJCYlCBERiUkJQkREYvp/3WBUaV0iziQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "res = stats.probplot(train_df_no_outliers['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After transforming the salePrice to have normal distribution, we have reduced the Right skewness in the data and this should generally improve the performance of the models. Let's see which of our models gives the better performance now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have transformed the 'salePrice' on whole of our data, so let's split that to get the train and test data separately\n",
    "train_data, test_data = shuffle_split_data(train_df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's fit the tuned Models Random Forest and Gradient Boost regression models and check the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827660864120087"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8820529573279148"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990132167351831"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9040576437398787"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9206741073817067"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111918087287757"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9225614144665581"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(train_data[tot_features_to_use].drop('SalePrice', axis = 1), train_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9094267203555231"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(test_data[tot_features_to_use].drop('SalePrice', axis = 1), test_data[tot_features_to_use]['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We were not able to use simple linear regression model on the transformed data, it errored out, so not including that in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****************************END OF PART 2 ************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3) RESEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Consider Feature selection for the research part as we have seen improvement after selecting the features manually by checking the correlation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are very frequently used techniques of feature selection for regression problems\n",
    "### 1) StepWise Regression\n",
    "### 2) Forward Selection\n",
    "### 3) Backward Elemination\n",
    "### 4) Forward Selection + Backward Elemination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the feature selection method that we used in Part 2 of this assignment, we have seen that it has least effect on RandomForest and Gradient boost regressor models, so for this case we will consider a simple linear regression model and build out our research part\n",
    "\n",
    "## We will be making use of Backward Elimination process to select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's read the data once again, apply all the preprocessing that we applied for all the features, except for 'SalePrice'\n",
    "train_df = read_data(\"./house-prices-advanced-regression-techniques/train.csv\")\n",
    "test_df = read_data(\"./house-prices-advanced-regression-techniques/test.csv\")\n",
    "train_df.drop(\"Id\", axis = 1, inplace = True)\n",
    "test_df.drop('Id', axis = 1, inplace = True)\n",
    "train_df, test_df = preprocess_categorical_features(train_df, test_df)\n",
    "train_df, test_df = handle_missing_values(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove outliers using the same method (isolation forest and using minmax scaler to scale the values)\n",
    "train_df_no_outliers, test_df = outlier_detection_and_removal(train_df, test_df, \"isoforest\",\"minmax\")\n",
    "\n",
    "# Let's split our training data to test the model performance at later stage\n",
    "train_data, test_data = shuffle_split_data(train_df_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (1156, 199)\n",
      "test data shape:  (289, 199)\n"
     ]
    }
   ],
   "source": [
    "# Let's once have a look at the shape of train and test dataset:\n",
    "print(\"train data shape: \", train_data.shape)\n",
    "print(\"test data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score on Train data:  0.930168073268189\n",
      "R2 Score on Test data:  0.899511265062054\n"
     ]
    }
   ],
   "source": [
    "# Let's use our best performing Linear regression model to fit on this data and check how it is performing with all the features\n",
    "BestLRM.fit(train_data.drop('SalePrice', axis = 1), train_data['SalePrice'])\n",
    "print(\"R2 Score on Train data: \", BestLRM.score(train_data.drop('SalePrice', axis = 1), train_data['SalePrice']))\n",
    "print(\"R2 Score on Test data: \", BestLRM.score(test_data.drop('SalePrice', axis = 1), test_data['SalePrice']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the difference between train and test score is 0.03 using all the features. Now we will do backward elimination process to select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('SalePrice', axis = 1)\n",
    "y_train = train_data['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop('SalePrice', axis = 1)\n",
    "y_test = test_data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Linear regression equation is: x0 + x1theta1 + x2theta2 +... + xnthetan\n",
    "where x1 to xn are features, theta1 to thetan are parameters (learnable) x0 is a bias (value is always 1) so to our dataset, we will add another column where each row will have a value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.append(arr = np.ones((1156,1)).astype(int), values=X_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.append(arr = np.ones((289,1)).astype(int), values=X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 199)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 199)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.930</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.917</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   70.29</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 13 Dec 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:21:59</td>     <th>  Log-Likelihood:    </th> <td> -13147.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1156</td>      <th>  AIC:               </th> <td>2.666e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   971</td>      <th>  BIC:               </th> <td>2.760e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   184</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-4.801e+05</td> <td> 4.91e+05</td> <td>   -0.977</td> <td> 0.329</td> <td>-1.44e+06</td> <td> 4.84e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-1.648e+04</td> <td> 1.61e+04</td> <td>   -1.023</td> <td> 0.306</td> <td>-4.81e+04</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 4.074e+04</td> <td> 1.45e+04</td> <td>    2.809</td> <td> 0.005</td> <td> 1.23e+04</td> <td> 6.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  1.09e+05</td> <td> 2.12e+04</td> <td>    5.147</td> <td> 0.000</td> <td> 6.74e+04</td> <td> 1.51e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 9329.7944</td> <td> 1.34e+04</td> <td>    0.695</td> <td> 0.487</td> <td> -1.7e+04</td> <td> 3.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1228.1490</td> <td> 3957.239</td> <td>   -0.310</td> <td> 0.756</td> <td>-8993.875</td> <td> 6537.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> 2730.9895</td> <td> 1797.953</td> <td>    1.519</td> <td> 0.129</td> <td> -797.331</td> <td> 6259.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-1928.9958</td> <td> 3093.309</td> <td>   -0.624</td> <td> 0.533</td> <td>-7999.337</td> <td> 4141.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td> 4.738e+04</td> <td> 2.71e+04</td> <td>    1.749</td> <td> 0.081</td> <td>-5781.893</td> <td> 1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td> 4848.4977</td> <td> 3637.144</td> <td>    1.333</td> <td> 0.183</td> <td>-2289.071</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td> 8751.9080</td> <td> 1138.472</td> <td>    7.687</td> <td> 0.000</td> <td> 6517.760</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td> 5438.2136</td> <td>  972.234</td> <td>    5.594</td> <td> 0.000</td> <td> 3530.291</td> <td> 7346.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>  312.7536</td> <td>   86.149</td> <td>    3.630</td> <td> 0.000</td> <td>  143.694</td> <td>  481.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -7.7814</td> <td>   62.578</td> <td>   -0.124</td> <td> 0.901</td> <td> -130.585</td> <td>  115.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> 5.382e+04</td> <td> 1.07e+04</td> <td>    5.024</td> <td> 0.000</td> <td> 3.28e+04</td> <td> 7.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> 6550.4401</td> <td> 2339.424</td> <td>    2.800</td> <td> 0.005</td> <td> 1959.530</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>-1890.2116</td> <td> 2383.827</td> <td>   -0.793</td> <td> 0.428</td> <td>-6568.257</td> <td> 2787.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> 3186.9634</td> <td> 1881.471</td> <td>    1.694</td> <td> 0.091</td> <td> -505.254</td> <td> 6879.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>-4781.2704</td> <td> 2406.947</td> <td>   -1.986</td> <td> 0.047</td> <td>-9504.688</td> <td>  -57.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td> 4939.0572</td> <td>  946.997</td> <td>    5.215</td> <td> 0.000</td> <td> 3080.662</td> <td> 6797.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td> -294.8331</td> <td>  565.791</td> <td>   -0.521</td> <td> 0.602</td> <td>-1405.147</td> <td>  815.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 1.051e+05</td> <td> 1.61e+04</td> <td>    6.536</td> <td> 0.000</td> <td> 7.35e+04</td> <td> 1.37e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>  527.7643</td> <td> 1398.395</td> <td>    0.377</td> <td> 0.706</td> <td>-2216.460</td> <td> 3271.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td> 6413.5017</td> <td> 1.11e+04</td> <td>    0.577</td> <td> 0.564</td> <td>-1.54e+04</td> <td> 2.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>-1.395e+04</td> <td> 7015.536</td> <td>   -1.988</td> <td> 0.047</td> <td>-2.77e+04</td> <td> -180.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td> 9.327e+04</td> <td> 1.65e+04</td> <td>    5.666</td> <td> 0.000</td> <td>  6.1e+04</td> <td> 1.26e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td> 1425.5454</td> <td> 1080.373</td> <td>    1.319</td> <td> 0.187</td> <td> -694.590</td> <td> 3545.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>  -43.1318</td> <td> 4273.484</td> <td>   -0.010</td> <td> 0.992</td> <td>-8429.460</td> <td> 8343.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> 1.487e+05</td> <td> 1.68e+04</td> <td>    8.858</td> <td> 0.000</td> <td> 1.16e+05</td> <td> 1.82e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>  8.69e+04</td> <td> 1.15e+04</td> <td>    7.556</td> <td> 0.000</td> <td> 6.43e+04</td> <td> 1.09e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>-1.208e+04</td> <td> 1.45e+04</td> <td>   -0.832</td> <td> 0.406</td> <td>-4.06e+04</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td> 1.546e+05</td> <td> 1.32e+04</td> <td>   11.681</td> <td> 0.000</td> <td> 1.29e+05</td> <td> 1.81e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>  839.4755</td> <td> 2224.244</td> <td>    0.377</td> <td> 0.706</td> <td>-3525.403</td> <td> 5204.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> 2640.5796</td> <td> 3325.182</td> <td>    0.794</td> <td> 0.427</td> <td>-3884.792</td> <td> 9165.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>  943.0864</td> <td> 2511.664</td> <td>    0.375</td> <td> 0.707</td> <td>-3985.829</td> <td> 5872.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td> 1746.2672</td> <td> 2363.746</td> <td>    0.739</td> <td> 0.460</td> <td>-2892.372</td> <td> 6384.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>-6232.8601</td> <td> 1518.711</td> <td>   -4.104</td> <td> 0.000</td> <td>-9213.195</td> <td>-3252.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>-1.383e+04</td> <td> 6360.240</td> <td>   -2.175</td> <td> 0.030</td> <td>-2.63e+04</td> <td>-1351.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td> 5678.4173</td> <td> 1860.341</td> <td>    3.052</td> <td> 0.002</td> <td> 2027.666</td> <td> 9329.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td> 1577.6300</td> <td> 1069.219</td> <td>    1.475</td> <td> 0.140</td> <td> -520.617</td> <td> 3675.877</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td> 6335.3193</td> <td> 1252.953</td> <td>    5.056</td> <td> 0.000</td> <td> 3876.511</td> <td> 8794.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td> 3404.8548</td> <td> 2468.172</td> <td>    1.380</td> <td> 0.168</td> <td>-1438.711</td> <td> 8248.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>-1005.1221</td> <td>  889.577</td> <td>   -1.130</td> <td> 0.259</td> <td>-2750.837</td> <td>  740.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   41.7262</td> <td> 1366.112</td> <td>    0.031</td> <td> 0.976</td> <td>-2639.145</td> <td> 2722.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td> 3175.8055</td> <td> 2686.516</td> <td>    1.182</td> <td> 0.237</td> <td>-2096.241</td> <td> 8447.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td> 9719.7345</td> <td> 1.23e+04</td> <td>    0.788</td> <td> 0.431</td> <td>-1.45e+04</td> <td> 3.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td> 1841.9576</td> <td> 4302.726</td> <td>    0.428</td> <td> 0.669</td> <td>-6601.755</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>  991.5011</td> <td> 4402.119</td> <td>    0.225</td> <td> 0.822</td> <td>-7647.263</td> <td> 9630.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td> 9168.5609</td> <td> 5685.247</td> <td>    1.613</td> <td> 0.107</td> <td>-1988.226</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>   1.4e+04</td> <td> 6907.821</td> <td>    2.027</td> <td> 0.043</td> <td>  444.759</td> <td> 2.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td> 9342.3547</td> <td> 7891.076</td> <td>    1.184</td> <td> 0.237</td> <td>-6143.172</td> <td> 2.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td> 7880.4092</td> <td> 1.22e+04</td> <td>    0.646</td> <td> 0.518</td> <td>-1.61e+04</td> <td> 3.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td> 1.107e+04</td> <td> 6713.034</td> <td>    1.649</td> <td> 0.100</td> <td>-2105.916</td> <td> 2.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>-9.473e+04</td> <td> 2.98e+04</td> <td>   -3.175</td> <td> 0.002</td> <td>-1.53e+05</td> <td>-3.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td> 5.871e+04</td> <td> 9396.788</td> <td>    6.248</td> <td> 0.000</td> <td> 4.03e+04</td> <td> 7.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td> -603.7364</td> <td>  688.899</td> <td>   -0.876</td> <td> 0.381</td> <td>-1955.639</td> <td>  748.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td> 1702.0070</td> <td> 4402.972</td> <td>    0.387</td> <td> 0.699</td> <td>-6938.430</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>-2626.2733</td> <td> 2.46e+04</td> <td>   -0.107</td> <td> 0.915</td> <td> -5.1e+04</td> <td> 4.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td> -733.3921</td> <td>  282.194</td> <td>   -2.599</td> <td> 0.009</td> <td>-1287.173</td> <td> -179.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>  187.3368</td> <td>  583.777</td> <td>    0.321</td> <td> 0.748</td> <td> -958.273</td> <td> 1332.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>-9.438e+04</td> <td> 9.84e+04</td> <td>   -0.959</td> <td> 0.338</td> <td>-2.88e+05</td> <td> 9.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>-8.414e+04</td> <td> 9.86e+04</td> <td>   -0.853</td> <td> 0.394</td> <td>-2.78e+05</td> <td> 1.09e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>-9.799e+04</td> <td> 9.87e+04</td> <td>   -0.993</td> <td> 0.321</td> <td>-2.92e+05</td> <td> 9.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>-1.094e+05</td> <td> 9.81e+04</td> <td>   -1.115</td> <td> 0.265</td> <td>-3.02e+05</td> <td> 8.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>-9.424e+04</td> <td> 9.84e+04</td> <td>   -0.957</td> <td> 0.339</td> <td>-2.87e+05</td> <td> 9.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>-2.666e+04</td> <td> 2.16e+04</td> <td>   -1.236</td> <td> 0.217</td> <td> -6.9e+04</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>  -90.5828</td> <td> 2.62e+04</td> <td>   -0.003</td> <td> 0.997</td> <td>-5.15e+04</td> <td> 5.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>-6867.7912</td> <td> 2.16e+04</td> <td>   -0.318</td> <td> 0.750</td> <td>-4.92e+04</td> <td> 3.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>-1.102e+04</td> <td> 1.99e+04</td> <td>   -0.555</td> <td> 0.579</td> <td>   -5e+04</td> <td> 2.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>-3.219e+04</td> <td> 2.06e+04</td> <td>   -1.561</td> <td> 0.119</td> <td>-7.27e+04</td> <td> 8284.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td> -3.21e+04</td> <td> 2.01e+04</td> <td>   -1.597</td> <td> 0.111</td> <td>-7.15e+04</td> <td> 7342.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>-6369.9949</td> <td> 1.98e+04</td> <td>   -0.322</td> <td> 0.748</td> <td>-4.52e+04</td> <td> 3.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>-3.106e+04</td> <td> 1.99e+04</td> <td>   -1.563</td> <td> 0.118</td> <td>   -7e+04</td> <td> 7932.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>-2.895e+04</td> <td> 2.01e+04</td> <td>   -1.438</td> <td> 0.151</td> <td>-6.85e+04</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>-2.036e+04</td> <td> 2.03e+04</td> <td>   -1.005</td> <td> 0.315</td> <td>-6.01e+04</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>-1.312e+04</td> <td> 2.18e+04</td> <td>   -0.603</td> <td> 0.547</td> <td>-5.58e+04</td> <td> 2.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td> -3.61e+04</td> <td> 2.01e+04</td> <td>   -1.800</td> <td> 0.072</td> <td>-7.55e+04</td> <td> 3261.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>-3.037e+04</td> <td> 1.98e+04</td> <td>   -1.536</td> <td> 0.125</td> <td>-6.92e+04</td> <td> 8439.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>  -72.6205</td> <td> 2.47e+04</td> <td>   -0.003</td> <td> 0.998</td> <td>-4.86e+04</td> <td> 4.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>-3.246e+04</td> <td>    2e+04</td> <td>   -1.622</td> <td> 0.105</td> <td>-7.17e+04</td> <td> 6812.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>-5235.2326</td> <td> 2.06e+04</td> <td>   -0.254</td> <td> 0.800</td> <td>-4.57e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td> 8141.6260</td> <td> 2.03e+04</td> <td>    0.400</td> <td> 0.689</td> <td>-3.18e+04</td> <td> 4.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>-2.509e+04</td> <td> 1.99e+04</td> <td>   -1.258</td> <td> 0.209</td> <td>-6.42e+04</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>-3.052e+04</td> <td> 2.08e+04</td> <td>   -1.466</td> <td> 0.143</td> <td>-7.14e+04</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>-2.425e+04</td> <td>    2e+04</td> <td>   -1.213</td> <td> 0.225</td> <td>-6.35e+04</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td> -2.64e+04</td> <td> 2.04e+04</td> <td>   -1.293</td> <td> 0.196</td> <td>-6.65e+04</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>-2.575e+04</td> <td> 2.06e+04</td> <td>   -1.251</td> <td> 0.211</td> <td>-6.62e+04</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td> 7265.3148</td> <td>  2.1e+04</td> <td>    0.345</td> <td> 0.730</td> <td> -3.4e+04</td> <td> 4.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>-3.033e+04</td> <td> 2.04e+04</td> <td>   -1.487</td> <td> 0.137</td> <td>-7.03e+04</td> <td> 9684.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>-2.018e+04</td> <td> 2.13e+04</td> <td>   -0.949</td> <td> 0.343</td> <td>-6.19e+04</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>-5.285e+04</td> <td> 5.49e+04</td> <td>   -0.962</td> <td> 0.336</td> <td>-1.61e+05</td> <td> 5.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>-5.145e+04</td> <td> 5.46e+04</td> <td>   -0.943</td> <td> 0.346</td> <td>-1.58e+05</td> <td> 5.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>-4.294e+04</td> <td> 5.45e+04</td> <td>   -0.788</td> <td> 0.431</td> <td> -1.5e+05</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>-6.619e+04</td> <td> 5.62e+04</td> <td>   -1.177</td> <td> 0.239</td> <td>-1.77e+05</td> <td> 4.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>-4.699e+04</td> <td> 5.52e+04</td> <td>   -0.852</td> <td> 0.395</td> <td>-1.55e+05</td> <td> 6.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>-7.211e+04</td> <td>  5.5e+04</td> <td>   -1.312</td> <td> 0.190</td> <td> -1.8e+05</td> <td> 3.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>-4.432e+04</td> <td> 5.49e+04</td> <td>   -0.807</td> <td> 0.420</td> <td>-1.52e+05</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>-6.446e+04</td> <td> 5.69e+04</td> <td>   -1.133</td> <td> 0.258</td> <td>-1.76e+05</td> <td> 4.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>-3.882e+04</td> <td> 5.58e+04</td> <td>   -0.696</td> <td> 0.486</td> <td>-1.48e+05</td> <td> 7.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td> 3246.2039</td> <td> 3.18e+04</td> <td>    0.102</td> <td> 0.919</td> <td>-5.93e+04</td> <td> 6.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>-2.546e+04</td> <td> 1.93e+04</td> <td>   -1.317</td> <td> 0.188</td> <td>-6.34e+04</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>-1.219e+04</td> <td> 1.45e+04</td> <td>   -0.839</td> <td> 0.402</td> <td>-4.07e+04</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>  3.81e-11</td> <td> 3.08e-11</td> <td>    1.236</td> <td> 0.217</td> <td>-2.24e-11</td> <td> 9.86e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td> -6.19e+04</td> <td> 2.91e+04</td> <td>   -2.124</td> <td> 0.034</td> <td>-1.19e+05</td> <td>-4715.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td> -9.21e+04</td> <td> 9.86e+04</td> <td>   -0.934</td> <td> 0.351</td> <td>-2.86e+05</td> <td> 1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>-8.254e+04</td> <td> 9.85e+04</td> <td>   -0.838</td> <td> 0.402</td> <td>-2.76e+05</td> <td> 1.11e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>-9.331e+04</td> <td> 9.84e+04</td> <td>   -0.948</td> <td> 0.343</td> <td>-2.86e+05</td> <td> 9.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>-1.077e+05</td> <td> 9.84e+04</td> <td>   -1.095</td> <td> 0.274</td> <td>-3.01e+05</td> <td> 8.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>-1.045e+05</td> <td> 9.86e+04</td> <td>   -1.060</td> <td> 0.289</td> <td>-2.98e+05</td> <td> 8.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td> 1.779e+04</td> <td> 1.45e+04</td> <td>    1.229</td> <td> 0.219</td> <td>-1.06e+04</td> <td> 4.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td> 2.891e+04</td> <td> 1.72e+04</td> <td>    1.682</td> <td> 0.093</td> <td>-4813.804</td> <td> 6.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td> 2.599e+04</td> <td> 1.61e+04</td> <td>    1.617</td> <td> 0.106</td> <td>-5553.997</td> <td> 5.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td> 9568.0433</td> <td> 1.67e+04</td> <td>    0.574</td> <td> 0.566</td> <td>-2.31e+04</td> <td> 4.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td> 1.283e+04</td> <td> 1.43e+04</td> <td>    0.896</td> <td> 0.371</td> <td>-1.53e+04</td> <td> 4.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td> 2.017e+04</td> <td> 1.62e+04</td> <td>    1.249</td> <td> 0.212</td> <td>-1.15e+04</td> <td> 5.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td> 2.069e+04</td> <td> 1.57e+04</td> <td>    1.322</td> <td> 0.187</td> <td>   -1e+04</td> <td> 5.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td>-7.675e+04</td> <td> 8.23e+04</td> <td>   -0.932</td> <td> 0.352</td> <td>-2.38e+05</td> <td> 8.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td>-8.552e+04</td> <td> 8.19e+04</td> <td>   -1.044</td> <td> 0.297</td> <td>-2.46e+05</td> <td> 7.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>-8.264e+04</td> <td> 8.23e+04</td> <td>   -1.004</td> <td> 0.315</td> <td>-2.44e+05</td> <td> 7.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td>-8.259e+04</td> <td>  8.2e+04</td> <td>   -1.007</td> <td> 0.314</td> <td>-2.44e+05</td> <td> 7.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th>  <td>-1.023e+05</td> <td> 8.41e+04</td> <td>   -1.216</td> <td> 0.224</td> <td>-2.67e+05</td> <td> 6.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th>  <td>-5.037e+04</td> <td> 8.66e+04</td> <td>   -0.582</td> <td> 0.561</td> <td> -2.2e+05</td> <td>  1.2e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th>  <td>  318.7137</td> <td>    2e+04</td> <td>    0.016</td> <td> 0.987</td> <td> -3.9e+04</td> <td> 3.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th>  <td>-2.473e+04</td> <td> 2.14e+04</td> <td>   -1.156</td> <td> 0.248</td> <td>-6.67e+04</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th>  <td>-1.772e+04</td> <td> 2.69e+04</td> <td>   -0.659</td> <td> 0.510</td> <td>-7.05e+04</td> <td>  3.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th>  <td> 5.833e+04</td> <td> 2.31e+04</td> <td>    2.524</td> <td> 0.012</td> <td>  1.3e+04</td> <td> 1.04e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th>  <td> 1.337e+04</td> <td> 2.62e+04</td> <td>    0.510</td> <td> 0.610</td> <td>-3.81e+04</td> <td> 6.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th>  <td>-1.979e+04</td> <td> 3.84e+04</td> <td>   -0.515</td> <td> 0.607</td> <td>-9.52e+04</td> <td> 5.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th>  <td> 4.412e+04</td> <td> 4.14e+04</td> <td>    1.067</td> <td> 0.286</td> <td> -3.7e+04</td> <td> 1.25e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th>  <td> 2.265e+04</td> <td> 2.27e+04</td> <td>    0.997</td> <td> 0.319</td> <td>-2.19e+04</td> <td> 6.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th>  <td>  -1.8e+04</td> <td> 2.31e+04</td> <td>   -0.779</td> <td> 0.436</td> <td>-6.34e+04</td> <td> 2.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th>  <td> 1.328e+04</td> <td> 3.36e+04</td> <td>    0.396</td> <td> 0.692</td> <td>-5.26e+04</td> <td> 7.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th>  <td> 8588.5415</td> <td> 2.32e+04</td> <td>    0.371</td> <td> 0.711</td> <td>-3.69e+04</td> <td>  5.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th>  <td> 1.596e+04</td> <td> 2.49e+04</td> <td>    0.640</td> <td> 0.522</td> <td> -3.3e+04</td> <td> 6.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th>  <td>-1145.5276</td> <td> 2.33e+04</td> <td>   -0.049</td> <td> 0.961</td> <td>-4.68e+04</td> <td> 4.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th>  <td> 5111.9622</td> <td> 2.59e+04</td> <td>    0.197</td> <td> 0.844</td> <td>-4.58e+04</td> <td>  5.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th>  <td>-6490.0055</td> <td> 2.43e+04</td> <td>   -0.267</td> <td> 0.789</td> <td>-5.41e+04</td> <td> 4.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th>  <td> 4544.4494</td> <td> 2.31e+04</td> <td>    0.197</td> <td> 0.844</td> <td>-4.07e+04</td> <td> 4.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th>  <td> 1.308e+04</td> <td> 2.39e+04</td> <td>    0.547</td> <td> 0.584</td> <td>-3.38e+04</td> <td>    6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th>  <td>-4.097e+04</td> <td> 3.64e+04</td> <td>   -1.126</td> <td> 0.261</td> <td>-1.12e+05</td> <td> 3.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th>  <td>-2.478e+04</td> <td>  3.8e+04</td> <td>   -0.652</td> <td> 0.515</td> <td>-9.94e+04</td> <td> 4.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th>  <td>-2.547e+04</td> <td> 3.84e+04</td> <td>   -0.663</td> <td> 0.508</td> <td>-1.01e+05</td> <td>    5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th>  <td>-3.854e+04</td> <td> 3.48e+04</td> <td>   -1.108</td> <td> 0.268</td> <td>-1.07e+05</td> <td> 2.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th>  <td>  -1.8e+04</td> <td> 2.31e+04</td> <td>   -0.779</td> <td> 0.436</td> <td>-6.34e+04</td> <td> 2.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th>  <td>-3.436e+04</td> <td> 4.21e+04</td> <td>   -0.816</td> <td> 0.415</td> <td>-1.17e+05</td> <td> 4.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th>  <td>-3.616e+04</td> <td> 3.43e+04</td> <td>   -1.055</td> <td> 0.292</td> <td>-1.03e+05</td> <td> 3.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th>  <td>-2.175e+04</td> <td> 3.51e+04</td> <td>   -0.620</td> <td> 0.535</td> <td>-9.06e+04</td> <td> 4.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th>  <td> -3.94e+04</td> <td> 3.49e+04</td> <td>   -1.130</td> <td> 0.259</td> <td>-1.08e+05</td> <td>  2.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th>  <td>-3.343e+04</td> <td> 3.45e+04</td> <td>   -0.970</td> <td> 0.332</td> <td>-1.01e+05</td> <td> 3.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th>  <td>-4.588e+04</td> <td> 3.76e+04</td> <td>   -1.222</td> <td> 0.222</td> <td> -1.2e+05</td> <td> 2.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th>  <td>-2.767e+04</td> <td>  3.5e+04</td> <td>   -0.791</td> <td> 0.429</td> <td>-9.63e+04</td> <td>  4.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th>  <td>  -1.9e+04</td> <td>  3.5e+04</td> <td>   -0.543</td> <td> 0.587</td> <td>-8.76e+04</td> <td> 4.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th>  <td>-3.326e+04</td> <td> 3.43e+04</td> <td>   -0.970</td> <td> 0.332</td> <td>-1.01e+05</td> <td>  3.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th>  <td>-4.145e+04</td> <td> 3.44e+04</td> <td>   -1.204</td> <td> 0.229</td> <td>-1.09e+05</td> <td> 2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th>  <td>-9953.5235</td> <td> 1.21e+04</td> <td>   -0.826</td> <td> 0.409</td> <td>-3.36e+04</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th>  <td>  746.2332</td> <td> 9862.835</td> <td>    0.076</td> <td> 0.940</td> <td>-1.86e+04</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th>  <td> 7165.0140</td> <td> 9808.707</td> <td>    0.730</td> <td> 0.465</td> <td>-1.21e+04</td> <td> 2.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th>  <td> 7857.9592</td> <td> 1.01e+04</td> <td>    0.781</td> <td> 0.435</td> <td>-1.19e+04</td> <td> 2.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th>  <td>-7.711e+04</td> <td> 8.16e+04</td> <td>   -0.945</td> <td> 0.345</td> <td>-2.37e+05</td> <td>  8.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th>  <td>-7.625e+04</td> <td> 8.17e+04</td> <td>   -0.933</td> <td> 0.351</td> <td>-2.37e+05</td> <td> 8.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th>  <td>-7.239e+04</td> <td> 8.18e+04</td> <td>   -0.885</td> <td> 0.377</td> <td>-2.33e+05</td> <td> 8.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th>  <td>-6.644e+04</td> <td> 8.21e+04</td> <td>   -0.809</td> <td> 0.419</td> <td>-2.28e+05</td> <td> 9.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th>  <td>-6.605e+04</td> <td> 8.33e+04</td> <td>   -0.793</td> <td> 0.428</td> <td> -2.3e+05</td> <td> 9.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th>  <td>-1.219e+05</td> <td> 8.42e+04</td> <td>   -1.448</td> <td> 0.148</td> <td>-2.87e+05</td> <td> 4.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th>  <td> 2.421e+04</td> <td> 1.53e+04</td> <td>    1.585</td> <td> 0.113</td> <td>-5772.108</td> <td> 5.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th>  <td> 1.909e+04</td> <td> 1.66e+04</td> <td>    1.148</td> <td> 0.251</td> <td>-1.35e+04</td> <td> 5.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th>  <td> 2.508e+04</td> <td> 1.87e+04</td> <td>    1.340</td> <td> 0.181</td> <td>-1.17e+04</td> <td> 6.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th>  <td> 3.505e+04</td> <td> 2.34e+04</td> <td>    1.499</td> <td> 0.134</td> <td>-1.08e+04</td> <td> 8.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th>  <td>-1.399e+04</td> <td> 1.81e+04</td> <td>   -0.771</td> <td> 0.441</td> <td>-4.96e+04</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th>  <td>-2.083e+04</td> <td> 1.88e+04</td> <td>   -1.110</td> <td> 0.267</td> <td>-5.77e+04</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th>  <td>  841.9643</td> <td> 3.06e+04</td> <td>    0.027</td> <td> 0.978</td> <td>-5.93e+04</td> <td>  6.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th>  <td>-1.738e+04</td> <td>  1.8e+04</td> <td>   -0.967</td> <td> 0.334</td> <td>-5.27e+04</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th>  <td>-2.809e+04</td> <td> 1.63e+04</td> <td>   -1.721</td> <td> 0.086</td> <td>-6.01e+04</td> <td> 3948.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th>  <td>-2.081e+04</td> <td> 1.19e+04</td> <td>   -1.743</td> <td> 0.082</td> <td>-4.42e+04</td> <td> 2614.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th>  <td>-7617.5903</td> <td> 1.34e+04</td> <td>   -0.569</td> <td> 0.570</td> <td>-3.39e+04</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th>  <td>-1.915e+04</td> <td> 1.26e+04</td> <td>   -1.521</td> <td> 0.129</td> <td>-4.39e+04</td> <td> 5564.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th>  <td>-1.997e+04</td> <td> 1.54e+04</td> <td>   -1.300</td> <td> 0.194</td> <td>-5.01e+04</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th>  <td>-1.476e+04</td> <td> 1.17e+04</td> <td>   -1.263</td> <td> 0.207</td> <td>-3.77e+04</td> <td> 8174.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th>  <td> -496.9236</td> <td> 1942.635</td> <td>   -0.256</td> <td> 0.798</td> <td>-4309.171</td> <td> 3315.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th>  <td>-6.752e+04</td> <td> 5.55e+04</td> <td>   -1.216</td> <td> 0.224</td> <td>-1.77e+05</td> <td> 4.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th>  <td>-3.283e+04</td> <td> 5.59e+04</td> <td>   -0.587</td> <td> 0.557</td> <td>-1.43e+05</td> <td> 7.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th>  <td> -2.88e+04</td> <td>    6e+04</td> <td>   -0.480</td> <td> 0.631</td> <td>-1.47e+05</td> <td>  8.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th>  <td>-5.824e+04</td> <td> 5.54e+04</td> <td>   -1.052</td> <td> 0.293</td> <td>-1.67e+05</td> <td> 5.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th>  <td>-6.373e+04</td> <td> 5.62e+04</td> <td>   -1.134</td> <td> 0.257</td> <td>-1.74e+05</td> <td> 4.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th>  <td>-7.278e+04</td> <td> 5.64e+04</td> <td>   -1.291</td> <td> 0.197</td> <td>-1.83e+05</td> <td> 3.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th>  <td>-3.748e+04</td> <td> 5.58e+04</td> <td>   -0.671</td> <td> 0.502</td> <td>-1.47e+05</td> <td> 7.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th>  <td>-5.405e+04</td> <td> 5.62e+04</td> <td>   -0.962</td> <td> 0.336</td> <td>-1.64e+05</td> <td> 5.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th>  <td>-6.471e+04</td> <td> 5.49e+04</td> <td>   -1.179</td> <td> 0.239</td> <td>-1.72e+05</td> <td>  4.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th>  <td>-8.699e+04</td> <td> 8.19e+04</td> <td>   -1.062</td> <td> 0.289</td> <td>-2.48e+05</td> <td> 7.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th>  <td>-5.947e+04</td> <td> 8.41e+04</td> <td>   -0.707</td> <td> 0.479</td> <td>-2.24e+05</td> <td> 1.05e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th>  <td>-8.223e+04</td> <td> 8.28e+04</td> <td>   -0.993</td> <td> 0.321</td> <td>-2.45e+05</td> <td> 8.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th>  <td>-8.455e+04</td> <td> 8.22e+04</td> <td>   -1.029</td> <td> 0.304</td> <td>-2.46e+05</td> <td> 7.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th>  <td>-7.853e+04</td> <td> 8.22e+04</td> <td>   -0.956</td> <td> 0.339</td> <td> -2.4e+05</td> <td> 8.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th>  <td>-8.836e+04</td> <td> 8.32e+04</td> <td>   -1.062</td> <td> 0.288</td> <td>-2.52e+05</td> <td> 7.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th>  <td>-1.286e+05</td> <td> 9.93e+04</td> <td>   -1.296</td> <td> 0.195</td> <td>-3.23e+05</td> <td> 6.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th>  <td>-7.893e+04</td> <td>  9.9e+04</td> <td>   -0.797</td> <td> 0.425</td> <td>-2.73e+05</td> <td> 1.15e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th>  <td>-9.275e+04</td> <td> 9.81e+04</td> <td>   -0.946</td> <td> 0.345</td> <td>-2.85e+05</td> <td> 9.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th>  <td>-8.618e+04</td> <td> 9.81e+04</td> <td>   -0.878</td> <td> 0.380</td> <td>-2.79e+05</td> <td> 1.06e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th>  <td>-9.364e+04</td> <td> 9.83e+04</td> <td>   -0.953</td> <td> 0.341</td> <td>-2.87e+05</td> <td> 9.93e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>288.153</td> <th>  Durbin-Watson:     </th> <td>   2.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4147.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.735</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>12.162</td>  <th>  Cond. No.          </th> <td>1.47e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 6.3e-25. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.930\n",
       "Model:                            OLS   Adj. R-squared:                  0.917\n",
       "Method:                 Least Squares   F-statistic:                     70.29\n",
       "Date:                Sun, 13 Dec 2020   Prob (F-statistic):               0.00\n",
       "Time:                        22:21:59   Log-Likelihood:                -13147.\n",
       "No. Observations:                1156   AIC:                         2.666e+04\n",
       "Df Residuals:                     971   BIC:                         2.760e+04\n",
       "Df Model:                         184                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -4.801e+05   4.91e+05     -0.977      0.329   -1.44e+06    4.84e+05\n",
       "x1         -1.648e+04   1.61e+04     -1.023      0.306   -4.81e+04    1.51e+04\n",
       "x2          4.074e+04   1.45e+04      2.809      0.005    1.23e+04    6.92e+04\n",
       "x3           1.09e+05   2.12e+04      5.147      0.000    6.74e+04    1.51e+05\n",
       "x4          9329.7944   1.34e+04      0.695      0.487    -1.7e+04    3.57e+04\n",
       "x5         -1228.1490   3957.239     -0.310      0.756   -8993.875    6537.577\n",
       "x6          2730.9895   1797.953      1.519      0.129    -797.331    6259.310\n",
       "x7         -1928.9958   3093.309     -0.624      0.533   -7999.337    4141.345\n",
       "x8          4.738e+04   2.71e+04      1.749      0.081   -5781.893    1.01e+05\n",
       "x9          4848.4977   3637.144      1.333      0.183   -2289.071     1.2e+04\n",
       "x10         8751.9080   1138.472      7.687      0.000    6517.760     1.1e+04\n",
       "x11         5438.2136    972.234      5.594      0.000    3530.291    7346.136\n",
       "x12          312.7536     86.149      3.630      0.000     143.694     481.813\n",
       "x13           -7.7814     62.578     -0.124      0.901    -130.585     115.022\n",
       "x14         5.382e+04   1.07e+04      5.024      0.000    3.28e+04    7.48e+04\n",
       "x15         6550.4401   2339.424      2.800      0.005    1959.530    1.11e+04\n",
       "x16        -1890.2116   2383.827     -0.793      0.428   -6568.257    2787.834\n",
       "x17         3186.9634   1881.471      1.694      0.091    -505.254    6879.181\n",
       "x18        -4781.2704   2406.947     -1.986      0.047   -9504.688     -57.853\n",
       "x19         4939.0572    946.997      5.215      0.000    3080.662    6797.453\n",
       "x20         -294.8331    565.791     -0.521      0.602   -1405.147     815.481\n",
       "x21         1.051e+05   1.61e+04      6.536      0.000    7.35e+04    1.37e+05\n",
       "x22          527.7643   1398.395      0.377      0.706   -2216.460    3271.989\n",
       "x23         6413.5017   1.11e+04      0.577      0.564   -1.54e+04    2.82e+04\n",
       "x24        -1.395e+04   7015.536     -1.988      0.047   -2.77e+04    -180.228\n",
       "x25         9.327e+04   1.65e+04      5.666      0.000     6.1e+04    1.26e+05\n",
       "x26         1425.5454   1080.373      1.319      0.187    -694.590    3545.681\n",
       "x27          -43.1318   4273.484     -0.010      0.992   -8429.460    8343.196\n",
       "x28         1.487e+05   1.68e+04      8.858      0.000    1.16e+05    1.82e+05\n",
       "x29          8.69e+04   1.15e+04      7.556      0.000    6.43e+04    1.09e+05\n",
       "x30        -1.208e+04   1.45e+04     -0.832      0.406   -4.06e+04    1.64e+04\n",
       "x31         1.546e+05   1.32e+04     11.681      0.000    1.29e+05    1.81e+05\n",
       "x32          839.4755   2224.244      0.377      0.706   -3525.403    5204.354\n",
       "x33         2640.5796   3325.182      0.794      0.427   -3884.792    9165.951\n",
       "x34          943.0864   2511.664      0.375      0.707   -3985.829    5872.001\n",
       "x35         1746.2672   2363.746      0.739      0.460   -2892.372    6384.907\n",
       "x36        -6232.8601   1518.711     -4.104      0.000   -9213.195   -3252.525\n",
       "x37        -1.383e+04   6360.240     -2.175      0.030   -2.63e+04   -1351.298\n",
       "x38         5678.4173   1860.341      3.052      0.002    2027.666    9329.169\n",
       "x39         1577.6300   1069.219      1.475      0.140    -520.617    3675.877\n",
       "x40         6335.3193   1252.953      5.056      0.000    3876.511    8794.128\n",
       "x41         3404.8548   2468.172      1.380      0.168   -1438.711    8248.420\n",
       "x42        -1005.1221    889.577     -1.130      0.259   -2750.837     740.593\n",
       "x43           41.7262   1366.112      0.031      0.976   -2639.145    2722.597\n",
       "x44         3175.8055   2686.516      1.182      0.237   -2096.241    8447.852\n",
       "x45         9719.7345   1.23e+04      0.788      0.431   -1.45e+04    3.39e+04\n",
       "x46         1841.9576   4302.726      0.428      0.669   -6601.755    1.03e+04\n",
       "x47          991.5011   4402.119      0.225      0.822   -7647.263    9630.265\n",
       "x48         9168.5609   5685.247      1.613      0.107   -1988.226    2.03e+04\n",
       "x49           1.4e+04   6907.821      2.027      0.043     444.759    2.76e+04\n",
       "x50         9342.3547   7891.076      1.184      0.237   -6143.172    2.48e+04\n",
       "x51         7880.4092   1.22e+04      0.646      0.518   -1.61e+04    3.18e+04\n",
       "x52         1.107e+04   6713.034      1.649      0.100   -2105.916    2.42e+04\n",
       "x53        -9.473e+04   2.98e+04     -3.175      0.002   -1.53e+05   -3.62e+04\n",
       "x54         5.871e+04   9396.788      6.248      0.000    4.03e+04    7.71e+04\n",
       "x55         -603.7364    688.899     -0.876      0.381   -1955.639     748.166\n",
       "x56         1702.0070   4402.972      0.387      0.699   -6938.430    1.03e+04\n",
       "x57        -2626.2733   2.46e+04     -0.107      0.915    -5.1e+04    4.57e+04\n",
       "x58         -733.3921    282.194     -2.599      0.009   -1287.173    -179.611\n",
       "x59          187.3368    583.777      0.321      0.748    -958.273    1332.946\n",
       "x60        -9.438e+04   9.84e+04     -0.959      0.338   -2.88e+05    9.88e+04\n",
       "x61        -8.414e+04   9.86e+04     -0.853      0.394   -2.78e+05    1.09e+05\n",
       "x62        -9.799e+04   9.87e+04     -0.993      0.321   -2.92e+05    9.57e+04\n",
       "x63        -1.094e+05   9.81e+04     -1.115      0.265   -3.02e+05    8.31e+04\n",
       "x64        -9.424e+04   9.84e+04     -0.957      0.339   -2.87e+05    9.89e+04\n",
       "x65        -2.666e+04   2.16e+04     -1.236      0.217    -6.9e+04    1.57e+04\n",
       "x66          -90.5828   2.62e+04     -0.003      0.997   -5.15e+04    5.13e+04\n",
       "x67        -6867.7912   2.16e+04     -0.318      0.750   -4.92e+04    3.55e+04\n",
       "x68        -1.102e+04   1.99e+04     -0.555      0.579      -5e+04    2.79e+04\n",
       "x69        -3.219e+04   2.06e+04     -1.561      0.119   -7.27e+04    8284.494\n",
       "x70         -3.21e+04   2.01e+04     -1.597      0.111   -7.15e+04    7342.823\n",
       "x71        -6369.9949   1.98e+04     -0.322      0.748   -4.52e+04    3.25e+04\n",
       "x72        -3.106e+04   1.99e+04     -1.563      0.118      -7e+04    7932.979\n",
       "x73        -2.895e+04   2.01e+04     -1.438      0.151   -6.85e+04    1.05e+04\n",
       "x74        -2.036e+04   2.03e+04     -1.005      0.315   -6.01e+04    1.94e+04\n",
       "x75        -1.312e+04   2.18e+04     -0.603      0.547   -5.58e+04    2.96e+04\n",
       "x76         -3.61e+04   2.01e+04     -1.800      0.072   -7.55e+04    3261.973\n",
       "x77        -3.037e+04   1.98e+04     -1.536      0.125   -6.92e+04    8439.576\n",
       "x78          -72.6205   2.47e+04     -0.003      0.998   -4.86e+04    4.84e+04\n",
       "x79        -3.246e+04      2e+04     -1.622      0.105   -7.17e+04    6812.316\n",
       "x80        -5235.2326   2.06e+04     -0.254      0.800   -4.57e+04    3.53e+04\n",
       "x81         8141.6260   2.03e+04      0.400      0.689   -3.18e+04    4.81e+04\n",
       "x82        -2.509e+04   1.99e+04     -1.258      0.209   -6.42e+04     1.4e+04\n",
       "x83        -3.052e+04   2.08e+04     -1.466      0.143   -7.14e+04    1.03e+04\n",
       "x84        -2.425e+04      2e+04     -1.213      0.225   -6.35e+04     1.5e+04\n",
       "x85         -2.64e+04   2.04e+04     -1.293      0.196   -6.65e+04    1.37e+04\n",
       "x86        -2.575e+04   2.06e+04     -1.251      0.211   -6.62e+04    1.47e+04\n",
       "x87         7265.3148    2.1e+04      0.345      0.730    -3.4e+04    4.86e+04\n",
       "x88        -3.033e+04   2.04e+04     -1.487      0.137   -7.03e+04    9684.622\n",
       "x89        -2.018e+04   2.13e+04     -0.949      0.343   -6.19e+04    2.16e+04\n",
       "x90        -5.285e+04   5.49e+04     -0.962      0.336   -1.61e+05    5.49e+04\n",
       "x91        -5.145e+04   5.46e+04     -0.943      0.346   -1.58e+05    5.56e+04\n",
       "x92        -4.294e+04   5.45e+04     -0.788      0.431    -1.5e+05     6.4e+04\n",
       "x93        -6.619e+04   5.62e+04     -1.177      0.239   -1.77e+05    4.41e+04\n",
       "x94        -4.699e+04   5.52e+04     -0.852      0.395   -1.55e+05    6.13e+04\n",
       "x95        -7.211e+04    5.5e+04     -1.312      0.190    -1.8e+05    3.58e+04\n",
       "x96        -4.432e+04   5.49e+04     -0.807      0.420   -1.52e+05    6.34e+04\n",
       "x97        -6.446e+04   5.69e+04     -1.133      0.258   -1.76e+05    4.72e+04\n",
       "x98        -3.882e+04   5.58e+04     -0.696      0.486   -1.48e+05    7.06e+04\n",
       "x99         3246.2039   3.18e+04      0.102      0.919   -5.93e+04    6.57e+04\n",
       "x100       -2.546e+04   1.93e+04     -1.317      0.188   -6.34e+04    1.25e+04\n",
       "x101       -1.219e+04   1.45e+04     -0.839      0.402   -4.07e+04    1.63e+04\n",
       "x102         3.81e-11   3.08e-11      1.236      0.217   -2.24e-11    9.86e-11\n",
       "x103        -6.19e+04   2.91e+04     -2.124      0.034   -1.19e+05   -4715.628\n",
       "x104        -9.21e+04   9.86e+04     -0.934      0.351   -2.86e+05    1.01e+05\n",
       "x105       -8.254e+04   9.85e+04     -0.838      0.402   -2.76e+05    1.11e+05\n",
       "x106       -9.331e+04   9.84e+04     -0.948      0.343   -2.86e+05    9.99e+04\n",
       "x107       -1.077e+05   9.84e+04     -1.095      0.274   -3.01e+05    8.54e+04\n",
       "x108       -1.045e+05   9.86e+04     -1.060      0.289   -2.98e+05    8.89e+04\n",
       "x109        1.779e+04   1.45e+04      1.229      0.219   -1.06e+04    4.62e+04\n",
       "x110        2.891e+04   1.72e+04      1.682      0.093   -4813.804    6.26e+04\n",
       "x111        2.599e+04   1.61e+04      1.617      0.106   -5553.997    5.75e+04\n",
       "x112        9568.0433   1.67e+04      0.574      0.566   -2.31e+04    4.23e+04\n",
       "x113        1.283e+04   1.43e+04      0.896      0.371   -1.53e+04    4.09e+04\n",
       "x114        2.017e+04   1.62e+04      1.249      0.212   -1.15e+04    5.19e+04\n",
       "x115        2.069e+04   1.57e+04      1.322      0.187      -1e+04    5.14e+04\n",
       "x116       -7.675e+04   8.23e+04     -0.932      0.352   -2.38e+05    8.48e+04\n",
       "x117       -8.552e+04   8.19e+04     -1.044      0.297   -2.46e+05    7.52e+04\n",
       "x118       -8.264e+04   8.23e+04     -1.004      0.315   -2.44e+05    7.88e+04\n",
       "x119       -8.259e+04    8.2e+04     -1.007      0.314   -2.44e+05    7.83e+04\n",
       "x120       -1.023e+05   8.41e+04     -1.216      0.224   -2.67e+05    6.27e+04\n",
       "x121       -5.037e+04   8.66e+04     -0.582      0.561    -2.2e+05     1.2e+05\n",
       "x122         318.7137      2e+04      0.016      0.987    -3.9e+04    3.96e+04\n",
       "x123       -2.473e+04   2.14e+04     -1.156      0.248   -6.67e+04    1.73e+04\n",
       "x124       -1.772e+04   2.69e+04     -0.659      0.510   -7.05e+04     3.5e+04\n",
       "x125        5.833e+04   2.31e+04      2.524      0.012     1.3e+04    1.04e+05\n",
       "x126        1.337e+04   2.62e+04      0.510      0.610   -3.81e+04    6.49e+04\n",
       "x127       -1.979e+04   3.84e+04     -0.515      0.607   -9.52e+04    5.56e+04\n",
       "x128        4.412e+04   4.14e+04      1.067      0.286    -3.7e+04    1.25e+05\n",
       "x129        2.265e+04   2.27e+04      0.997      0.319   -2.19e+04    6.72e+04\n",
       "x130         -1.8e+04   2.31e+04     -0.779      0.436   -6.34e+04    2.74e+04\n",
       "x131        1.328e+04   3.36e+04      0.396      0.692   -5.26e+04    7.91e+04\n",
       "x132        8588.5415   2.32e+04      0.371      0.711   -3.69e+04     5.4e+04\n",
       "x133        1.596e+04   2.49e+04      0.640      0.522    -3.3e+04    6.49e+04\n",
       "x134       -1145.5276   2.33e+04     -0.049      0.961   -4.68e+04    4.45e+04\n",
       "x135        5111.9622   2.59e+04      0.197      0.844   -4.58e+04     5.6e+04\n",
       "x136       -6490.0055   2.43e+04     -0.267      0.789   -5.41e+04    4.12e+04\n",
       "x137        4544.4494   2.31e+04      0.197      0.844   -4.07e+04    4.98e+04\n",
       "x138        1.308e+04   2.39e+04      0.547      0.584   -3.38e+04       6e+04\n",
       "x139       -4.097e+04   3.64e+04     -1.126      0.261   -1.12e+05    3.05e+04\n",
       "x140       -2.478e+04    3.8e+04     -0.652      0.515   -9.94e+04    4.98e+04\n",
       "x141       -2.547e+04   3.84e+04     -0.663      0.508   -1.01e+05       5e+04\n",
       "x142       -3.854e+04   3.48e+04     -1.108      0.268   -1.07e+05    2.97e+04\n",
       "x143         -1.8e+04   2.31e+04     -0.779      0.436   -6.34e+04    2.74e+04\n",
       "x144       -3.436e+04   4.21e+04     -0.816      0.415   -1.17e+05    4.83e+04\n",
       "x145       -3.616e+04   3.43e+04     -1.055      0.292   -1.03e+05    3.11e+04\n",
       "x146       -2.175e+04   3.51e+04     -0.620      0.535   -9.06e+04    4.71e+04\n",
       "x147        -3.94e+04   3.49e+04     -1.130      0.259   -1.08e+05     2.9e+04\n",
       "x148       -3.343e+04   3.45e+04     -0.970      0.332   -1.01e+05    3.42e+04\n",
       "x149       -4.588e+04   3.76e+04     -1.222      0.222    -1.2e+05    2.78e+04\n",
       "x150       -2.767e+04    3.5e+04     -0.791      0.429   -9.63e+04     4.1e+04\n",
       "x151         -1.9e+04    3.5e+04     -0.543      0.587   -8.76e+04    4.96e+04\n",
       "x152       -3.326e+04   3.43e+04     -0.970      0.332   -1.01e+05     3.4e+04\n",
       "x153       -4.145e+04   3.44e+04     -1.204      0.229   -1.09e+05    2.61e+04\n",
       "x154       -9953.5235   1.21e+04     -0.826      0.409   -3.36e+04    1.37e+04\n",
       "x155         746.2332   9862.835      0.076      0.940   -1.86e+04    2.01e+04\n",
       "x156        7165.0140   9808.707      0.730      0.465   -1.21e+04    2.64e+04\n",
       "x157        7857.9592   1.01e+04      0.781      0.435   -1.19e+04    2.76e+04\n",
       "x158       -7.711e+04   8.16e+04     -0.945      0.345   -2.37e+05     8.3e+04\n",
       "x159       -7.625e+04   8.17e+04     -0.933      0.351   -2.37e+05    8.42e+04\n",
       "x160       -7.239e+04   8.18e+04     -0.885      0.377   -2.33e+05    8.82e+04\n",
       "x161       -6.644e+04   8.21e+04     -0.809      0.419   -2.28e+05    9.47e+04\n",
       "x162       -6.605e+04   8.33e+04     -0.793      0.428    -2.3e+05    9.75e+04\n",
       "x163       -1.219e+05   8.42e+04     -1.448      0.148   -2.87e+05    4.33e+04\n",
       "x164        2.421e+04   1.53e+04      1.585      0.113   -5772.108    5.42e+04\n",
       "x165        1.909e+04   1.66e+04      1.148      0.251   -1.35e+04    5.17e+04\n",
       "x166        2.508e+04   1.87e+04      1.340      0.181   -1.17e+04    6.18e+04\n",
       "x167        3.505e+04   2.34e+04      1.499      0.134   -1.08e+04    8.09e+04\n",
       "x168       -1.399e+04   1.81e+04     -0.771      0.441   -4.96e+04    2.16e+04\n",
       "x169       -2.083e+04   1.88e+04     -1.110      0.267   -5.77e+04     1.6e+04\n",
       "x170         841.9643   3.06e+04      0.027      0.978   -5.93e+04     6.1e+04\n",
       "x171       -1.738e+04    1.8e+04     -0.967      0.334   -5.27e+04    1.79e+04\n",
       "x172       -2.809e+04   1.63e+04     -1.721      0.086   -6.01e+04    3948.618\n",
       "x173       -2.081e+04   1.19e+04     -1.743      0.082   -4.42e+04    2614.571\n",
       "x174       -7617.5903   1.34e+04     -0.569      0.570   -3.39e+04    1.87e+04\n",
       "x175       -1.915e+04   1.26e+04     -1.521      0.129   -4.39e+04    5564.482\n",
       "x176       -1.997e+04   1.54e+04     -1.300      0.194   -5.01e+04    1.02e+04\n",
       "x177       -1.476e+04   1.17e+04     -1.263      0.207   -3.77e+04    8174.289\n",
       "x178        -496.9236   1942.635     -0.256      0.798   -4309.171    3315.324\n",
       "x179       -6.752e+04   5.55e+04     -1.216      0.224   -1.77e+05    4.15e+04\n",
       "x180       -3.283e+04   5.59e+04     -0.587      0.557   -1.43e+05    7.69e+04\n",
       "x181        -2.88e+04      6e+04     -0.480      0.631   -1.47e+05     8.9e+04\n",
       "x182       -5.824e+04   5.54e+04     -1.052      0.293   -1.67e+05    5.04e+04\n",
       "x183       -6.373e+04   5.62e+04     -1.134      0.257   -1.74e+05    4.65e+04\n",
       "x184       -7.278e+04   5.64e+04     -1.291      0.197   -1.83e+05    3.79e+04\n",
       "x185       -3.748e+04   5.58e+04     -0.671      0.502   -1.47e+05    7.21e+04\n",
       "x186       -5.405e+04   5.62e+04     -0.962      0.336   -1.64e+05    5.62e+04\n",
       "x187       -6.471e+04   5.49e+04     -1.179      0.239   -1.72e+05     4.3e+04\n",
       "x188       -8.699e+04   8.19e+04     -1.062      0.289   -2.48e+05    7.38e+04\n",
       "x189       -5.947e+04   8.41e+04     -0.707      0.479   -2.24e+05    1.05e+05\n",
       "x190       -8.223e+04   8.28e+04     -0.993      0.321   -2.45e+05    8.02e+04\n",
       "x191       -8.455e+04   8.22e+04     -1.029      0.304   -2.46e+05    7.68e+04\n",
       "x192       -7.853e+04   8.22e+04     -0.956      0.339    -2.4e+05    8.27e+04\n",
       "x193       -8.836e+04   8.32e+04     -1.062      0.288   -2.52e+05    7.49e+04\n",
       "x194       -1.286e+05   9.93e+04     -1.296      0.195   -3.23e+05    6.62e+04\n",
       "x195       -7.893e+04    9.9e+04     -0.797      0.425   -2.73e+05    1.15e+05\n",
       "x196       -9.275e+04   9.81e+04     -0.946      0.345   -2.85e+05    9.97e+04\n",
       "x197       -8.618e+04   9.81e+04     -0.878      0.380   -2.79e+05    1.06e+05\n",
       "x198       -9.364e+04   9.83e+04     -0.953      0.341   -2.87e+05    9.93e+04\n",
       "==============================================================================\n",
       "Omnibus:                      288.153   Durbin-Watson:                   2.055\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4147.438\n",
       "Skew:                           0.735   Prob(JB):                         0.00\n",
       "Kurtosis:                      12.162   Cond. No.                     1.47e+17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 6.3e-25. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS=smf.OLS(endog = y_train, exog=X_train).fit()  \n",
    "regressor_OLS.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ashishsaxena2209/step-by-step-regression-backward-elimination\n",
    "\n",
    "def DoBackwardElimination(the_regressor, X, y, minP2eliminate):\n",
    "    \n",
    "    assert np.shape(X)[0] == np.shape(y)[0], 'Length of X and y do not match'\n",
    "    assert minP2eliminate > 0, 'Minimum P value to eliminate cannot be zero or negative'\n",
    "    \n",
    "    original_list = list(range(0, np.shape(the_regressor.pvalues)[0]))\n",
    "    \n",
    "    max_p = 10        # Initializing with random value of maximum P value\n",
    "    i = 0\n",
    "    r2adjusted = []   # Will store R Square adjusted value for each loop\n",
    "    r2 = []           # Will store R Square value  for each loop\n",
    "    list_of_originallist = [] # Will store modified index of X at each loop\n",
    "    classifiers_list = [] # fitted classifiers at each loop\n",
    "    \n",
    "    while max_p >= minP2eliminate:\n",
    "        \n",
    "        p_values = list(the_regressor.pvalues)\n",
    "        r2adjusted.append(the_regressor.rsquared_adj)\n",
    "        r2.append(the_regressor.rsquared)\n",
    "        list_of_originallist.append(original_list)\n",
    "        \n",
    "        max_p = max(p_values)\n",
    "        max_p_idx = p_values.index(max_p)\n",
    "        \n",
    "        if max_p_idx == 0:\n",
    "            \n",
    "            temp_p = set(p_values)\n",
    "            \n",
    "            # removing the largest element from temp list\n",
    "            temp_p.remove(max(temp_p))\n",
    "            \n",
    "            max_p = max(temp_p)\n",
    "            max_p_idx = p_values.index(max_p)\n",
    "            \n",
    "            print('Index value 0 found!! Next index value is {}'.format(max_p_idx))\n",
    "            \n",
    "            if max_p < minP2eliminate:\n",
    "                \n",
    "                print('Max P value found less than 0.1 with 0 index ...Loop Ends!!')\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if max_p < minP2eliminate:\n",
    "            \n",
    "            print('Max P value found less than 0.1 without 0 index...Loop Ends!!')\n",
    "            \n",
    "            break\n",
    "        \n",
    "        val_at_idx = original_list[max_p_idx]\n",
    "        \n",
    "        idx_in_org_lst = original_list.index(val_at_idx)\n",
    "        \n",
    "        original_list.remove(val_at_idx)\n",
    "        \n",
    "        print('Popped column index out of original array is {} with P-Value {}'.format(val_at_idx, np.round(np.array(p_values)[max_p_idx], decimals= 4)))\n",
    "        \n",
    "        X_new = X[:, original_list]\n",
    "        \n",
    "        the_regressor = smf.OLS(endog = y, exog = X_new).fit()\n",
    "        classifiers_list.append(the_regressor)\n",
    "        \n",
    "        print('==================================================================================================')\n",
    "        \n",
    "    return classifiers_list, r2, r2adjusted, list_of_originallist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popped column index out of original array is 78 with P-Value 0.9977\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 66 with P-Value 0.9994\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 27 with P-Value 0.992\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 122 with P-Value 0.9868\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 170 with P-Value 0.9767\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 43 with P-Value 0.9762\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 134 with P-Value 0.9593\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 102 with P-Value 0.9934\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 155 with P-Value 0.9398\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 99 with P-Value 0.9193\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 57 with P-Value 0.9149\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 13 with P-Value 0.9002\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 47 with P-Value 0.8137\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 178 with P-Value 0.8208\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 5 with P-Value 0.7534\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 59 with P-Value 0.7598\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 34 with P-Value 0.7042\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 32 with P-Value 0.7239\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 22 with P-Value 0.6927\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 56 with P-Value 0.6795\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 20 with P-Value 0.6692\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 80 with P-Value 0.6543\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 67 with P-Value 0.7698\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 71 with P-Value 0.7888\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 135 with P-Value 0.6274\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 181 with P-Value 0.6214\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 180 with P-Value 0.8592\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 185 with P-Value 0.7101\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 131 with P-Value 0.5847\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 174 with P-Value 0.574\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 46 with P-Value 0.6932\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 112 with P-Value 0.5679\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 7 with P-Value 0.5576\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 137 with P-Value 0.5652\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 127 with P-Value 0.5204\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 35 with P-Value 0.5172\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 113 with P-Value 0.4981\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 126 with P-Value 0.4977\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 4 with P-Value 0.4858\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 33 with P-Value 0.4497\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 75 with P-Value 0.4292\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 114 with P-Value 0.4804\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 68 with P-Value 0.4465\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 16 with P-Value 0.4586\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 45 with P-Value 0.4505\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 51 with P-Value 0.4344\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 55 with P-Value 0.4256\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 115 with P-Value 0.3489\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 1 with P-Value 0.3944\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 101 with P-Value 0.3383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Popped column index out of original array is 100 with P-Value 0.3567\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 9 with P-Value 0.3024\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 136 with P-Value 0.3067\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 42 with P-Value 0.2902\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 41 with P-Value 0.3987\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 141 with P-Value 0.2791\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 146 with P-Value 0.9916\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 150 with P-Value 0.6883\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 144 with P-Value 0.8192\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 130 with P-Value 0.5483\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 143 with P-Value 0.5483\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 140 with P-Value 0.4602\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 151 with P-Value 0.4634\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 139 with P-Value 0.3615\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 176 with P-Value 0.254\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 186 with P-Value 0.2516\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 124 with P-Value 0.2072\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 168 with P-Value 0.1868\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 169 with P-Value 0.4452\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 171 with P-Value 0.3178\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 50 with P-Value 0.203\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 48 with P-Value 0.2199\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 8 with P-Value 0.2128\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 172 with P-Value 0.2143\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 177 with P-Value 0.3731\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 175 with P-Value 0.4806\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 52 with P-Value 0.2182\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 6 with P-Value 0.2252\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 89 with P-Value 0.1975\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 109 with P-Value 0.1849\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 39 with P-Value 0.2001\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 110 with P-Value 0.1869\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 149 with P-Value 0.1626\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 165 with P-Value 0.146\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 166 with P-Value 0.1972\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 167 with P-Value 0.248\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 164 with P-Value 0.2114\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 152 with P-Value 0.2044\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 142 with P-Value 0.209\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 182 with P-Value 0.149\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 183 with P-Value 0.2003\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 184 with P-Value 0.1906\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 147 with P-Value 0.1081\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 132 with P-Value 0.1275\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 145 with P-Value 0.362\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 23 with P-Value 0.1069\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 74 with P-Value 0.0993\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 26 with P-Value 0.1052\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 17 with P-Value 0.109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Popped column index out of original array is 18 with P-Value 0.2375\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 128 with P-Value 0.092\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 133 with P-Value 0.0887\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 30 with P-Value 0.1062\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 31 with P-Value 0.9166\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 138 with P-Value 0.07\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 103 with P-Value 0.0629\n",
      "==================================================================================================\n",
      "Popped column index out of original array is 153 with P-Value 0.051\n",
      "==================================================================================================\n",
      "Max P value found less than 0.1 without 0 index...Loop Ends!!\n"
     ]
    }
   ],
   "source": [
    "regressor_list, r2, r2adjusted, list_of_changes = DoBackwardElimination(the_regressor=regressor_OLS, \n",
    "                                                                        X= X_train, y= y_train, minP2eliminate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = list_of_changes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we now have 92 features selected using backward elimination out of 198 features (we have eliminated 106 features). Lets make use of those 92 features to get our new train and test data set and check how the Linear regression model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = X_train[:,new_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1156, 92)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test = X_test[:, new_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 92)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236011642819651"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9005244776400707"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLRM.score(new_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02310000000000001"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9236 - 0.9005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check if we get any difference in performance of Random Forest, Gradient boost and Bayesian Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_RFRM = RandomForestRegressor(max_features = 'auto', \n",
    "                                   min_samples_split = 2,\n",
    "                                   max_depth = None,\n",
    "                                   n_estimators = 3000,\n",
    "                                   random_state = 195877,\n",
    "                                   warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=3000, random_state=195877, warm_start=True)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981686786181487"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88013226919887"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_RFRM.score(new_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuned_GBRM = GradientBoostingRegressor(learning_rate = 0.02,\n",
    " max_depth = 4,\n",
    " n_estimators = 3000,\n",
    " random_state = 195877,\n",
    " subsample = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=3000,\n",
       "                          random_state=195877, subsample=0.5)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991754110027975"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177264285241813"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuned_GBRM.score(new_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.fit(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230882947917883"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(new_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9014601266619305"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestBRM.score(new_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021600000000000064"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9230 - 0.9014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so, we can see that after backward elimination the difference between train and test R2 score has reduced to 0.023. Not a significant improvement, but we are successfull in reducing the overfitting by a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************END OF PART3**********************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
